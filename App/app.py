import os
import sys
import re
import json
import logging
import datetime
import pytz
import gevent.monkey
import hashlib
import pandas as pd
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…tokenizerså¹¶è¡ŒåŒ–è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

gevent.monkey.patch_all()

from flask import (
    Flask, request, jsonify, send_from_directory, 
    render_template, redirect, url_for, session, 
    flash, send_file
)
from flask_cors import CORS
from openai import OpenAI
from dotenv import load_dotenv
import httpx
from werkzeug.middleware.proxy_fix import ProxyFix
from werkzeug.security import generate_password_hash, check_password_hash
import unicodedata
import pickle
# æš‚æ—¶æ³¨é‡Šæ‰ä»¥ä¸“é—¨æµ‹è¯•é…ç½®API
# from sentence_transformers import SentenceTransformer
from functools import wraps
from sklearn.metrics.pairwise import cosine_similarity

# è®¾ç½®Pythonæ¨¡å—æœç´¢è·¯å¾„ï¼Œä¾¿äºå¯¼å…¥é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from App.mcp_client_wrapper import MCPClientWrapper  # type: ignore


# åˆ›å»ºFlaskåº”ç”¨å®ä¾‹
app = Flask(__name__, 
    static_url_path='/static',  # é™æ€æ–‡ä»¶URLè·¯å¾„
    static_folder='static',     # é™æ€æ–‡ä»¶ç›®å½•
    template_folder='templates' # æ¨¡æ¿æ–‡ä»¶ç›®å½•
)

# é…ç½®åº”ç”¨
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # é™åˆ¶ä¸Šä¼ æ–‡ä»¶å¤§å°ä¸º16MB
app.config['UPLOAD_FOLDER'] = os.path.join(app.root_path, 'data', 'uploads')
app.config['CACHE_FOLDER'] = os.path.join(app.root_path, 'data', 'cache')
app.config['EMBEDDINGS_FOLDER'] = os.path.join(app.config['CACHE_FOLDER'], 'embeddings')
app.config['CHUNKS_FOLDER'] = os.path.join(app.config['CACHE_FOLDER'], 'chunks')
app.config['ALLOWED_EXTENSIONS'] = {'pdf', 'docx', 'txt', 'md', 'json', 'xlsx'}

# ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„ç›®å½•å­˜åœ¨
for folder in [
    app.config['UPLOAD_FOLDER'], 
    app.config['CACHE_FOLDER'],
    app.config['EMBEDDINGS_FOLDER'],
    app.config['CHUNKS_FOLDER']
]:
    os.makedirs(folder, exist_ok=True)

# æ–‡æ¡£æè¿°å­˜å‚¨æ–‡ä»¶
app.config['DOC_DESCRIPTIONS_FILE'] = os.path.join(app.config['CACHE_FOLDER'], 'doc_descriptions.json')

# åº”ç”¨é…ç½®å­˜å‚¨æ–‡ä»¶
app.config['APP_CONFIG_FILE'] = os.path.join(app.config['CACHE_FOLDER'], 'app_config.json')
app.config['TRAVEL_PURPOSES_FILE'] = os.path.join(app.config['CACHE_FOLDER'], 'travel_purposes.json')
app.config['TRAVEL_PREFERENCES_FILE'] = os.path.join(app.config['CACHE_FOLDER'], 'travel_preferences.json')

# å…¨å±€å˜é‡ï¼šå½“å‰æ—…æ¸¸åŸå¸‚
current_city = "æµ·å£"  # é»˜è®¤å€¼

def update_current_city():
    """æ›´æ–°å½“å‰åŸå¸‚å…¨å±€å˜é‡"""
    global current_city
    try:
        config = load_app_config()
        current_city = config.get('city_config', {}).get('name', 'æµ·å£')
        logging.info(f"å½“å‰åŸå¸‚å·²æ›´æ–°ä¸º: {current_city}")
    except Exception as e:
        logging.error(f"æ›´æ–°å½“å‰åŸå¸‚å¤±è´¥: {e}")
        current_city = 'æµ·å£'  # ä¿æŒé»˜è®¤å€¼

# è®¾ç½®åº”ç”¨å¯†é’¥ - æ¯æ¬¡é‡å¯ç”Ÿæˆæ–°å¯†é’¥ä»¥ç¡®ä¿ä¼šè¯å®‰å…¨
import secrets
app.secret_key = os.environ.get("SECRET_KEY", secrets.token_hex(32))  # ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–éšæœºå¯†é’¥

# è®¾ç½®ä¼šè¯é…ç½® - ç¡®ä¿ä¼šè¯åœ¨æµè§ˆå™¨å…³é—­æ—¶è¿‡æœŸ
app.config['PERMANENT_SESSION_LIFETIME'] = 3600  # 1å°æ—¶åè¿‡æœŸ
app.config['SESSION_COOKIE_HTTPONLY'] = True     # é˜²æ­¢XSSæ”»å‡»
app.config['SESSION_COOKIE_SECURE'] = False      # å¼€å‘ç¯å¢ƒè®¾ä¸ºFalseï¼Œç”Ÿäº§ç¯å¢ƒåº”è®¾ä¸ºTrue
app.config['SESSION_COOKIE_SAMESITE'] = 'Lax'    # CSRFä¿æŠ¤

# å¯ç”¨CORSå’Œä»£ç†æ”¯æŒ
CORS(app, supports_credentials=True)
app.wsgi_app = ProxyFix(app.wsgi_app, x_proto=1, x_host=1)

# åˆ é™¤é‡å¤çš„ /api/files è·¯ç”±ï¼Œä¿ç•™åŠŸèƒ½å®Œæ•´çš„ç‰ˆæœ¬ï¼ˆåœ¨ç¬¬1141è¡Œï¼‰

# å¯ç”¨æ¨¡å‹åˆ—è¡¨
AVAILABLE_MODELS = [
    "doubao-1-5-lite-32k-250115",
    "doubao-1-5-pro-32k-250115",
    "doubao-seed-1-6-250615"
]

# æ¨¡å‹è¯¦ç»†ä¿¡æ¯å­—å…¸
MODEL_INFO = {
    "doubao-1-5-lite-32k-250115": {
        "name": "Doubao-1.5-lite-32k",
        "type": "è½»é‡ç‰ˆ",
        "context_length": "32k", 
        "max_output": "12k",
        "version": "250115",
        "capabilities": ["é€šç”¨ä»»åŠ¡", "é«˜æ€§ä»·æ¯”", "å¿«é€Ÿå“åº”"],
        "suitable_for": ["æ—¥å¸¸å¯¹è¯", "ç®€å•æŸ¥è¯¢", "åŸºç¡€ä»»åŠ¡"],
        "cost_level": "ä½"
    },
    "doubao-1-5-pro-32k-250115": {
        "name": "Doubao-1.5-pro-32k",
        "type": "ä¸“ä¸šç‰ˆ",
        "context_length": "32k",
        "max_output": "12k",
        "version": "250115",
        "capabilities": ["é€šç”¨ä»»åŠ¡", "å·¥å…·è°ƒç”¨", "ä¸“ä¸šæ¨ç†", "ä»£ç ", "ä¸­æ–‡"],
        "suitable_for": ["å¤æ‚æ¨ç†", "å·¥å…·è°ƒç”¨", "ä¸“ä¸šåˆ†æ", "ä»£ç ç”Ÿæˆ"],
        "cost_level": "é«˜"
    },
    "doubao-seed-1-6-250615": {
        "name": "Doubao-Seed-1.6",
        "type": "åˆ›æ„ç‰ˆ",
        "context_length": "32k",
        "max_output": "12k",
        "version": "250615",
        "capabilities": ["åˆ›æ„ç”Ÿæˆ", "æ–‡æ¡ˆå†™ä½œ", "è‰ºæœ¯åˆ›ä½œ", "åˆ›æ–°æ€ç»´"],
        "suitable_for": ["åˆ›æ„å†™ä½œ", "æ–‡æ¡ˆç”Ÿæˆ", "è‰ºæœ¯åˆ›ä½œ", "åˆ›æ–°æ–¹æ¡ˆ"],
        "cost_level": "ä¸­"
    }
}

# ç”¨æˆ·è®¤è¯è£…é¥°å™¨
def login_required(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not session.get('logged_in'):
            if request.is_json:
                return jsonify({'error': 'Unauthorized', 'redirect': url_for('login')}), 401
            return redirect(url_for('login'))
        return f(*args, **kwargs)
    return decorated_function

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

def secure_chinese_filename(filename):
    """
    åˆ›å»ºæ”¯æŒä¸­æ–‡å­—ç¬¦çš„å®‰å…¨æ–‡ä»¶åå‡½æ•°
    ä¿ç•™ä¸­æ–‡å­—ç¬¦ï¼Œåªè¿‡æ»¤å±é™©çš„è·¯å¾„å­—ç¬¦
    """
    if not filename:
        return filename
    
    # å®šä¹‰å±é™©å­—ç¬¦åˆ—è¡¨ï¼ˆè·¯å¾„éå†å’Œç³»ç»Ÿä¿ç•™å­—ç¬¦ï¼‰
    dangerous_chars = ['/', '\\', '..', '<', '>', ':', '"', '|', '?', '*', '\0']
    
    # ç§»é™¤å±é™©å­—ç¬¦
    safe_filename = filename
    for char in dangerous_chars:
        safe_filename = safe_filename.replace(char, '_')
    
    # ç§»é™¤é¦–å°¾ç©ºæ ¼å’Œç‚¹
    safe_filename = safe_filename.strip('. ')
    
    # å¦‚æœæ–‡ä»¶åä¸ºç©ºæˆ–åªåŒ…å«ç‚¹ï¼Œä½¿ç”¨é»˜è®¤åç§°
    if not safe_filename or safe_filename == '.' or safe_filename == '..':
        safe_filename = 'unnamed_file'
    
    # é™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆä¿ç•™æ‰©å±•åï¼‰
    name_part, ext_part = os.path.splitext(safe_filename)
    if len(name_part) > 200:  # é™åˆ¶ä¸»æ–‡ä»¶åé•¿åº¦
        name_part = name_part[:200]
        safe_filename = name_part + ext_part
    
    return safe_filename

def safe_file_path(filename, base_folder):
    """
    å®‰å…¨åœ°æ„å»ºæ–‡ä»¶è·¯å¾„ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
    """
    # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦ï¼Œåªä¿ç•™æ–‡ä»¶å
    filename = os.path.basename(filename)
    
    # æ„å»ºå®Œæ•´è·¯å¾„
    file_path = os.path.join(base_folder, filename)
    
    # è·å–è§„èŒƒåŒ–çš„ç»å¯¹è·¯å¾„
    file_path = os.path.abspath(file_path)
    base_folder = os.path.abspath(base_folder)
    
    # éªŒè¯æ–‡ä»¶è·¯å¾„æ˜¯å¦åœ¨å…è®¸çš„ç›®å½•å†…
    if not file_path.startswith(base_folder + os.sep) and file_path != base_folder:
        raise ValueError(f"éæ³•æ–‡ä»¶è·¯å¾„: {filename}")
    
    return file_path

def validate_upload_request(request):
    """
    éªŒè¯ä¸Šä¼ è¯·æ±‚çš„æœ‰æ•ˆæ€§
    è¿”å› (success, error_info, file_obj)
    """
    if 'file' not in request.files:
        return ErrorHandler.handle_error(
            ErrorHandler.VALIDATION_ERROR, 
            'æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ '
        ) + (None,)
    
    file = request.files['file']
    if file.filename == '':
        return ErrorHandler.handle_error(
            ErrorHandler.VALIDATION_ERROR, 
            'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'
        ) + (None,)
    
    if not allowed_file(file.filename):
        return ErrorHandler.handle_error(
            ErrorHandler.VALIDATION_ERROR, 
            'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'
        ) + (None,)
    
    return True, None, file

def generate_unique_filename(original_filename, upload_folder):
    """
    ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…æ–‡ä»¶åå†²çª
    è¿”å› (final_filename, file_path)
    """
    extension = os.path.splitext(original_filename)[1].lower()
    base_filename = secure_chinese_filename(os.path.splitext(original_filename)[0])
    filename = f"{base_filename}{extension}"
    file_path = os.path.join(upload_folder, filename)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™æ·»åŠ æ•°å­—åç¼€
    counter = 1
    while os.path.exists(file_path):
        new_filename = f"{base_filename}_{counter}{extension}"
        file_path = os.path.join(upload_folder, new_filename)
        counter += 1
    
    return os.path.basename(file_path), file_path

def save_uploaded_file(file, file_path):
    """
    ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶å¹¶è®¾ç½®æƒé™
    """
    file.save(file_path)
    os.chmod(file_path, 0o644)  # ç¡®ä¿æ–‡ä»¶å¯è¯»

def handle_file_upload_core(file, upload_folder, update_index=True):
    """
    æ ¸å¿ƒæ–‡ä»¶ä¸Šä¼ å¤„ç†é€»è¾‘
    è¿”å› (success, info, filename)
    """
    try:
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        os.makedirs(upload_folder, exist_ok=True)
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        final_filename, file_path = generate_unique_filename(file.filename, upload_folder)
        
        # ä¿å­˜æ–‡ä»¶
        save_uploaded_file(file, file_path)
        
        # æ›´æ–°å‘é‡ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if update_index:
            message = 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå‘é‡ç´¢å¼•å·²æ›´æ–°'
            update_embeddings()
        else:
            message = 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œè¯·ç‚¹å‡»"ç”Ÿæˆç´¢å¼•"æŒ‰é’®æ›´æ–°å‘é‡ç´¢å¼•'
        
        success, info = ErrorHandler.handle_success(message, {'filename': final_filename})
        return success, info, final_filename
        
    except Exception as e:
        # å¦‚æœä¿å­˜æ–‡ä»¶åå‡ºé”™ï¼Œå°è¯•åˆ é™¤å·²ä¿å­˜çš„æ–‡ä»¶
        if 'file_path' in locals() and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass
        success, info = ErrorHandler.handle_error(
            ErrorHandler.UPLOAD_ERROR,
            'æ–‡ä»¶å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™',
            f'æ–‡ä»¶ä¸Šä¼ å¤„ç†é”™è¯¯: {str(e)}'
        )
        return success, info, None

def handle_file_deletion_core(filename, upload_folder):
    """
    æ ¸å¿ƒæ–‡ä»¶åˆ é™¤å¤„ç†é€»è¾‘
    è¿”å› (success, info)
    """
    try:
        # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶è·¯å¾„æ„å»º
        file_path = safe_file_path(filename, upload_folder)
        if os.path.exists(file_path):
            # åˆ é™¤æ–‡ä»¶
            os.remove(file_path)
            
            # åŒæ—¶åˆ é™¤å¯¹åº”çš„æè¿°
            descriptions = load_doc_descriptions()
            if filename in descriptions:
                del descriptions[filename]
                save_doc_descriptions(descriptions)
            
            return ErrorHandler.handle_success('æ–‡ä»¶å’Œæè¿°åˆ é™¤æˆåŠŸ')
        else:
            return ErrorHandler.handle_error(
                ErrorHandler.NOT_FOUND_ERROR,
                'æ–‡ä»¶ä¸å­˜åœ¨'
            )
    except ValueError as e:
        return ErrorHandler.handle_error(
            ErrorHandler.SECURITY_ERROR,
            str(e),
            f'å®‰å…¨é”™è¯¯: {str(e)}'
        )
    except Exception as e:
        return ErrorHandler.handle_error(
            ErrorHandler.SERVER_ERROR,
            'åˆ é™¤å¤±è´¥',
            f'æ–‡ä»¶åˆ é™¤é”™è¯¯: {str(e)}'
        )

class ErrorHandler:
    """ç»Ÿä¸€é”™è¯¯å¤„ç†ç±»"""
    
    # é”™è¯¯ç±»å‹å¸¸é‡
    VALIDATION_ERROR = 'validation_error'
    SECURITY_ERROR = 'security_error'
    NOT_FOUND_ERROR = 'not_found_error'
    SERVER_ERROR = 'server_error'
    UPLOAD_ERROR = 'upload_error'
    
    # HTTPçŠ¶æ€ç æ˜ å°„
    STATUS_CODE_MAP = {
        VALIDATION_ERROR: 400,
        SECURITY_ERROR: 400,
        NOT_FOUND_ERROR: 404,
        SERVER_ERROR: 500,
        UPLOAD_ERROR: 500
    }
    
    @staticmethod
    def handle_error(error_type, message, details=None, log_error=True):
        """
        ç»Ÿä¸€é”™è¯¯å¤„ç†æ–¹æ³•
        :param error_type: é”™è¯¯ç±»å‹
        :param message: ç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        :param details: è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        :param log_error: æ˜¯å¦è®°å½•é”™è¯¯æ—¥å¿—
        :return: (success, error_info)
        """
        if log_error:
            log_message = details or message
            logging.error(f'[{error_type.upper()}] {log_message}')
        
        return False, {
            'type': error_type,
            'message': message,
            'status_code': ErrorHandler.STATUS_CODE_MAP.get(error_type, 500)
        }
    
    @staticmethod
    def handle_success(message, data=None):
        """
        ç»Ÿä¸€æˆåŠŸå¤„ç†æ–¹æ³•
        :param message: æˆåŠŸä¿¡æ¯
        :param data: è¿”å›æ•°æ®
        :return: (success, success_info)
        """
        result = {'message': message}
        if data:
            result.update(data)
        return True, result
    
    @staticmethod
    def format_response(success, info, response_type='json'):
        """
        æ ¼å¼åŒ–å“åº”
        :param success: æ˜¯å¦æˆåŠŸ
        :param info: å“åº”ä¿¡æ¯
        :param response_type: å“åº”ç±»å‹ ('json' æˆ– 'flash')
        :return: Flaskå“åº”å¯¹è±¡
        """
        if response_type == 'json':
            if success:
                return jsonify({'success': True, **info})
            else:
                return jsonify({
                    'success': False,
                    'error': info['type'],
                    'msg': info['message']
                }), info['status_code']
        
        elif response_type == 'flash':
            if success:
                flash(info['message'], 'success')
            else:
                # æ ¹æ®é”™è¯¯ç±»å‹è®¾ç½®ä¸åŒçš„flashç±»åˆ«
                if info['type'] == ErrorHandler.SECURITY_ERROR:
                    flash(f"å®‰å…¨é”™è¯¯: {info['message']}", 'error')
                elif info['type'] == ErrorHandler.NOT_FOUND_ERROR:
                    flash(f"æœªæ‰¾åˆ°: {info['message']}", 'warning')
                elif info['type'] == ErrorHandler.VALIDATION_ERROR:
                    flash(f"è¾“å…¥é”™è¯¯: {info['message']}", 'warning')
                else:
                    flash(f"é”™è¯¯: {info['message']}", 'error')
            
            return redirect(url_for('document_management'))

def unified_error_handler(response_type='json'):
    """
    ç»Ÿä¸€é”™è¯¯å¤„ç†è£…é¥°å™¨
    :param response_type: å“åº”ç±»å‹ ('json' æˆ– 'flash')
    """
    def decorator(func):
        @wraps(func)
        def decorated_function(*args, **kwargs):
            try:
                result = func(*args, **kwargs)
                # å¦‚æœå‡½æ•°è¿”å›çš„æ˜¯é”™è¯¯å¤„ç†ç»“æœ
                if isinstance(result, tuple) and len(result) == 2:
                    success, info = result
                    if isinstance(info, dict) and ('type' in info or 'message' in info):
                        return ErrorHandler.format_response(success, info, response_type)
                return result
            except ValueError as e:
                success, info = ErrorHandler.handle_error(
                    ErrorHandler.SECURITY_ERROR, 
                    str(e), 
                    f'Security error in {func.__name__}: {str(e)}'
                )
                return ErrorHandler.format_response(success, info, response_type)
            except FileNotFoundError as e:
                success, info = ErrorHandler.handle_error(
                    ErrorHandler.NOT_FOUND_ERROR, 
                    'æ–‡ä»¶ä¸å­˜åœ¨', 
                    f'File not found in {func.__name__}: {str(e)}'
                )
                return ErrorHandler.format_response(success, info, response_type)
            except Exception as e:
                success, info = ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR, 
                    'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯', 
                    f'Unexpected error in {func.__name__}: {str(e)}'
                )
                return ErrorHandler.format_response(success, info, response_type)
        return decorated_function
    return decorator

# ä¿æŒå‘åå…¼å®¹
def json_error_handler(func):
    """ç»Ÿä¸€JSONé”™è¯¯å¤„ç†è£…é¥°å™¨ï¼ˆå‘åå…¼å®¹ï¼‰"""
    return unified_error_handler('json')(func)

def load_doc_descriptions():
    """
    åŠ è½½æ–‡æ¡£æè¿°æ•°æ®
    """
    try:
        descriptions_file = app.config['DOC_DESCRIPTIONS_FILE']
        if os.path.exists(descriptions_file):
            with open(descriptions_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    except Exception as e:
        logging.error(f'åŠ è½½æ–‡æ¡£æè¿°å¤±è´¥: {str(e)}')
        return {}

def save_doc_descriptions(descriptions):
    """
    ä¿å­˜æ–‡æ¡£æè¿°æ•°æ®
    """
    try:
        descriptions_file = app.config['DOC_DESCRIPTIONS_FILE']
        with open(descriptions_file, 'w', encoding='utf-8') as f:
            json.dump(descriptions, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logging.error(f'ä¿å­˜æ–‡æ¡£æè¿°å¤±è´¥: {str(e)}')
        return False

def check_cache_and_docs_status():
    """
    æ£€æŸ¥æ–‡æ¡£å’Œå‘é‡ç¼“å­˜çŠ¶æ€
    è¿”å›: dict åŒ…å«has_docsã€has_cacheã€doc_listç­‰ä¿¡æ¯
    """
    try:
        # 1. æ£€æŸ¥æ–‡æ¡£æè¿°æ˜¯å¦å­˜åœ¨
        descriptions = load_doc_descriptions()
        has_docs = bool(descriptions)
        
        # 2. æ£€æŸ¥å‘é‡ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        cache_data = load_embedding_cache()
        has_cache = False
        if cache_data is not None:
            # éªŒè¯ç¼“å­˜æ•°æ®å®Œæ•´æ€§
            if all(key in cache_data for key in ['texts', 'embeddings', 'meta']):
                texts = cache_data.get('texts', [])
                embeddings = cache_data.get('embeddings')
                meta = cache_data.get('meta', [])
                if texts and embeddings is not None and meta:
                    has_cache = True
        
        # 3. æ„å»ºæ–‡æ¡£åˆ—è¡¨ï¼ˆå¦‚æœéƒ½å­˜åœ¨ï¼‰
        doc_list = []
        if has_docs and has_cache:
            for doc_name, description in descriptions.items():
                doc_list.append({
                    'name': doc_name,
                    'description': description
                })
        
        return {
            'has_docs': has_docs,
            'has_cache': has_cache,
            'doc_query_available': has_docs and has_cache,
            'doc_list': doc_list
        }
    
    except Exception as e:
        logging.error(f'æ£€æŸ¥ç¼“å­˜å’Œæ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}')
        return {
            'has_docs': False,
            'has_cache': False,
            'doc_query_available': False,
            'doc_list': []
        }

def get_system_status():
    """
    è·å–ç³»ç»ŸçŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ¨¡å‹å’Œç¼“å­˜çŠ¶æ€
    è¿”å›: dict åŒ…å«æ¨¡å‹çŠ¶æ€ã€ç¼“å­˜çŠ¶æ€ç­‰ä¿¡æ¯
    """
    global _embedding_model, _model_loading, _has_vector_cache
    
    cache_status = check_cache_and_docs_status()
    
    return {
        'model_loaded': _embedding_model is not None,
        'model_loading': _model_loading,
        'has_vector_cache': _has_vector_cache,
        'doc_query_available': cache_status['doc_query_available'],
        'startup_strategy': 'smart' if _has_vector_cache else 'minimal',
        'cache_status': cache_status
    }

def get_file_list_data(upload_folder):
    """
    è·å–æ–‡ä»¶åˆ—è¡¨æ•°æ®
    è¿”å›æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    files = []
    descriptions = load_doc_descriptions()
    
    try:
        for filename in os.listdir(upload_folder):
            file_path = os.path.join(upload_folder, filename)
            if os.path.isfile(file_path):
                # è·å–æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
                mtime = os.path.getmtime(file_path)
                upload_time = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
                
                # è·å–æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(file_path)
                if file_size < 1024:
                    size_str = f"{file_size} B"
                elif file_size < 1024 * 1024:
                    size_str = f"{file_size/1024:.1f} KB"
                else:
                    size_str = f"{file_size/(1024*1024):.1f} MB"
                
                files.append({
                    'name': filename,
                    'uploadTime': upload_time,
                    'size': size_str,
                    'type': os.path.splitext(filename)[1].lstrip('.').upper() or 'Unknown',
                    'description': descriptions.get(filename, '')  # æ·»åŠ æè¿°å­—æ®µ
                })
    except Exception as e:
        logging.error(f'è·å–æ–‡ä»¶åˆ—è¡¨é”™è¯¯: {str(e)}')
    return files

# ç™»å½•ç›¸å…³è·¯ç”±
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'GET':
        return render_template('login.html')
    
    data = request.get_json() or {}
    username = data.get('username', '')
    password = data.get('password', '')
    
    # éªŒè¯ç”¨æˆ·åå’Œå¯†ç 
    if not username or not password:
        return jsonify({
            'success': False,
            'message': 'ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º'
        }), 400
    
    if (username == os.getenv('ADMIN_USERNAME') and 
        password == os.getenv('ADMIN_PASSWORD')):
        session['logged_in'] = True
        session.permanent = False  # è®¾ç½®ä¸ºä¸´æ—¶ä¼šè¯ï¼Œæµè§ˆå™¨å…³é—­æ—¶è¿‡æœŸ
        return jsonify({
            'success': True,
            'message': 'ç™»å½•æˆåŠŸ',
            'redirect': url_for('document_management')
        })
    else:
        return jsonify({
            'success': False,
            'message': 'ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯'
        }), 401

@app.route('/logout')
def logout():
    session.pop('logged_in', None)
    return redirect(url_for('login'))

# åŸºç¡€æ¨¡å‹é…ç½®
BASE_MODEL = "doubao-1-5-lite-32k-250115"  # ç”¨äºåŸºç¡€å¯¹è¯å’Œç®€å•ä»»åŠ¡

# è§„åˆ’æ¨¡å‹é…ç½®  
PLANNING_MODEL = "doubao-1-5-pro-32k-250115"  # ç”¨äºå¤æ‚æ¨ç†ã€å·¥å…·è°ƒç”¨å’Œè¡Œç¨‹è§„åˆ’

# å¤šå·¥å…·è°ƒç”¨é…ç½®
MAX_TOOL_ITERATIONS = 5
MAX_CONTEXT_LENGTH = 8000
LOOP_DETECTION_WINDOW = 4
REASONING_TIMEOUT = 30

# æ¨¡å‹é…ç½®
REASONING_MODEL = "doubao-1-5-pro-32k-250115"  # ç”¨äºæ¨ç†åˆ¤æ–­
TOOL_GENERATION_MODEL = "doubao-1-5-lite-32k-250115"  # ç”¨äºå¯¹è¯å¤„ç†ï¼ˆåŒ…æ‹¬å·¥å…·è°ƒç”¨å†³ç­–ï¼‰
FINAL_RESPONSE_MODEL = "doubao-1-5-lite-32k-250115"  # ç”¨äºæœ€ç»ˆå›å¤

# æ¨ç†åˆ¤æ–­æç¤ºè¯
REASONING_SYSTEM_PROMPT = """
èƒŒæ™¯ï¼šæ‰€åœ¨åŸå¸‚æ˜¯{current_city}ï¼Œç”¨æˆ·ä¼šè¯¢é—®ä½ å…³äº{current_city}æ—…æ¸¸çš„ä»»ä½•é—®é¢˜ã€‚
åŸºäºå½“å‰è·å¾—çš„å·¥å…·è°ƒç”¨ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯å®Œæ•´å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

åˆ¤æ–­æ ‡å‡†ï¼š
- ç”¨æˆ·é—®é¢˜çš„æ‰€æœ‰å…³é”®ä¿¡æ¯ç‚¹æ˜¯å¦éƒ½å·²è·å–ï¼Ÿ
- æ˜¯å¦è¿˜æœ‰æ˜æ˜¾ç¼ºå¤±çš„æ•°æ®ï¼Ÿ
- å½“å‰ä¿¡æ¯æ˜¯å¦è¶³ä»¥ç»™å‡ºæ»¡æ„çš„å›ç­”ï¼Ÿ
- å¯¹äºä¾èµ–è°ƒç”¨åœºæ™¯ï¼šæ˜¯å¦è·å¾—äº†æ‰§è¡Œä¸‹ä¸€æ­¥æ‰€éœ€çš„å‚æ•°ï¼Ÿ
- å¯¹äºç‹¬ç«‹è°ƒç”¨åœºæ™¯ï¼šæ˜¯å¦è¦†ç›–äº†ç”¨æˆ·è¯¢é—®çš„æ‰€æœ‰åœ°ç‚¹/äº‹é¡¹ï¼Ÿ
- å¯¹äºé™„è¿‘æœç´¢ç»“æœï¼šå¦‚æœæœç´¢åŠå¾„å·²æ»¡è¶³ç”¨æˆ·è¦æ±‚ï¼Œæ— éœ€å†æ¬¡éªŒè¯è·ç¦»

è¿”å›æ ¼å¼ï¼š
SUFFICIENT: true/false
REASON: [è¯¦ç»†çš„åˆ¤æ–­ç†ç”±ï¼Œè¯´æ˜å·²è·å¾—å“ªäº›ä¿¡æ¯ï¼Œè¿˜ç¼ºå°‘ä»€ä¹ˆä¿¡æ¯]
NEXT_INSTRUCTION: [å¦‚æœä¸å……åˆ†ï¼Œç”Ÿæˆä¸‹ä¸€æ¡å·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼Œæ ¼å¼ä¸º [{{"name": "å·¥å…·å", "parameters": {{...}}}}]]

ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘
ç³»ç»Ÿæ”¯æŒä»¥ä¸‹5ä¸ªå·¥å…·ï¼Œå·¥å…·åç§°å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ï¼š
1. "è·å–å¤©æ°”ä¿¡æ¯" - æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”æƒ…å†µ
2. "æœç´¢å…´è¶£ç‚¹" - å…³é”®è¯æœç´¢POIä¿¡æ¯
3. "é™„è¿‘æœç´¢" - ä»¥æŸä¸ªä½ç½®ä¸ºä¸­å¿ƒæœç´¢å‘¨è¾¹POIä¿¡æ¯
4. "ç›®çš„åœ°è·ç¦»" - æµ‹é‡ä¸¤ä¸ªåœ°ç‚¹ä¹‹é—´çš„è·ç¦»
5. "æ–‡æ¡£æŸ¥è¯¢" - ä»æœ¬åœ°çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶å›ç­”é—®é¢˜

ã€å¤šå·¥å…·è°ƒç”¨ç­–ç•¥ã€‘
æŸäº›å¤æ‚æŸ¥è¯¢å¯èƒ½éœ€è¦å¤šè½®å·¥å…·è°ƒç”¨ï¼š
1. **ä¾èµ–è°ƒç”¨åœºæ™¯**ï¼šåç»­å·¥å…·éœ€è¦å‰é¢å·¥å…·çš„ç»“æœ
    - ç¤ºä¾‹ï¼š"ä¸‡ç»¿å›­é™„è¿‘çš„é…’åº—" â†’ å…ˆæœç´¢ä¸‡ç»¿å›­è·å–åæ ‡ â†’ å†æœç´¢é™„è¿‘é…’åº—
2. **ç‹¬ç«‹è°ƒç”¨åœºæ™¯**ï¼šç”¨æˆ·è¯¢é—®å¤šä¸ªç‹¬ç«‹çš„åœ°ç‚¹æˆ–ä¿¡æ¯
    - ç¤ºä¾‹ï¼š"äº”å…¬ç¥ å’Œæµ·å—çœåšç‰©é¦†çš„ä½ç½®" â†’ åˆ†åˆ«æœç´¢ä¸¤ä¸ªåœ°ç‚¹
3. **è·ç¦»æµ‹é‡åœºæ™¯**ï¼šéœ€è¦å…ˆè·å–ä¸¤ä¸ªåœ°ç‚¹çš„åæ ‡
    - ç¤ºä¾‹ï¼š"ä»Aåœ°åˆ°Båœ°å¤šè¿œ" â†’ æœç´¢Aåœ°åæ ‡ â†’ æœç´¢Båœ°åæ ‡ â†’ è®¡ç®—è·ç¦»

å·¥å…·ä½¿ç”¨è¯´æ˜ï¼š
è·å–å¤©æ°”ä¿¡æ¯ï¼š
## å·¥å…·è¯´æ˜ï¼šæŸ¥è¯¢æŒ‡å®šåŸå¸‚æˆ–åœ°ç‚¹çš„å¤©æ°”æƒ…å†µï¼Œé€‚ç”¨äºç”¨æˆ·è¯¢é—®å¤©æ°”æ—¶ã€‚
## å‚æ•°ï¼šcityï¼ˆåŸå¸‚åæˆ–adcodeï¼Œå­—ç¬¦ä¸²ï¼‰
## è°ƒç”¨å·¥å…·çš„Promptç¤ºä¾‹ï¼š
- "æµ·å£çš„å¤©æ°”å¦‚ä½•ï¼Ÿ"
## è°ƒç”¨æŒ‡ä»¤ï¼š
{"name": "è·å–å¤©æ°”ä¿¡æ¯", "parameters": {"city": "æµ·å£"}}

æœç´¢å…´è¶£ç‚¹ï¼š
## å·¥å…·è¯´æ˜ï¼šå…³é”®è¯æœç´¢æŸåœ°çš„ç›¸å…³POIä¿¡æ¯ï¼Œé€‚ç”¨äºç”¨æˆ·è¯¢é—®æŸåœ°æœ‰ä»€ä¹ˆå¥½ç©çš„ã€å¥½åƒçš„ç­‰ã€‚
## å‚æ•°ï¼škeywordsï¼ˆå…³é”®è¯ï¼Œå­—ç¬¦ä¸²ï¼‰ï¼Œcityï¼ˆåŸå¸‚åï¼Œå¿…å¡«ï¼Œå­—ç¬¦ä¸²ï¼‰
## è°ƒç”¨å·¥å…·çš„Promptç¤ºä¾‹ï¼š
- "æµ·å£çš„æµ·ç”¸å²›ä¸Šæœ‰ä»€ä¹ˆè±ªåé…’åº—ï¼Ÿ"
## è°ƒç”¨æŒ‡ä»¤ï¼š
{"name": "æœç´¢å…´è¶£ç‚¹", "parameters": {"keywords": "æµ·ç”¸å²›è±ªåå‹é…’åº—", "city": "æµ·å£"}}

é™„è¿‘æœç´¢ï¼š
## å·¥å…·è¯´æ˜ï¼šä»¥æŸä¸ªä½ç½®ä¸ºä¸­å¿ƒç‚¹æœç´¢å‘¨è¾¹POIä¿¡æ¯ï¼Œé€‚ç”¨äºç”¨æˆ·è¯¢é—®"é™„è¿‘"ã€"å‘¨è¾¹"ç­‰åœºæ™¯ã€‚
## å‚æ•°è¯´æ˜ï¼š
1. locationï¼ˆå¿…å¡«ï¼‰ï¼šä¸­å¿ƒç‚¹ç»çº¬åº¦
    - æ ¼å¼ï¼šç»åº¦,çº¬åº¦ï¼ˆä¸å¸¦å¼•å·ï¼‰
    - ç¤ºä¾‹ï¼š110.312589,20.055793
    - æ³¨æ„ï¼šç»çº¬åº¦ä¹‹é—´ç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œä¸è¦å¸¦ç©ºæ ¼
    - âš ï¸ é‡è¦ï¼šå¦‚æœä¸çŸ¥é“åœ°ç‚¹çš„ç»çº¬åº¦ï¼Œéœ€è¦å…ˆè°ƒç”¨"æœç´¢å…´è¶£ç‚¹"å·¥å…·è·å–
2. keywordsï¼ˆå¿…å¡«ï¼‰ï¼šæœç´¢å…³é”®è¯
    - ç¤ºä¾‹ï¼š"é…’åº—"ã€"é¤å…"ã€"æ™¯ç‚¹"ç­‰
3. typesï¼ˆå¿…å¡«ï¼‰ï¼šPOIç±»å‹
    - å¯ä»¥ä¸ºç©ºå­—ç¬¦ä¸²ï¼š""
4. radiusï¼ˆå¿…å¡«ï¼‰ï¼šæœç´¢åŠå¾„
    - å•ä½ï¼šç±³
    - ç¤ºä¾‹ï¼š1000ï¼ˆè¡¨ç¤º1å…¬é‡Œï¼‰
    - é»˜è®¤å€¼ï¼š1000
## è°ƒç”¨ç¤ºä¾‹ï¼š
- "ä¸‡ç»¿å›­é™„è¿‘æœ‰ä»€ä¹ˆé…’åº—ï¼Ÿ"
## è°ƒç”¨é€»è¾‘ï¼š
1. å¦‚æœä¸çŸ¥é“ä¸‡ç»¿å›­çš„ç»çº¬åº¦ï¼Œå…ˆè°ƒç”¨ï¼š{"name": "æœç´¢å…´è¶£ç‚¹", "parameters": {"keywords": "ä¸‡ç»¿å›­", "city": "æµ·å£"}}
2. è·å¾—ç»çº¬åº¦åï¼Œå†è°ƒç”¨ï¼š{"name": "é™„è¿‘æœç´¢", "parameters": {"location": "110.312589,20.055793", "keywords": "é…’åº—", "types": "", "radius": 1000}}

ã€æ³¨æ„äº‹é¡¹ã€‘
1. å‚æ•°å¿…é¡»æŒ‰ç…§é¡ºåºæä¾›ï¼šlocation -> keywords -> types -> radius
2. location å‚æ•°ä¸è¦å¸¦åŒå¼•å·
3. types å‚æ•°å³ä½¿ä¸ºç©ºä¹Ÿå¿…é¡»ä¼ å…¥ç©ºå­—ç¬¦ä¸²
4. radius å‚æ•°å¯ä»¥æ˜¯æ•°å­—æˆ–å­—ç¬¦ä¸²ç±»å‹

ç›®çš„åœ°è·ç¦»ï¼š
## å·¥å…·è¯´æ˜ï¼šæµ‹é‡ä¸¤ä¸ªåœ°ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œé€‚ç”¨äºç”¨æˆ·è¯¢é—®è·ç¦»ã€è·¯ç¨‹ç­‰åœºæ™¯ã€‚
## å‚æ•°ï¼š
- originï¼ˆèµ·ç‚¹ç»çº¬åº¦ï¼Œå¿…å¡«ï¼Œå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"ï¼‰
- destinationï¼ˆç»ˆç‚¹ç»çº¬åº¦ï¼Œå¿…å¡«ï¼Œå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"ï¼‰
- typeï¼ˆè·ç¦»ç±»å‹ï¼Œå¯é€‰ï¼Œå­—ç¬¦ä¸²ï¼Œé»˜è®¤"1"ï¼‰
    * "0"ï¼šç›´çº¿è·ç¦»
    * "1"ï¼šé©¾è½¦å¯¼èˆªè·ç¦»
    * "3"ï¼šæ­¥è¡Œå¯¼èˆªè·ç¦»
## è°ƒç”¨å·¥å…·çš„Promptç¤ºä¾‹ï¼š
- "ä»æµ·å£æ¹¾å¹¿åœºåˆ°å‡æ—¥æµ·æ»©æœ‰å¤šè¿œï¼Ÿ"
## è°ƒç”¨é€»è¾‘ï¼š
âš ï¸ å¦‚æœä¸çŸ¥é“åœ°ç‚¹çš„ç»çº¬åº¦ï¼Œéœ€è¦å…ˆåˆ†åˆ«è°ƒç”¨"æœç´¢å…´è¶£ç‚¹"å·¥å…·è·å–èµ·ç‚¹å’Œç»ˆç‚¹çš„åæ ‡
1. æœç´¢èµ·ç‚¹ï¼š{{"name": "æœç´¢å…´è¶£ç‚¹", "parameters": {{"keywords": "æµ·å£æ¹¾å¹¿åœº", "city": "æµ·å£"}}}}
2. æœç´¢ç»ˆç‚¹ï¼š{{"name": "æœç´¢å…´è¶£ç‚¹", "parameters": {{"keywords": "å‡æ—¥æµ·æ»©", "city": "æµ·å£"}}}}
3. è®¡ç®—è·ç¦»ï¼š{{"name": "ç›®çš„åœ°è·ç¦»", "parameters": {{"origin": "110.312589,20.055793", "destination": "110.237390,20.036904"}}}}

ã€æœç´¢åŠå¾„è¯´æ˜ã€‘
åœ¨ä½¿ç”¨"é™„è¿‘æœç´¢"å·¥å…·æ—¶ï¼Œå¯ä»¥é€šè¿‡radiuså‚æ•°æŒ‡å®šæœç´¢åŠå¾„ï¼š
- å‚æ•°åï¼šradius
- å•ä½ï¼šç±³
- é»˜è®¤å€¼ï¼š1000ï¼ˆ1å…¬é‡Œï¼‰
- å»ºè®®å€¼ï¼š
    * 200ï¼šæ­¥è¡Œå¯è¾¾èŒƒå›´ï¼ˆ3-5åˆ†é’Ÿæ­¥è¡Œï¼‰
    * 500ï¼šæ­¥è¡Œç¨è¿œèŒƒå›´ï¼ˆ5-8åˆ†é’Ÿæ­¥è¡Œï¼‰
    * 1000ï¼šé»˜è®¤èŒƒå›´ï¼ˆçº¦15åˆ†é’Ÿæ­¥è¡Œï¼‰
    * 2000ï¼šéª‘è¡ŒèŒƒå›´ï¼ˆçº¦10åˆ†é’Ÿéª‘è¡Œï¼‰
    * 3000ï¼šé©¾è½¦çŸ­é€”èŒƒå›´ï¼ˆçº¦5-10åˆ†é’Ÿé©¾è½¦ï¼‰
    * 5000ï¼šé©¾è½¦èŒƒå›´ï¼ˆçº¦15-20åˆ†é’Ÿé©¾è½¦ï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
1. æ­¥è¡ŒèŒƒå›´å†…çš„é¤å…ï¼š{"location": "110.312589,20.055793", "keywords": "é¤å…", "radius": 500}
2. é©¾è½¦èŒƒå›´å†…çš„æ™¯ç‚¹ï¼š{"location": "110.312589,20.055793", "keywords": "æ™¯ç‚¹", "radius": 5000}

æ–‡æ¡£æŸ¥è¯¢ï¼š
## å·¥å…·è¯´æ˜ï¼šä»æœ¬åœ°çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œé€‚ç”¨èŒƒå›´å‚è€ƒæ–‡æ¡£åˆ—è¡¨ã€‚
## âš ï¸ ä½¿ç”¨å‰æï¼šæ­¤å·¥å…·éœ€è¦ç®¡ç†å‘˜äº‹å…ˆä¸Šä¼ æ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡ç´¢å¼•ï¼Œå¦‚æœç´¢å¼•ä¸å­˜åœ¨ä¼šè‡ªåŠ¨æç¤ºç”¨æˆ·è”ç³»ç®¡ç†å‘˜ã€‚
## å‚æ•°ï¼šqueryï¼ˆæ£€ç´¢å…³é”®è¯ï¼Œå­—ç¬¦ä¸²ï¼‰- **é‡è¦**ï¼šéœ€è¦æ ¹æ®ç”¨æˆ·çš„å®Œæ•´é—®é¢˜æå–æ ¸å¿ƒæ£€ç´¢å…³é”®è¯ï¼Œå»é™¤è¯­æ°”è¯ã€æ— å…³ä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´ã€å‡†ç¡®çš„æŸ¥è¯¢è¯
## å‚æ•°ç”ŸæˆåŸåˆ™ï¼š
- ä»ç”¨æˆ·é—®é¢˜ä¸­æå–æœ€æ ¸å¿ƒçš„æŸ¥è¯¢æ„å›¾
- å»é™¤è¯­æ°”è¯ã€æ„Ÿå¹è¯ã€æ— å…³çš„ä¿®é¥°è¯­
- ä¿ç•™å…³é”®çš„åœ°ç‚¹ã€ç±»å‹ã€ç‰¹å¾ç­‰å®ä½“ä¿¡æ¯
- ä½¿ç”¨ç®€æ´æ˜äº†çš„å…³é”®è¯ç»„åˆ
## è°ƒç”¨å·¥å…·çš„Promptç¤ºä¾‹ï¼š
- ç”¨æˆ·é—®ï¼š"å“å‘€ï¼Œæˆ‘æƒ³çŸ¥é“æµ·å—å…ç¨è´­ç‰©åˆ°åº•æœ‰ä»€ä¹ˆé™åˆ¶å•Šï¼Ÿå¬è¯´æŒºå¤æ‚çš„" â†’ query: "æµ·å—å…ç¨è´­ç‰©é™åˆ¶"
- ç”¨æˆ·é—®ï¼š"èƒ½ä¸èƒ½æ¨èä¸€äº›æµ·å£çš„ç‰¹è‰²é¤å…ï¼Œæˆ‘æ¯”è¾ƒå–œæ¬¢æµ·é²œ" â†’ query: "æµ·å£ç‰¹è‰²é¤å… æµ·é²œ"
- ç”¨æˆ·é—®ï¼š"æˆ‘ä»¬ä¸€å®¶ä¸‰å£è¦å»æµ·å£æ—…æ¸¸ï¼Œå­©å­8å²ï¼Œæœ‰ä»€ä¹ˆé€‚åˆäº²å­æ¸¸çš„é…’åº—å—ï¼Ÿ" â†’ query: "æµ·å£äº²å­é…’åº—"
## è°ƒç”¨æŒ‡ä»¤ï¼š
{"name": "æ–‡æ¡£æŸ¥è¯¢", "parameters": {"query": "æµ·å—å…ç¨è´­ç‰©é™åˆ¶"}}

## æ³¨æ„äº‹é¡¹ï¼š
- æ–‡æ¡£æŸ¥è¯¢å·¥å…·ä¾èµ–ç®¡ç†å‘˜é¢„å…ˆç”Ÿæˆçš„å‘é‡ç´¢å¼•
- å¦‚æœå‘é‡ç´¢å¼•ä¸å­˜åœ¨ï¼Œå·¥å…·ä¼šè‡ªåŠ¨è¿”å›æç¤ºä¿¡æ¯ï¼Œæ— éœ€é¢å¤–å¤„ç†
- å»ºè®®åœ¨æ¨èæœ¬åœ°çŸ¥è¯†ï¼ˆé¤å…ã€é…’åº—ã€æ”»ç•¥ï¼‰æ—¶ä¼˜å…ˆå°è¯•æ­¤å·¥å…·

ã€é‡è¦æŒ‡ä»¤ã€‘
- å½“ä½ éœ€è¦è·å–å®æ—¶æ•°æ®ï¼ˆå¦‚å¤©æ°”ã€åœ°ç‚¹ã€è·¯çº¿ç­‰ï¼‰ï¼Œ**å¿…é¡»**ç”¨å¦‚ä¸‹æ ¼å¼è¾“å‡ºå·¥å…·è°ƒç”¨ï¼š
<|FunctionCallBegin|>
[
{
    "name": "å·¥å…·åç§°",
    "parameters": {
        "å‚æ•°å": "å‚æ•°å€¼"
    }
}
]
<|FunctionCallEnd|>
- å·¥å…·è°ƒç”¨æ ¼å¼å¿…é¡»ä¸¥æ ¼åŒ…å«åœ¨ <|FunctionCallBegin|> å’Œ <|FunctionCallEnd|> ä¹‹é—´ï¼Œä¸”å†…å®¹ä¸º JSON æ•°ç»„ã€‚
- **æ¯æ¬¡åªç”Ÿæˆä¸€æ¡å·¥å…·è°ƒç”¨æŒ‡ä»¤**ï¼Œå·¥å…·è°ƒç”¨åè¯·ç­‰å¾…å·¥å…·ç»“æœè¿”å›ï¼Œå†ç»§ç»­å›å¤ç”¨æˆ·ã€‚
- å¦‚æœéœ€è¦å¤šä¸ªå·¥å…·é…åˆå®Œæˆä»»åŠ¡ï¼Œä¼šè‡ªåŠ¨è¿›è¡Œå¤šè½®è°ƒç”¨ï¼Œä½ åªéœ€ä¸“æ³¨äºå½“å‰æœ€éœ€è¦çš„ä¸€ä¸ªå·¥å…·ã€‚
- **å·¥å…·åç§°å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹5ä¸ªåç§°ä¹‹ä¸€ï¼Œä¸å…è®¸ä»»ä½•å˜ä½“**ï¼š
    * "è·å–å¤©æ°”ä¿¡æ¯"
    * "æœç´¢å…´è¶£ç‚¹" 
    * "é™„è¿‘æœç´¢"
    * "ç›®çš„åœ°è·ç¦»"
    * "æ–‡æ¡£æŸ¥è¯¢"
- æœ‰ç»çº¬åº¦å‚æ•°éƒ½å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·åŒ…è£¹ï¼Œä½œä¸ºå­—ç¬¦ä¸²ä¼ é€’ï¼Œä¾‹å¦‚"110.237390,20.036904"
"""

# æœ€ç»ˆå›å¤æç¤ºè¯
FINAL_RESPONSE_SYSTEM_PROMPT = """
ä½ å·²ç»è·å¾—äº†æ‰€æœ‰éœ€è¦çš„å·¥å…·è°ƒç”¨ç»“æœã€‚è¯·ä½ åªç”¨è‡ªç„¶è¯­è¨€å›å¤ç”¨æˆ·ï¼Œä¸¥ç¦å†è¾“å‡ºä»»ä½•å·¥å…·è°ƒç”¨æŒ‡ä»¤ã€‚

ã€ä»»åŠ¡ã€‘è¯·æ ¹æ®å·¥å…·è¿”å›ä¿¡æ¯ï¼Œç»¼åˆè€ƒè™‘ç”¨æˆ·çš„é—®é¢˜å’Œéœ€æ±‚ï¼Œç”¨å‹å–„ã€äº²åˆ‡çš„è¯­æ°”å‡†ç¡®åœ°å›å¤ç”¨æˆ·ã€‚
ã€é‡è¦ã€‘å¦‚æœæ—¢è°ƒç”¨äº†"æ–‡æ¡£æŸ¥è¯¢"å·¥å…·ï¼Œåˆè°ƒç”¨äº†MCPå·¥å…·ï¼Œè¯·ç»“åˆä¸¤ç§å·¥å…·çš„è¿”å›ç»“æœæ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸èƒ½åªç”¨å…¶ä¸­ä¸€ç§å·¥å…·çš„ç»“æœã€‚

ã€æ–‡æ¡£æŸ¥è¯¢å·¥å…·å¤„ç†è¯´æ˜ã€‘
å½“å·¥å…·è¿”å›ä¿¡æ¯ä¸­åŒ…å«"**æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š**"æ—¶ï¼š
- è¿™è¡¨ç¤ºæ˜¯æ–‡æ¡£æŸ¥è¯¢å·¥å…·çš„è¿”å›ç»“æœï¼ŒåŒ…å«äº†ä»çŸ¥è¯†åº“æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹
- ä½ éœ€è¦åŸºäºè¿™äº›æ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
- è¯·ç›´æ¥æå–å’Œæ•´ç†æ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼Œç”¨è‡ªç„¶è¯­è¨€é‡æ–°ç»„ç»‡å›ç­”
- ä¿ç•™æ–‡æ¡£ä¸­çš„"ğŸ“š **ä¿¡æ¯æ¥æº**"æ ‡æ³¨

ã€é‡è¦ã€‘å›å¤å†…å®¹è¦æ’ç‰ˆç¾è§‚ã€å¯è¯»æ€§å¼ºï¼Œå–„ç”¨æ¢è¡Œã€ç¼©è¿›ã€emojiç­‰ã€‚
æ¯ä¸ªæ¨èé¡¹å¿…é¡»å•ç‹¬ä¸€è¡Œæˆ–ä½¿ç”¨ Markdown åˆ—è¡¨æ ¼å¼ï¼Œé¿å…æ‰€æœ‰ä¿¡æ¯æŒ¤åœ¨ä¸€èµ·ã€‚
ä½¿ç”¨ Markdown çš„æœ‰åºæˆ–æ— åºåˆ—è¡¨ã€åˆ†æ®µã€æ ‡é¢˜ç­‰æ ¼å¼ï¼Œç¡®ä¿æ¯æ¡æ¨èæ¸…æ™°åˆ†æ˜ã€‚
å¦‚æœ‰å¤šä¸ªåœ°ç‚¹/é¥­åº—/æ™¯ç‚¹ï¼ŒåŠ¡å¿…åˆ†æ¡åˆ—å‡ºï¼Œæ¯æ¡å•ç‹¬ä¸€è¡Œã€‚
åœ°ç‚¹åªéœ€è¦æŠ¥å…·ä½“ä½ç½®(ç²¾ç¡®åˆ°è·¯å)ï¼Œä¸éœ€è¦æŠ¥è¯¥åœ°ç‚¹çš„ç»çº¬åº¦ï¼ˆç»çº¬åº¦ä¿¡æ¯æ˜¯ç»™å¤§æ¨¡å‹çœ‹çš„ï¼Œä¸æ˜¯ç»™äººçœ‹çš„ï¼‰ã€‚

ã€æ™¯ç‚¹æ¨èåœºæ™¯ã€‘
- å½“å›å¤ä¸­åŒ…å«æ™¯ç‚¹ç…§ç‰‡ä¿¡æ¯æ—¶ï¼Œå¿…é¡»ä¿ç•™åŸå§‹ç…§ç‰‡æ¡†HTMLä»£ç ï¼ˆ`<div class="poi-photo-container">`ï¼‰ã€‚
- å¦‚æœæ²¡æœ‰æ™¯ç‚¹ç…§ç‰‡ï¼Œåˆ™ä¸éœ€è¦ç”Ÿæˆç…§ç‰‡æ¡†HTMLä»£ç ã€‚
- ç¤ºä¾‹æ­£ç¡®å›å¤ï¼š
  ```
  ### å‡æ—¥æµ·æ»©
  - åœ°å€ï¼šæµ·å£å¸‚ç§€è‹±åŒºæ»¨æµ·å¤§é“126å·
  <div class="poi-photo-container" data-poi-index="1">
    <button class="poi-photo-nav poi-photo-nav-prev" onclick="changePhoto(-1, 1)">&#10094;</button>  <!-- å·¦ç®­å¤´ï¼Œè¡¨ç¤ºä¸Šä¸€å¼ ç…§ç‰‡ -->
    <img src="photo1.jpg" alt="æ™¯ç‚¹ç…§ç‰‡" class="poi-photo" style="display:block">
    <img src="photo2.jpg" alt="æ™¯ç‚¹ç…§ç‰‡" class="poi-photo" style="display:none">
    <button class="poi-photo-nav poi-photo-nav-next" onclick="changePhoto(1, 1)">&#10095;</button>  <!-- å³ç®­å¤´ï¼Œè¡¨ç¤ºä¸‹ä¸€å¼ ç…§ç‰‡ -->
  </div>
  ```

ã€é€šç”¨è§„åˆ™ã€‘
- ä½¿ç”¨è‡ªç„¶è¯­è¨€ç»„ç»‡å†…å®¹ï¼Œä½†ä¸å¾—åˆ é™¤æˆ–ä¿®æ”¹å·²æœ‰çš„HTMLæ ‡ç­¾ã€‚
- ä¿æŒMarkdownæ ¼å¼ï¼ˆå¦‚`### æ ‡é¢˜`ã€`- åˆ—è¡¨é¡¹`ï¼‰ã€‚
- ç¦æ­¢æ–°å¢å·¥å…·è°ƒç”¨æŒ‡ä»¤ã€‚

ã€æ’ç‰ˆè¦æ±‚ã€‘
- æ™¯ç‚¹åç§°ç”¨ä¸‰çº§æ ‡é¢˜ï¼ˆ`###`ï¼‰
- æ¯ä¸ªæ™¯ç‚¹å•ç‹¬åˆ†æ®µ
"""

# è¡Œç¨‹è¡¨ç”Ÿæˆpromptæ¨¡æ¿  
ITINERARY_GENERATION_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{current_city}æ—…æ¸¸è§„åˆ’åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²ï¼Œåˆ†æç”¨æˆ·çš„æ—…è¡Œéœ€æ±‚å¹¶ç”Ÿæˆè¯¦ç»†çš„è¡Œç¨‹è¡¨ã€‚

## å½“å‰ç”¨æˆ·è¡Œç¨‹è¡¨æ•°æ®ï¼š
{current_itinerary_json}

## è¡Œç¨‹è¡¨å¤„ç†è¯´æ˜ï¼š
**å¦‚æœä¸Šæ–¹æœ‰è¡Œç¨‹è¡¨JSONæ•°æ®**ï¼š
- è¿™æ˜¯ç”¨æˆ·å½“å‰ä¿å­˜çš„è¡Œç¨‹å®‰æ’
- è¯·åŸºäºç°æœ‰è¡Œç¨‹è¿›è¡Œæ™ºèƒ½è°ƒæ•´å’Œä¼˜åŒ–
- ä¿æŒç”¨æˆ·å·²ç¡®å®šçš„å®‰æ’ï¼ˆfixed: trueçš„é¡¹ç›®ï¼‰
- æ ¹æ®ç”¨æˆ·çš„æ–°éœ€æ±‚è¿›è¡Œå¢é‡å¼ä¿®æ”¹
- ä¼˜åŒ–æ—¶é—´å®‰æ’å’Œåœ°ç†è·¯çº¿ï¼Œä½†å°½é‡ä¿æŒåŸæœ‰ç»“æ„

**å¦‚æœä¸Šæ–¹æ²¡æœ‰è¡Œç¨‹è¡¨æ•°æ®**ï¼š
- è¿™æ˜¯ç”¨æˆ·é¦–æ¬¡è§„åˆ’è¡Œç¨‹
- è¯·æ ¹æ®å¯¹è¯å†å²ç”Ÿæˆå…¨æ–°çš„è¡Œç¨‹è¡¨
- é‡ç‚¹å…³æ³¨ç”¨æˆ·çš„æ—¶é—´ã€åå¥½å’Œå…·ä½“éœ€æ±‚

## å¯¹è¯å†å²ï¼š
{conversation_history}

## å¿…é¡»æå–çš„ä¿¡æ¯ï¼š
1. **æ—…è¡Œæ—¶é—´**ï¼šå‡ºå‘æ—¥æœŸã€ç»“æŸæ—¥æœŸã€æ€»å¤©æ•°
2. **æ—…è¡Œåå¥½**ï¼šæ™¯ç‚¹ç±»å‹ã€æ´»åŠ¨åå¥½ã€é¢„ç®—èŒƒå›´  
3. **äººå‘˜æ„æˆ**ï¼šåŒè¡Œäººæ•°ã€å¹´é¾„æ®µã€ç‰¹æ®Šéœ€æ±‚
4. **å·²æ¨èåœ°ç‚¹**ï¼šä»å¯¹è¯ä¸­æåŠçš„æ‰€æœ‰æ™¯ç‚¹ã€é¤å…ã€é…’åº—ç­‰
5. **å›ºå®šå®‰æ’**ï¼šå·²ç¡®å®šçš„æ—¶é—´å®‰æ’ï¼ˆå¦‚èˆªç­ã€æ¼”å‡ºç­‰ï¼‰
6. **è°ƒæ•´éœ€æ±‚**ï¼šç”¨æˆ·å¸Œæœ›ä¿®æ”¹çš„å…·ä½“å†…å®¹ï¼ˆå¦‚æœæœ‰ç°æœ‰è¡Œç¨‹ï¼‰

## æ™ºèƒ½æ’æœŸåŸåˆ™ï¼š
1. **ä¿æŒè¿ç»­æ€§**ï¼šæœ‰ç°æœ‰è¡Œç¨‹æ—¶ï¼Œä¿æŒç”¨æˆ·æ»¡æ„çš„å®‰æ’ä¸å˜
2. **æ—¶é—´åˆç†æ€§**ï¼šç¡®ä¿åœ°ç‚¹é—´è½¬ç§»æ—¶é—´å……è¶³ï¼Œé¿å…è¿‡äºç´§å‡‘
3. **åœ°ç†ä¼˜åŒ–**ï¼šåŒä¸€åŒºåŸŸçš„åœ°ç‚¹å®‰æ’åœ¨åŒä¸€å¤©æˆ–ç›¸é‚»æ—¶é—´
4. **æ´»åŠ¨æ­é…**ï¼šåˆç†æ­é…å®¤å†…å¤–æ´»åŠ¨ï¼Œè€ƒè™‘ä½“åŠ›åˆ†é…
5. **ç”¨é¤æ—¶é—´**ï¼šåœ¨åˆé€‚æ—¶é—´å®‰æ’é¤å…ï¼Œé¿å…é¥¥é¥¿æˆ–è¿‡é¥±
6. **å›ºå®šä¼˜å…ˆ**ï¼šä¼˜å…ˆå®‰æ’å›ºå®šæ—¶é—´çš„æ´»åŠ¨ï¼ˆèˆªç­ã€æ¼”å‡ºç­‰ï¼‰

## è¾“å‡ºè¦æ±‚ï¼š
ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—è¯´æ˜ï¼š

{{
  "days": [
    {{
      "date": "YYYY-MM-DD",
      "day_number": 1,
      "locations": [
        {{
          "address": "å…·ä½“åœ°ç‚¹åç§°",
          "time": "HH:MMæˆ–ç©ºå­—ç¬¦ä¸²",
          "notes": "ç›¸å…³è¯´æ˜å’Œå»ºè®®",
          "fixed": false,
          "visit_order": 1
        }}
      ]
    }}
  ]
}}

**é‡è¦æé†’**ï¼š
- åªè¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€æˆ–åç¼€æ–‡å­—
- ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
- fixedå­—æ®µï¼šèˆªç­ã€æ¼”å‡ºã€é¢„è®¢ç­‰è®¾ä¸ºtrueï¼Œç”¨æˆ·æ˜ç¡®è¦æ±‚ä¿æŒçš„å®‰æ’ä¹Ÿè®¾ä¸ºtrue
- timeå­—æ®µï¼šæœ‰æ˜ç¡®æ—¶é—´çš„å¡«å†™"HH:MM"ï¼Œå¦åˆ™å¡«å†™ç©ºå­—ç¬¦ä¸²""
- noteså­—æ®µï¼šæä¾›å®ç”¨çš„æ¸¸è§ˆå»ºè®®å’Œæ³¨æ„äº‹é¡¹
- å¦‚æœæ˜¯åŸºäºç°æœ‰è¡Œç¨‹è°ƒæ•´ï¼Œè¯·åœ¨notesä¸­è¯´æ˜è°ƒæ•´åŸå› 

**é‡è¦æé†’**ï¼š
- åªè¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦æœ‰ä»»ä½•å‰ç¼€æˆ–åç¼€æ–‡å­—
- ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
- fixedå­—æ®µï¼šèˆªç­ã€æ¼”å‡ºã€é¢„è®¢ç­‰è®¾ä¸ºtrue
- timeå­—æ®µï¼šæœ‰æ˜ç¡®æ—¶é—´çš„å¡«å†™"HH:MM"ï¼Œå¦åˆ™å¡«å†™ç©ºå­—ç¬¦ä¸²""
- noteså­—æ®µï¼šæä¾›å®ç”¨çš„æ¸¸è§ˆå»ºè®®å’Œæ³¨æ„äº‹é¡¹
"""

# è¡Œç¨‹è¡¨ç”Ÿæˆè¾…åŠ©å‡½æ•°
def format_full_conversation_for_itinerary(messages):
    """æ ¼å¼åŒ–å®Œæ•´å¯¹è¯å†å²ä¾›è¡Œç¨‹è¡¨ç”Ÿæˆä½¿ç”¨"""
    formatted = []
    # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯ï¼Œä½¿ç”¨æœ€è¿‘15è½®ç”¨æˆ·å’ŒåŠ©æ‰‹çš„å¯¹è¯
    user_messages = [msg for msg in messages if msg.get("role") in ["user", "assistant"]]
    
    for msg in user_messages[-15:]:
        role = "ç”¨æˆ·" if msg.get("role") == "user" else "åŠ©æ‰‹"
        content = msg.get("content", "")
        formatted.append(f"{role}: {content}")
    
    return "\n".join(formatted)

def generate_itinerary_from_conversation(messages, current_itinerary=None):
    """
    åŸºäºå®Œæ•´å¯¹è¯å†å²ç”Ÿæˆè¡Œç¨‹è¡¨ï¼Œæ”¯æŒåŸºäºç°æœ‰è¡Œç¨‹çš„å¢é‡è°ƒæ•´
    
    Args:
        messages: å®Œæ•´å¯¹è¯å†å²
        current_itinerary: å½“å‰è¡Œç¨‹è¡¨æ•°æ®ï¼ˆæ”¯æŒçŸ­æœŸè®°å¿†åŠŸèƒ½ï¼‰
    """
    try:
        logging.info("å¼€å§‹ç”Ÿæˆè¡Œç¨‹è¡¨")
        
        # æ£€æŸ¥å¯¹è¯å†å²æ˜¯å¦è¶³å¤Ÿ - å¦‚æœä¿¡æ¯ä¸å¤Ÿï¼Œç»™å‡ºæ¸©é¦¨æç¤º
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        logging.info(f"ç”¨æˆ·æ¶ˆæ¯æ•°é‡: {len(user_messages)}")
        if len(user_messages) < 2:
            # è¿”å›æ¸©é¦¨æç¤ºè€Œä¸æ˜¯é”™è¯¯
            return {
                "success": True,
                "response": f"æ‚¨å¥½ï¼æˆ‘å¯ä»¥ä¸ºæ‚¨è§„åˆ’{current_city}æ—…æ¸¸è¡Œç¨‹ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³å‚è§‚ä»€ä¹ˆæ™¯ç‚¹ï¼Œæˆ–è€…æ‚¨çš„æ—…è¡Œåå¥½ï¼Œè¿™æ ·æˆ‘å°±èƒ½ä¸ºæ‚¨æ¨èæœ€åˆé€‚çš„è¡Œç¨‹äº†ï¼ğŸ˜Š",
                "action": "friendly_prompt"
            }
        
        # æ ¼å¼åŒ–å®Œæ•´å¯¹è¯å†å²
        logging.info("å¼€å§‹æ ¼å¼åŒ–å¯¹è¯å†å²")
        conversation_history = format_full_conversation_for_itinerary(messages)
        logging.info(f"å¯¹è¯å†å²æ ¼å¼åŒ–å®Œæˆï¼Œé•¿åº¦: {len(conversation_history)}")
        
        # æ„å»ºåŒ…å«å½“å‰è¡Œç¨‹çš„prompt
        logging.info("æ„å»ºè¡Œç¨‹è¡¨ç”Ÿæˆprompt")
        
        # è°ƒè¯•ï¼šè¯¦ç»†æ£€æŸ¥current_itineraryå‚æ•°
        logging.info(f"è°ƒè¯• - å‡½æ•°å†…current_itineraryç±»å‹: {type(current_itinerary)}")
        logging.info(f"è°ƒè¯• - å‡½æ•°å†…current_itineraryå†…å®¹: {current_itinerary}")
        
        itinerary_json_str = ""
        if current_itinerary and current_itinerary.get('days'):
            itinerary_json_str = f"å½“å‰ç”¨æˆ·è¡Œç¨‹è¡¨JSONæ•°æ®ï¼š\n```json\n{json.dumps(current_itinerary, ensure_ascii=False, indent=2)}\n```\n\n"
            logging.info("å·²åŒ…å«å½“å‰è¡Œç¨‹è¡¨æ•°æ®ä½œä¸ºçŸ­æœŸè®°å¿†")
            logging.info(f"è°ƒè¯• - è¡Œç¨‹è¡¨æ•°æ®é•¿åº¦: {len(itinerary_json_str)}")
        else:
            logging.info("æ— å½“å‰è¡Œç¨‹è¡¨æ•°æ®ï¼Œå°†ç”Ÿæˆå…¨æ–°è¡Œç¨‹")
            if current_itinerary:
                logging.info(f"è°ƒè¯• - current_itineraryå­˜åœ¨ä½†æ²¡æœ‰dayså­—æ®µ: {current_itinerary}")
            else:
                logging.info("è°ƒè¯• - current_itineraryä¸ºNoneæˆ–ç©º")
        
        itinerary_prompt = ITINERARY_GENERATION_PROMPT.format(
            current_city=current_city,
            current_itinerary_json=itinerary_json_str,
            conversation_history=conversation_history
        )
        logging.info(f"è¡Œç¨‹è¡¨promptæ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(itinerary_prompt)}")
        
        # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆè¡Œç¨‹è¡¨
        logging.info("å¼€å§‹è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆè¡Œç¨‹è¡¨")
        response = client.chat.completions.create(
            model=PLANNING_MODEL,  # ä½¿ç”¨è§„åˆ’æ¨¡å‹å¤„ç†å¤æ‚ä»»åŠ¡
            messages=[{"role": "user", "content": itinerary_prompt}]
        )
        logging.info("å¤§æ¨¡å‹è°ƒç”¨æˆåŠŸ")
        
        # è§£æJSONè¾“å‡º
        itinerary_content = response.choices[0].message.content.strip()
        logging.info(f"å¤§æ¨¡å‹åŸå§‹å“åº”å†…å®¹: {itinerary_content}")
        
        # å¤„ç†å¯èƒ½çš„markdownä»£ç å—åŒ…è£…
        if "```json" in itinerary_content:
            start = itinerary_content.find("```json") + 7
            end = itinerary_content.rfind("```")
            itinerary_content = itinerary_content[start:end].strip()
        elif "```" in itinerary_content:
            start = itinerary_content.find("```") + 3
            end = itinerary_content.rfind("```")
            itinerary_content = itinerary_content[start:end].strip()
        
        try:
            itinerary_json = json.loads(itinerary_content)
            logging.info(f"è¡Œç¨‹è¡¨JSONè§£ææˆåŠŸï¼ŒåŒ…å« {len(itinerary_json.get('days', []))} å¤©è¡Œç¨‹")
            return {
                "success": True,
                "itinerary": itinerary_json
            }
        except json.JSONDecodeError as e:
            logging.error(f"JSONè§£æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": "ç”Ÿæˆçš„è¡Œç¨‹è¡¨æ ¼å¼ä¸æ­£ç¡®",
                "raw_response": itinerary_content
            }
        
    except Exception as e:
        logging.error(f"è¡Œç¨‹è¡¨ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error": f"è¡Œç¨‹è¡¨ç”Ÿæˆå¤±è´¥: {str(e)}"
        }

def analyze_current_itinerary(current_itinerary):
    """
    åˆ†æå½“å‰è¡Œç¨‹è¡¨ï¼Œæä¾›ä¸“ä¸šçš„æ—…è¡Œå»ºè®®
    
    Args:
        current_itinerary: å½“å‰è¡Œç¨‹è¡¨æ•°æ®
    
    Returns:
        dict: åŒ…å«successçŠ¶æ€å’Œåˆ†æç»“æœçš„å­—å…¸
    """
    try:
        logging.info("å¼€å§‹åˆ†æè¡Œç¨‹è¡¨")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¡Œç¨‹è¡¨æ•°æ®
        if not current_itinerary or not current_itinerary.get('days'):
            return {
                "success": True,
                "response": "æ‚¨è¿˜æ²¡æœ‰åˆ¶å®šè¡Œç¨‹è¡¨å‘¢ï¼è¯·å…ˆå‘Šè¯‰æˆ‘æ‚¨çš„æ—…è¡Œéœ€æ±‚ï¼Œæˆ‘ä¸ºæ‚¨ç”Ÿæˆè¡Œç¨‹è¡¨åå†è¿›è¡Œåˆ†æã€‚ğŸ˜Š"
            }
        
        # è°ƒè¯•ï¼šè¯¦ç»†æ£€æŸ¥current_itineraryå‚æ•°
        logging.info(f"è°ƒè¯• - åˆ†æå‡½æ•°å†…current_itineraryç±»å‹: {type(current_itinerary)}")
        logging.info(f"è°ƒè¯• - åˆ†æå‡½æ•°å†…current_itineraryå¤©æ•°: {len(current_itinerary.get('days', []))}")
        
        # æ„å»ºç®€åŒ–çš„è¡Œç¨‹åˆ†æprompt
        analysis_prompt = f"""è¯·ä½œä¸ºä¸“ä¸šçš„æ—…è¡Œé¡¾é—®ï¼Œåˆ†æä»¥ä¸‹è¡Œç¨‹è¡¨å¹¶æä¾›å»ºè®®ï¼š

è¡Œç¨‹è¡¨æ•°æ®ï¼š
{json.dumps(current_itinerary, ensure_ascii=False, indent=2)}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼š

1. **æ—¶é—´å®‰æ’åˆç†æ€§**
   - æ¯æ—¥æ™¯ç‚¹æ•°é‡æ˜¯å¦åˆé€‚
   - æ¸¸è§ˆæ—¶é—´åˆ†é…æ˜¯å¦å……è¶³
   - äº¤é€šæ—¶é—´æ˜¯å¦è€ƒè™‘å……åˆ†

2. **è·¯çº¿ä¼˜åŒ–å»ºè®®**
   - æ™¯ç‚¹é—´çš„åœ°ç†è·ç¦»å’Œäº¤é€šä¾¿åˆ©æ€§
   - æ˜¯å¦å­˜åœ¨é‡å¤è·¯çº¿æˆ–ç»•è·¯æƒ…å†µ
   - æ¨èæ›´é«˜æ•ˆçš„æ¸¸è§ˆé¡ºåº

3. **ä½“éªŒè´¨é‡è¯„ä¼°**
   - æ™¯ç‚¹ç±»å‹æ­é…æ˜¯å¦ä¸°å¯Œ
   - æ˜¯å¦å¹³è¡¡äº†æ–‡åŒ–ã€è‡ªç„¶ã€ç¾é£Ÿç­‰ä¸åŒä½“éªŒ
   - èŠ‚å¥å®‰æ’æ˜¯å¦å¼ å¼›æœ‰åº¦

4. **å®ç”¨æ€§å»ºè®®**
   - æ ¹æ®æµ·å£å½“åœ°æƒ…å†µæä¾›æ³¨æ„äº‹é¡¹
   - æ¨èæœ€ä½³æ¸¸è§ˆæ—¶æ®µ
   - å¤©æ°”ã€äº¤é€šç­‰å®ç”¨ä¿¡æ¯

è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„è¯­è°ƒæä¾›åˆ†æï¼Œé‡ç‚¹çªå‡ºå¯æ“ä½œçš„å»ºè®®ã€‚"""
        
        # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ†æ
        response = client.chat.completions.create(
            model=PLANNING_MODEL,  # ä½¿ç”¨å¼ºå¤§æ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æ
            messages=[{"role": "user", "content": analysis_prompt}]
        )
        
        analysis_result = response.choices[0].message.content.strip()
        
        return {
            "success": True,
            "response": analysis_result
        }
        
    except Exception as e:
        logging.error(f"è¡Œç¨‹åˆ†æå¤±è´¥: {e}", exc_info=True)
        return {
            "success": False,
            "error": f"è¡Œç¨‹åˆ†æå¤±è´¥: {str(e)}"
        }

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()



# é…ç½®æ—¥å¿—
beijing_tz = pytz.timezone('Asia/Shanghai')

# è®¾ç½®æ—¥å¿—è®°å½•å™¨ä½¿ç”¨åŒ—äº¬æ—¶åŒº
class BeijingTimeFormatter(logging.Formatter):
    def formatTime(self, record, datefmt=None):
        dt = datetime.datetime.fromtimestamp(record.created, tz=beijing_tz)
        if datefmt:
            return dt.strftime(datefmt)
        return dt.strftime("%Y-%m-%d %H:%M:%S %z")

# é¿å…é‡å¤é…ç½®æ—¥å¿—å¤„ç†å™¨
root_logger = logging.getLogger()
if not root_logger.handlers:  # åªæœ‰åœ¨æ²¡æœ‰å¤„ç†å™¨æ—¶æ‰é…ç½®
    # é…ç½®æ§åˆ¶å°æ—¥å¿—å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(BeijingTimeFormatter('%(asctime)s %(levelname)s %(message)s'))
    
    # é…ç½®æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨
    log_dir = os.path.join(os.path.dirname(__file__), 'logs')
    os.makedirs(log_dir, exist_ok=True)
    now = datetime.datetime.now(beijing_tz)
    log_date = now.strftime('%Y%m%d-%H%M%S')
    log_file_path = os.path.join(log_dir, f'log_{log_date}.log')
    file_handler = logging.FileHandler(log_file_path, encoding='utf-8', mode='a')
    file_handler.setFormatter(BeijingTimeFormatter('%(asctime)s %(levelname)s %(message)s', "%Y-%m-%d %H:%M:%S %z"))
    file_handler.setLevel(logging.DEBUG)
    
    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger.setLevel(logging.DEBUG)
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)
    
    # werkzeugæ—¥å¿—è®¾ç½®ï¼ˆä¸æ·»åŠ é‡å¤çš„å¤„ç†å™¨ï¼‰
    flask_logger = logging.getLogger('werkzeug')
    flask_logger.setLevel(logging.INFO)
    # ä¸å†æ·»åŠ console_handlerï¼Œå› ä¸ºå®ƒä¼šç»§æ‰¿root_loggerçš„å¤„ç†å™¨

# æ·»åŠ è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
@app.before_request
def log_request_info():
    root_logger.debug('Request: %s %s', request.method, request.url)
    if request.method in ('POST', 'PUT', 'PATCH') and request.is_json:
        try:
            root_logger.debug('Request JSON: %s', request.get_json())
        except Exception as e:
            root_logger.warning(f'Failed to parse JSON: {e}')

@app.after_request
def log_response_info(response):
    root_logger.debug('Response: %s', response.status)
    return response

http_client = httpx.Client(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    timeout=httpx.Timeout(60.0)
)

client = OpenAI(
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    api_key=os.environ.get("ARK_API_KEY")
)


mcp_client = MCPClientWrapper()

# å…¨å±€æ¨¡å‹ç¼“å­˜ - æ™ºèƒ½åŠ è½½ç­–ç•¥
# å¯åŠ¨æ—¶æ£€æŸ¥å‘é‡ç¼“å­˜ï¼š
# 1. æœ‰ç¼“å­˜ç´¢å¼•ï¼šå¼‚æ­¥é¢„åŠ è½½æ¨¡å‹ï¼Œæå‡åç»­æŸ¥è¯¢é€Ÿåº¦
# 2. æ— ç¼“å­˜ç´¢å¼•ï¼šä¸åŠ è½½æ¨¡å‹ï¼Œç¦ç”¨æ–‡æ¡£æŸ¥è¯¢åŠŸèƒ½
_embedding_model = None
_embedding_cache = None
_model_loading = False  # æ ‡è®°æ¨¡å‹æ˜¯å¦æ­£åœ¨å¼‚æ­¥åŠ è½½
_has_vector_cache = False  # æ ‡è®°æ˜¯å¦å­˜åœ¨å‘é‡ç¼“å­˜
_async_model_task = None  # å¼‚æ­¥åŠ è½½ä»»åŠ¡
_async_model_task = None  # å¼‚æ­¥åŠ è½½ä»»åŠ¡

def check_vector_cache_exists():
    """
    æ£€æŸ¥å‘é‡ç¼“å­˜æ˜¯å¦å­˜åœ¨
    
    Returns:
        bool: Trueè¡¨ç¤ºå­˜åœ¨å‘é‡ç¼“å­˜ï¼ŒFalseè¡¨ç¤ºä¸å­˜åœ¨
    """
    cache_file = os.path.join(app.config['EMBEDDINGS_FOLDER'], 'embedding_cache.pkl')
    return os.path.exists(cache_file) and os.path.getsize(cache_file) > 0

def async_load_model():
    """
    å¼‚æ­¥åŠ è½½æ¨¡å‹ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
    """
    global _embedding_model, _model_loading
    
    if _embedding_model is not None or _model_loading:
        return  # æ¨¡å‹å·²åŠ è½½æˆ–æ­£åœ¨åŠ è½½
    
    try:
        _model_loading = True
        logging.info("ğŸ”„ å¼€å§‹å¼‚æ­¥åŠ è½½åµŒå…¥æ¨¡å‹...")
        
        # åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½æ¨¡å‹
        from sentence_transformers import SentenceTransformer
        import time
        
        MODEL_NAME = 'Qwen/Qwen3-Embedding-0.6B'
        start_time = time.time()
        
        # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
        cache_dir = os.path.join(os.path.dirname(__file__), 'model_cache')
        os.makedirs(cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        _embedding_model = SentenceTransformer(
            MODEL_NAME, 
            cache_folder=cache_dir,
            device='cpu'
        )
        
        load_time = time.time() - start_time
        logging.info(f"âœ… æ¨¡å‹å¼‚æ­¥åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
        
        # è¿›è¡Œä¸€æ¬¡å°æµ‹è¯•ç¡®ä¿æ¨¡å‹å·¥ä½œæ­£å¸¸
        test_embedding = _embedding_model.encode(["æµ‹è¯•æ–‡æœ¬"], show_progress_bar=False)
        logging.info(f"ğŸ§ª å¼‚æ­¥åŠ è½½çš„æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œembeddingç»´åº¦: {test_embedding.shape}")
        
    except Exception as e:
        logging.error(f"âŒ å¼‚æ­¥åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
        _embedding_model = None
    finally:
        _model_loading = False

def smart_startup_initialization():
    """
    æ™ºèƒ½å¯åŠ¨åˆå§‹åŒ–ï¼šæ ¹æ®å‘é‡ç¼“å­˜çŠ¶æ€å†³å®šæ˜¯å¦é¢„åŠ è½½æ¨¡å‹
    """
    global _has_vector_cache, _async_model_task
    
    try:
        # æ£€æŸ¥å‘é‡ç¼“å­˜æ˜¯å¦å­˜åœ¨
        _has_vector_cache = check_vector_cache_exists()
        
        if _has_vector_cache:
            logging.info("ğŸ” æ£€æµ‹åˆ°å‘é‡ç¼“å­˜æ–‡ä»¶ï¼Œå¯åŠ¨å¼‚æ­¥æ¨¡å‹åŠ è½½...")
            # å­˜åœ¨å‘é‡ç¼“å­˜ï¼Œå¼‚æ­¥é¢„åŠ è½½æ¨¡å‹
            import threading
            _async_model_task = threading.Thread(target=async_load_model, daemon=True)
            _async_model_task.start()
        else:
            logging.info("ğŸ“­ æœªæ£€æµ‹åˆ°å‘é‡ç¼“å­˜æ–‡ä»¶ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½ï¼Œæ–‡æ¡£æŸ¥è¯¢åŠŸèƒ½å°†è¢«ç¦ç”¨")
            
    except Exception as e:
        logging.error(f"å¯åŠ¨åˆå§‹åŒ–æ£€æŸ¥å¤±è´¥: {str(e)}")
        _has_vector_cache = False

def get_embedding_model():
    """è·å–æˆ–åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆæ™ºèƒ½åŠ è½½å•ä¾‹æ¨¡å¼ï¼‰"""
    global _embedding_model, _model_loading, _async_model_task
    
    # å¦‚æœæ¨¡å‹æ­£åœ¨å¼‚æ­¥åŠ è½½ï¼Œç­‰å¾…å®Œæˆ
    if _model_loading and _async_model_task and _async_model_task.is_alive():
        logging.info("â³ ç­‰å¾…å¼‚æ­¥æ¨¡å‹åŠ è½½å®Œæˆ...")
        _async_model_task.join(timeout=30)  # æœ€å¤šç­‰å¾…30ç§’
        
    if _embedding_model is None:
        try:
            # å°è¯•å¯¼å…¥SentenceTransformerï¼Œå¦‚æœå¤±è´¥å°±è·³è¿‡
            from sentence_transformers import SentenceTransformer
            
            MODEL_NAME = 'Qwen/Qwen3-Embedding-0.6B'
            import time
            start_time = time.time()
            logging.info(f"ğŸš€ æŒ‰éœ€åŠ è½½åµŒå…¥æ¨¡å‹: {MODEL_NAME}")
            logging.info("â³ æ­£åœ¨åˆå§‹åŒ–SentenceTransformeræ¨¡å‹ï¼Œè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´...")
            
            # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
            import os
            cache_dir = os.path.join(os.path.dirname(__file__), 'model_cache')
            os.makedirs(cache_dir, exist_ok=True)
            
            # åˆå§‹åŒ–æ¨¡å‹ï¼Œè®¾ç½®ç¼“å­˜ç›®å½•
            _embedding_model = SentenceTransformer(
                MODEL_NAME, 
                cache_folder=cache_dir,
                device='cpu'  # æ˜ç¡®æŒ‡å®šä½¿ç”¨CPUï¼Œé¿å…CUDAç›¸å…³é—®é¢˜
            )
            
            load_time = time.time() - start_time
            logging.info(f"âœ… SentenceTransformeræ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}ç§’")
            
            # è¿›è¡Œä¸€æ¬¡å°æµ‹è¯•ç¡®ä¿æ¨¡å‹å·¥ä½œæ­£å¸¸
            test_embedding = _embedding_model.encode(["æµ‹è¯•æ–‡æœ¬"], show_progress_bar=False)
            logging.info(f"ğŸ§ª æ¨¡å‹æµ‹è¯•æˆåŠŸï¼Œembeddingç»´åº¦: {test_embedding.shape}")
            
        except Exception as e:
            logging.error(f"âŒ åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            # åœ¨æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œå¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¿”å›None
            _embedding_model = None
            raise e
    else:
        logging.debug("â™»ï¸ ä½¿ç”¨å·²ç¼“å­˜çš„åµŒå…¥æ¨¡å‹å®ä¾‹")
    
    return _embedding_model

def load_embedding_cache():
    """
    åŠ è½½å‘é‡ç¼“å­˜ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        dict: ç¼“å­˜æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
    """
    global _embedding_cache
    cache_file = os.path.join(app.config['EMBEDDINGS_FOLDER'], 'embedding_cache.pkl')
    
    if _embedding_cache is None:
        if not os.path.exists(cache_file):
            logging.warning(f"å‘é‡ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
            return None
        
        try:
            logging.info(f"é¦–æ¬¡åŠ è½½å‘é‡ç¼“å­˜æ–‡ä»¶: {cache_file}")
            with open(cache_file, 'rb') as f:
                _embedding_cache = pickle.load(f)
            _embedding_cache['_mtime'] = os.path.getmtime(cache_file)
            logging.info(f"å‘é‡ç¼“å­˜åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(_embedding_cache.get('texts', []))} ä¸ªæ–‡æ¡£")
        except Exception as e:
            logging.error(f"åŠ è½½å‘é‡ç¼“å­˜å¤±è´¥: {str(e)}")
            return None
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦æœ‰æ›´æ–°
    elif os.path.exists(cache_file):
        try:
            cache_mtime = os.path.getmtime(cache_file)
            if _embedding_cache.get('_mtime', 0) < cache_mtime:
                logging.info("æ£€æµ‹åˆ°å‘é‡ç¼“å­˜æ–‡ä»¶æ›´æ–°ï¼Œé‡æ–°åŠ è½½")
                with open(cache_file, 'rb') as f:
                    _embedding_cache = pickle.load(f)
                    _embedding_cache['_mtime'] = cache_mtime
        except Exception as e:
            logging.error(f"é‡æ–°åŠ è½½å‘é‡ç¼“å­˜å¤±è´¥: {str(e)}")
            return None
    else:
        # ç¼“å­˜æ–‡ä»¶è¢«åˆ é™¤
        logging.warning("å‘é‡ç¼“å­˜æ–‡ä»¶å·²è¢«åˆ é™¤")
        _embedding_cache = None
        return None
    
    return _embedding_cache

def is_document_query_available():
    """
    æ£€æŸ¥æ–‡æ¡£æŸ¥è¯¢åŠŸèƒ½æ˜¯å¦å¯ç”¨
    
    Returns:
        bool: Trueè¡¨ç¤ºå¯ä»¥ä½¿ç”¨æ–‡æ¡£æŸ¥è¯¢ï¼ŒFalseè¡¨ç¤ºåŠŸèƒ½è¢«ç¦ç”¨
    """
    global _has_vector_cache
    return _has_vector_cache and check_vector_cache_exists()

def get_model_status():
    """
    è·å–æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    
    Returns:
        dict: åŒ…å«æ¨¡å‹çŠ¶æ€çš„ä¿¡æ¯
    """
    global _embedding_model, _model_loading, _has_vector_cache
    
    return {
        'has_vector_cache': _has_vector_cache,
        'model_loaded': _embedding_model is not None,
        'model_loading': _model_loading,
        'document_query_available': is_document_query_available()
    }

def clear_embedding_cache():
    """æ¸…é™¤å‘é‡ç¼“å­˜ï¼ˆç”¨äºæ›´æ–°ç´¢å¼•åï¼‰"""
    global _embedding_cache, _has_vector_cache
    _embedding_cache = None
    
    # é‡æ–°æ£€æŸ¥å‘é‡ç¼“å­˜çŠ¶æ€
    _has_vector_cache = check_vector_cache_exists()
    
    logging.info("å‘é‡ç¼“å­˜å·²æ¸…é™¤")

def perform_rag_query(query, original_question=None, top_k=5, similarity_threshold=0.3):
    """
    æ‰§è¡ŒRAGæŸ¥è¯¢çš„æ ¸å¿ƒå‡½æ•°
    
    Args:
        query: ä¼˜åŒ–åçš„æ£€ç´¢å…³é”®è¯ï¼ˆç”±å¤§æ¨¡å‹ä»ç”¨æˆ·é—®é¢˜ä¸­æå–ï¼‰
        original_question: ç”¨æˆ·çš„åŸå§‹å®Œæ•´é—®é¢˜ï¼ˆç”¨äºç”Ÿæˆå›ç­”ï¼‰
        top_k: è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£æ•°é‡
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
    
    Returns:
        tuple: (æ˜¯å¦æˆåŠŸ, å›ç­”å†…å®¹, ç›¸å…³æ–‡æ¡£åˆ—è¡¨)
    """
    try:
        # 0. é¦–å…ˆæ£€æŸ¥æ–‡æ¡£æŸ¥è¯¢åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not is_document_query_available():
            return False, "âŒ æ–‡æ¡£æŸ¥è¯¢åŠŸèƒ½å½“å‰ä¸å¯ç”¨ã€‚ç³»ç»Ÿæœªæ£€æµ‹åˆ°å‘é‡ç´¢å¼•æ–‡ä»¶ï¼Œè¯·è”ç³»ç®¡ç†å‘˜ä¸Šä¼ æ–‡æ¡£å¹¶ç”Ÿæˆç´¢å¼•åå†ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚", []
        
        # 1. æ£€æŸ¥å‘é‡ç¼“å­˜æ˜¯å¦å­˜åœ¨
        cache_data = load_embedding_cache()
        if cache_data is None:
            return False, "âŒ çŸ¥è¯†åº“ç´¢å¼•æœªç”Ÿæˆã€‚è¯·è”ç³»ç®¡ç†å‘˜ä¸Šä¼ æ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡ç´¢å¼•åå†ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚", []
        
        # éªŒè¯ç¼“å­˜æ•°æ®å®Œæ•´æ€§
        if not all(key in cache_data for key in ['texts', 'embeddings', 'meta']):
            return False, "âŒ çŸ¥è¯†åº“ç´¢å¼•æ•°æ®ä¸å®Œæ•´ã€‚è¯·è”ç³»ç®¡ç†å‘˜é‡æ–°ç”Ÿæˆå‘é‡ç´¢å¼•ã€‚", []
        
        texts = cache_data['texts']
        embeddings = cache_data['embeddings']
        meta = cache_data['meta']
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        if not texts or embeddings is None or not meta:
            return False, "âŒ çŸ¥è¯†åº“ä¸ºç©ºã€‚è¯·è”ç³»ç®¡ç†å‘˜ä¸Šä¼ æ–‡æ¡£å¹¶ç”Ÿæˆå‘é‡ç´¢å¼•ã€‚", []
        
        # ç¡®ä¿embeddingsæ˜¯numpyæ•°ç»„
        import numpy as np
        if not isinstance(embeddings, np.ndarray):
            embeddings = np.array(embeddings)
        
        logging.info(f"æ•°æ®æ£€æŸ¥å®Œæˆ - texts: {len(texts)}, embeddings: {embeddings.shape}, meta: {len(meta)}")
        
        # 2. å¯¹ä¼˜åŒ–åçš„æŸ¥è¯¢å…³é”®è¯è¿›è¡Œå‘é‡åŒ–ï¼ˆä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹ï¼‰
        try:
            model = get_embedding_model()
            query_embedding = model.encode([query], show_progress_bar=False)
            logging.info(f"æŸ¥è¯¢å‘é‡åŒ–å®Œæˆï¼Œquery: {query}, embedding shape: {query_embedding.shape}")
        except Exception as e:
            logging.error(f"æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥: {str(e)}")
            return False, f"âŒ å‘é‡åŒ–æŸ¥è¯¢å¤±è´¥: {str(e)}", []
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦å¹¶ç­›é€‰ç›¸å…³æ–‡æ¡£
        logging.info(f"è®¡ç®—ç›¸ä¼¼åº¦ï¼Œembeddings shape: {embeddings.shape}")
        similarities = cosine_similarity(query_embedding, embeddings)[0]
        logging.info(f"ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆï¼Œsimilarities shape: {similarities.shape}")
        
        # è·å–ç›¸ä¼¼åº¦æœ€é«˜çš„æ–‡æ¡£ç´¢å¼•
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        # ç¡®ä¿ç´¢å¼•ä¸ºintç±»å‹
        top_indices = [int(idx) for idx in top_indices]
        
        # è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
        relevant_docs = []
        relevant_texts = []
        for idx in top_indices:
            similarity_score = float(similarities[idx])
            if similarity_score >= similarity_threshold:
                relevant_docs.append({
                    'text': texts[idx],
                    'meta': meta[idx],
                    'similarity': similarity_score
                })
                relevant_texts.append(texts[idx])
        
        if not relevant_docs:
            return False, "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„é—®é¢˜", []
        
        # 4. æ„å»ºç›¸å…³æ–‡æ¡£å†…å®¹ï¼Œä½†ä¸ç”Ÿæˆå›ç­”ï¼ˆç•™ç»™æœ€ç»ˆå›å¤é˜¶æ®µå¤„ç†ï¼‰
        context_docs = []
        for doc in relevant_docs[:3]:  # åªä½¿ç”¨å‰3ä¸ªæœ€ç›¸å…³çš„
            context_docs.append({
                'text': doc['text'],
                'source': doc['meta']['source'],
                'similarity': doc['similarity']
            })
        
        # 5. æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹ç”¨äºè¿”å›
        context_text = "\n\n".join([doc['text'] for doc in relevant_docs[:3]])
        
        # 6. æ·»åŠ ä¿¡æ¯æ¥æºæ ‡æ³¨
        sources = list(set([doc['meta']['source'] for doc in relevant_docs]))
        source_text = f"\n\nğŸ“š **ä¿¡æ¯æ¥æº**: {', '.join(sources)}"
        
        # è¿”å›æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼Œè€Œä¸æ˜¯ç”Ÿæˆçš„å›ç­”
        formatted_context = f"**æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š**\n\n{context_text}{source_text}"
        
        return True, formatted_context, relevant_docs
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logging.error(f"RAGæŸ¥è¯¢å¤±è´¥: {str(e)}")
        logging.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {error_details}")
        return False, f"âŒ æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", []

def split_long_content(content, source, name, index, chunk_type, max_chars=500, overlap=50, use_semantic=False):
    """
    å°†é•¿æ–‡æœ¬å†…å®¹åˆ†å‰²æˆå°å—ã€‚
    æ”¯æŒè¯­ä¹‰åˆ†å—å’ŒåŸºäºå­—ç¬¦çš„åˆ†å—ã€‚
    """
    if len(content) <= max_chars:
        return [{
            'source': source,
            'name': f'{name} - {chunk_type.capitalize()} {index+1}',
            'description': content,
            'path': str(Path(app.config['UPLOAD_FOLDER']) / name),
            'chunk_id': f'{source}_{chunk_type}_{index}',
            'chunk_type': chunk_type,
            'chunk_index': index,
            'metadata': {
                'original_length': len(content),
                'is_split': False
            }
        }]
    
    chunks = []
    start = 0
    chunk_counter = 0
    
    if use_semantic:
        sentences = []
        for sent in content.replace('ã€‚', '.').replace('ï¼Ÿ', '?').replace('ï¼', '!').split('.'):
            if sent.strip():
                for subsent in sent.split('?'):
                    if subsent.strip():
                        for final_sent in subsent.split('!'):
                            if final_sent.strip():
                                sentences.append(final_sent.strip() + '.')
        
        if not sentences:
            use_semantic = False
        else:
            current_chunk = ''
            for sentence in sentences:
                if len(current_chunk) + len(sentence) > max_chars and current_chunk:
                    chunks.append({
                        'source': source,
                        'name': f'{name} - {chunk_type.capitalize()} {index+1} - Part {chunk_counter+1}',
                        'description': current_chunk,
                        'path': str(Path(app.config['UPLOAD_FOLDER']) / name),
                        'chunk_id': f'{source}_{chunk_type}_{index}_part_{chunk_counter}',
                        'chunk_type': chunk_type,
                        'chunk_index': f'{index}.{chunk_counter}',
                        'metadata': {
                            'original_length': len(current_chunk),
                            'is_split': True,
                            'split_method': 'semantic'
                        }
                    })
                    words = current_chunk.split()
                    if len(words) > 5:
                        overlap_text = ' '.join(words[-min(len(words)//3, 20):])
                        current_chunk = overlap_text + ' '
                    else:
                        current_chunk = ''
                    chunk_counter += 1
                current_chunk += sentence + ' '
            
            if current_chunk.strip():
                chunks.append({
                    'source': source,
                    'name': f'{name} - {chunk_type.capitalize()} {index+1} - Part {chunk_counter+1}',
                    'description': current_chunk.strip(),
                    'path': str(Path(app.config['UPLOAD_FOLDER']) / name),
                    'chunk_id': f'{source}_{chunk_type}_{index}_part_{chunk_counter}',
                    'chunk_type': chunk_type,
                    'chunk_index': f'{index}.{chunk_counter}',
                    'metadata': {
                        'original_length': len(current_chunk),
                        'is_split': True,
                        'split_method': 'semantic'
                    }
                })
    
    if not use_semantic:
        while start < len(content):
            end = start + max_chars
            if end < len(content):
                end = content.rfind('.', start, end)
                if end == -1 or end <= start:
                    # å¦‚æœæ²¡æ‰¾åˆ°å¥å·ï¼Œå°è¯•æ‰¾æ¢è¡Œç¬¦
                    end = content.rfind("\n", start, end)
                if end == -1 or end <= start:
                    # å¦‚æœæ²¡æ‰¾åˆ°æ¢è¡Œç¬¦ï¼Œå°è¯•æ‰¾é€—å·
                    end = content.rfind(",", start, end)
                if end == -1 or end <= start:
                    # å¦‚æœæ²¡æ‰¾åˆ°ä»»ä½•æ ‡ç‚¹ï¼Œå°±ç›´æ¥åœ¨è¯è¾¹ç•Œå¤„æˆªæ–­
                    end = content.rfind(" ", start, start + max_chars)
                if end == -1 or end <= start:
                    # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°±ç›´æ¥åœ¨æœ€å¤§é•¿åº¦å¤„æˆªæ–­
                    end = start + max_chars
            
            chunk_content = content[start:end].strip()
            if chunk_content:
                chunks.append({
                    'source': source,
                    'name': f'{name} - {chunk_type.capitalize()} {index+1} - Part {chunk_counter+1}',
                    'description': chunk_content,
                    'path': str(Path(app.config['UPLOAD_FOLDER']) / name),
                    'chunk_id': f'{source}_{chunk_type}_{index}_part_{chunk_counter}',
                    'chunk_type': chunk_type,
                    'chunk_index': f'{index}.{chunk_counter}',
                    'metadata': {
                        'original_length': len(chunk_content),
                        'is_split': True,
                        'split_method': 'character'
                    }
                })
            if end < len(content):
                start = max(start + max_chars - overlap, end - overlap)
            else:
                start = end
            chunk_counter += 1
    
    return chunks

def load_documents(chunk_size=500, chunk_overlap=50, use_semantic_chunking=False):
    """åŠ è½½å¹¶å¤„ç†æ‰€æœ‰æ–‡æ¡£"""
    docs = []
    
    # å¤„ç† JSON æ–‡ä»¶
    for file in Path(app.config['UPLOAD_FOLDER']).glob('*.json'):
        with open(file, 'r', encoding='utf-8') as f:
            content = json.load(f)
            if isinstance(content, dict) and 'records' in content:
                content = content['records']
            if isinstance(content, list):
                for item in content:
                    if not isinstance(item, dict):
                        item = {'content': str(item)}
                    item['source'] = file.stem
                    item['path'] = str(file)
                    docs.append(item)
            else:
                doc = {'content': str(content), 'source': file.stem, 'path': str(file)}
                docs.append(doc)
    
    # å¤„ç† Excel æ–‡ä»¶
    for file in Path(app.config['UPLOAD_FOLDER']).glob('*.xlsx'):
        try:
            df = pd.read_excel(file)
            for index, row in df.iterrows():
                content = ' '.join([f"{col}: {val}" for col, val in row.items() if pd.notna(val)])
                doc = {
                    'source': file.stem,
                    'name': f'{file.stem} - è¡Œ {index + 1}',
                    'description': content,
                    'path': str(file),
                    'chunk_id': f'{file.stem}_row_{index}',
                    'chunk_type': 'excel_row',
                    'chunk_index': index,
                    'metadata': {
                        'row_number': index + 1,
                        'columns': list(row.index)
                    }
                }
                docs.append(doc)
        except Exception as e:
            logging.warning(f'è¯»å–Excelæ–‡ä»¶ {file} æ—¶å‡ºé”™ï¼š{e}')
    
    # å¤„ç† Markdown å’Œ HTML æ–‡ä»¶
    for ext in ['*.md', '*.html']:
        for file in Path(app.config['UPLOAD_FOLDER']).glob(ext):
            with open(file, 'r', encoding='utf-8') as f:
                doc = {
                    'source': file.stem,
                    'name': file.name,
                    'description': f.read(),
                    'path': str(file)
                }
                docs.append(doc)
    
    # å¤„ç† Word æ–‡æ¡£
    try:
        from docx import Document
        for file in Path(app.config['UPLOAD_FOLDER']).glob('*.docx'):
            doc = Document(file)
            meta_info = {
                'title': doc.core_properties.title or file.stem,
                'author': doc.core_properties.author or 'æœªçŸ¥ä½œè€…',
                'created': str(doc.core_properties.created or 'æœªçŸ¥æ—¶é—´'),
                'last_modified': str(doc.core_properties.modified or 'æœªçŸ¥æ—¶é—´')
            }
            docs.append({
                'source': file.stem,
                'name': f'{file.name} - å…ƒæ•°æ®',
                'description': f'æ ‡é¢˜: {meta_info["title"]}\nä½œè€…: {meta_info["author"]}\nåˆ›å»ºæ—¶é—´: {meta_info["created"]}\nä¿®æ”¹æ—¶é—´: {meta_info["last_modified"]}',
                'path': str(file),
                'chunk_id': f'{file.stem}_metadata',
                'chunk_type': 'metadata',
                'chunk_index': 0,
                'metadata': meta_info
            })
            for i, para in enumerate(doc.paragraphs):
                content = para.text.strip()
                if content:
                    chunks = split_long_content(content, file.stem, file.name, i, 'paragraph', 
                                             max_chars=chunk_size, overlap=chunk_overlap, 
                                             use_semantic=use_semantic_chunking)
                    docs.extend(chunks)
    except ImportError:
        logging.warning('è­¦å‘Šï¼šæœªå®‰è£… python-docx æ¨¡å—ï¼Œæ— æ³•åŠ è½½ Word æ–‡æ¡£')
    
    # å¤„ç† PDF æ–‡ä»¶
    try:
        import PyPDF2
        for file in Path(app.config['UPLOAD_FOLDER']).glob('*.pdf'):
            with open(file, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                meta_info = {}
                if pdf_reader.metadata:
                    meta_info = {
                        'title': pdf_reader.metadata.get('/Title', file.stem),
                        'author': pdf_reader.metadata.get('/Author', 'æœªçŸ¥ä½œè€…'),
                        'created': pdf_reader.metadata.get('/CreationDate', 'æœªçŸ¥æ—¶é—´'),
                        'producer': pdf_reader.metadata.get('/Producer', 'æœªçŸ¥ç”Ÿäº§è€…')
                    }
                else:
                    meta_info = {
                        'title': file.stem,
                        'author': 'æœªçŸ¥ä½œè€…',
                        'pages': len(pdf_reader.pages)
                    }
                docs.append({
                    'source': file.stem,
                    'name': f'{file.name} - å…ƒæ•°æ®',
                    'description': f'æ ‡é¢˜: {meta_info.get("title")}\nä½œè€…: {meta_info.get("author")}\né¡µæ•°: {len(pdf_reader.pages)}',
                    'path': str(file),
                    'chunk_id': f'{file.stem}_metadata',
                    'chunk_type': 'metadata',
                    'chunk_index': 0,
                    'metadata': meta_info
                })
                for i, page in enumerate(pdf_reader.pages):
                    content = page.extract_text().strip()
                    if content:
                        chunks = split_long_content(content, file.stem, file.name, i, 'page',
                                                 max_chars=chunk_size, overlap=chunk_overlap,
                                                 use_semantic=use_semantic_chunking)
                        docs.extend(chunks)
    except ImportError:
        logging.warning('è­¦å‘Šï¼šæœªå®‰è£… PyPDF2 æ¨¡å—ï¼Œæ— æ³•åŠ è½½ PDF æ–‡æ¡£')
    except Exception as e:
        logging.warning(f'è­¦å‘Šï¼šåŠ è½½ PDF æ–‡æ¡£æ—¶å‡ºé”™ï¼š{e}')
    
    return docs

def format_doc(doc):
    """æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹ç”¨äºå‘é‡åŒ–"""
    source = doc.get('source', 'æœªçŸ¥æ¥æº')
    name = doc.get('name', 'æœªçŸ¥åç§°')
    chunk_type = doc.get('chunk_type', '')
    
    # ç‰¹æ®Šå¤„ç†Excelæ•°æ®
    if chunk_type == 'excel_row':
        description = doc.get('description', 'æš‚æ— æè¿°')
        row_number = doc.get('metadata', {}).get('row_number', 'æœªçŸ¥è¡Œå·')
        return f'[{source}] ç¬¬{row_number}è¡Œï¼š{description}'
    
    # å¤„ç†å…¶ä»–ç±»å‹çš„æ–‡æ¡£
    description = (
        doc.get('description') or
        doc.get('content') or
        doc.get('recommendation_reason') or
        doc.get('features') or
        doc.get('tags') or
        'æš‚æ— æè¿°'
    )
    if isinstance(description, list):
        description = 'ï¼Œ'.join(description)
    return f'[{source}] {name}ï¼š{description}'

def get_docs_hash(docs):
    """è®¡ç®—æ–‡æ¡£é›†åˆçš„å“ˆå¸Œå€¼"""
    m = hashlib.md5()
    for doc in docs:
        m.update(format_doc(doc).encode('utf-8'))
    return m.hexdigest()

def update_embeddings(file_path=None):
    """å¤„ç†æ–‡æ¡£å¹¶æ›´æ–°å‘é‡ç´¢å¼•"""
    try:
        # åŠ è½½æ‰€æœ‰æ–‡æ¡£
        docs = load_documents(chunk_size=500, chunk_overlap=50, use_semantic_chunking=True)
        if not docs:
            logging.warning('æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£')
            return False
            
        # æ ¼å¼åŒ–æ–‡æ¡£
        texts = [format_doc(doc) for doc in docs]
        
        # ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆæŒ‰éœ€åŠ è½½æ¨¡å‹ï¼‰
        try:
            logging.info(f"ğŸ“š å¼€å§‹ä¸º {len(texts)} ä¸ªæ–‡æ¡£ç”Ÿæˆå‘é‡åµŒå…¥...")
            model = get_embedding_model()  # è¿™é‡Œä¼šæŒ‰éœ€åŠ è½½æ¨¡å‹
            logging.info(f"ğŸ”¢ æ­£åœ¨ç”Ÿæˆå‘é‡åµŒå…¥...")
            embeddings = model.encode(texts, show_progress_bar=True)
            logging.info(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆï¼Œembedding shape: {embeddings.shape}")
        except Exception as e:
            logging.error(f"âŒ å‘é‡ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise e
        
        # è®¡ç®—æ–‡æ¡£å“ˆå¸Œå€¼
        docs_hash = get_docs_hash(docs)
        
        # ä¿å­˜ç¼“å­˜
        cache_data = {
            'hash': docs_hash,
            'texts': texts,
            'embeddings': embeddings,
            'meta': [{'source': doc.get('source'), 'name': doc.get('name', ''), 'path': doc.get('path', '')} for doc in docs]
        }
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        cache_file = os.path.join(app.config['EMBEDDINGS_FOLDER'], 'embedding_cache.pkl')
        with open(cache_file, 'wb') as f:
            pickle.dump(cache_data, f)
        
        logging.info('âœ… Embedding å·²ç¼“å­˜åˆ°æœ¬åœ°')
        
        # æ¸…é™¤å†…å­˜ä¸­çš„ç¼“å­˜ï¼Œä¸‹æ¬¡æŸ¥è¯¢æ—¶ä¼šé‡æ–°åŠ è½½
        clear_embedding_cache()
        
        # æ›´æ–°å…¨å±€çŠ¶æ€ï¼šç°åœ¨æœ‰å‘é‡ç¼“å­˜äº†
        global _has_vector_cache
        _has_vector_cache = True
        logging.info("ğŸ”„ å‘é‡ç¼“å­˜çŠ¶æ€å·²æ›´æ–°ï¼Œæ–‡æ¡£æŸ¥è¯¢åŠŸèƒ½ç°å·²å¯ç”¨")
        
        return True
        
    except Exception as e:
        logging.error(f'Error processing documents: {str(e)}')
        raise e

# ç™»å½•ç›¸å…³è·¯ç”±å·²åœ¨ç¬¬134-167è¡Œå®šä¹‰ï¼Œè¿™é‡Œåˆ é™¤é‡å¤å®šä¹‰

# æ–‡æ¡£ç®¡ç†è·¯ç”±
@app.route('/document')
@login_required
def document_management():
    files = []
    for filename in os.listdir(app.config['UPLOAD_FOLDER']):
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        if os.path.isfile(file_path):
            file_size = os.path.getsize(file_path)
            # è½¬æ¢æ–‡ä»¶å¤§å°ä¸ºå¯è¯»æ ¼å¼
            if file_size < 1024:
                size_str = f"{file_size} B"
            elif file_size < 1024 * 1024:
                size_str = f"{file_size/1024:.1f} KB"
            else:
                size_str = f"{file_size/(1024*1024):.1f} MB"
                
            files.append({
                'id': filename,
                'filename': filename,
                'size': size_str,
                'type': os.path.splitext(filename)[1].lstrip('.').upper() or 'Unknown',
                'modified': datetime.datetime.fromtimestamp(
                    os.path.getmtime(file_path)
                ).strftime('%Y-%m-%d %H:%M:%S')
            })
    return render_template('doc.html', documents=files)

# åŸºæœ¬è®¾ç½®ç®¡ç†è·¯ç”±
@app.route('/settings')
@login_required
def basic_settings():
    """åŸºæœ¬è®¾ç½®ç®¡ç†é¡µé¢"""
    # å¤ç”¨doc.htmlæ¨¡æ¿ï¼Œä½†è®¾ç½®é»˜è®¤æ˜¾ç¤ºåŸºæœ¬è®¾ç½®é¡µé¢
    return render_template('doc.html', documents=[], default_view='settings')

@app.route('/upload', methods=['POST'])
@login_required
@unified_error_handler('json')
def upload_file():
    """æ–‡ä»¶ä¸Šä¼ è·¯ç”±ï¼ˆæ”¯æŒå‘é‡ç´¢å¼•æ›´æ–°ï¼‰"""
    # éªŒè¯ä¸Šä¼ è¯·æ±‚
    success, error_info, file = validate_upload_request(request)
    if not success:
        return success, error_info
    
    # å¤„ç†æ–‡ä»¶ä¸Šä¼ ï¼ˆä¸è‡ªåŠ¨æ›´æ–°ç´¢å¼•ï¼‰
    success, info, filename = handle_file_upload_core(
        file, app.config['UPLOAD_FOLDER'], update_index=False
    )
    
    if success:
        return jsonify({'success': True, 'msg': info['message'], 'filename': filename})
    else:
        return success, info

@app.route('/delete/<filename>')
@login_required
@unified_error_handler('flash')
def delete_file(filename):
    """åˆ é™¤æ–‡ä»¶è·¯ç”±ï¼ˆä½¿ç”¨flashæ¶ˆæ¯ï¼‰"""
    return handle_file_deletion_core(filename, app.config['UPLOAD_FOLDER'])

@app.route('/download/<filename>')
@login_required
@unified_error_handler('flash')
def download_file(filename):
    """ä¸‹è½½æ–‡ä»¶è·¯ç”±"""
    try:
        # ä½¿ç”¨å®‰å…¨çš„æ–‡ä»¶è·¯å¾„æ„å»º
        file_path = safe_file_path(filename, app.config['UPLOAD_FOLDER'])
        if not os.path.exists(file_path):
            return ErrorHandler.handle_error(
                ErrorHandler.NOT_FOUND_ERROR,
                'æ–‡ä»¶ä¸å­˜åœ¨'
            )
        return send_file(file_path, as_attachment=True)
    except ValueError as e:
        return ErrorHandler.handle_error(
            ErrorHandler.SECURITY_ERROR,
            str(e)
        )

# APIè·¯ç”±
@app.route('/api/files', methods=['GET'])
@login_required
@json_error_handler
def api_list_files():
    """è·å–æ–‡ä»¶åˆ—è¡¨API"""
    files = get_file_list_data(app.config['UPLOAD_FOLDER'])
    return jsonify({'files': files})

@app.route('/api/upload', methods=['POST'])
@login_required
@unified_error_handler('json')
def api_upload_file():
    """APIæ–‡ä»¶ä¸Šä¼ è·¯ç”±ï¼ˆä¸æ›´æ–°å‘é‡ç´¢å¼•ï¼‰"""
    # éªŒè¯ä¸Šä¼ è¯·æ±‚
    success, error_info, file = validate_upload_request(request)
    if not success:
        return success, error_info
    
    # å¤„ç†æ–‡ä»¶ä¸Šä¼ ï¼ˆä¸è‡ªåŠ¨æ›´æ–°ç´¢å¼•ï¼‰
    success, info, filename = handle_file_upload_core(
        file, app.config['UPLOAD_FOLDER'], update_index=False
    )
    
    if success:
        return jsonify({'success': True, 'msg': info['message'], 'filename': filename})
    else:
        return success, info

@app.route('/api/index_status', methods=['GET'])
@login_required
@unified_error_handler('json')
def get_index_status():
    """è·å–ç´¢å¼•çŠ¶æ€API"""
    cache_file = os.path.join(app.config['EMBEDDINGS_FOLDER'], 'embedding_cache.pkl')
    
    if os.path.exists(cache_file):
        # è·å–ç¼“å­˜æ–‡ä»¶ä¿¡æ¯
        mtime = os.path.getmtime(cache_file)
        cache_time = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
        cache_size = os.path.getsize(cache_file) / (1024 * 1024)  # Convert to MB
        
        return ErrorHandler.handle_success(
            'ç´¢å¼•ç¼“å­˜å­˜åœ¨',
            {
                'hasCache': True,
                'cacheName': 'embedding_cache.pkl',
                'cacheTime': cache_time,
                'cacheSize': f'{cache_size:.2f}MB'
            }
        )
    else:
        return ErrorHandler.handle_success(
            'æš‚æ— ç´¢å¼•ç¼“å­˜',
            {'hasCache': False}
        )

@app.route('/api/delete/<filename>', methods=['DELETE'])
@login_required
@unified_error_handler('json')
def api_delete_file(filename):
    """APIåˆ é™¤æ–‡ä»¶è·¯ç”±ï¼ˆè¿”å›JSONå“åº”ï¼‰"""
    return handle_file_deletion_core(filename, app.config['UPLOAD_FOLDER'])

@app.route('/api/model_status', methods=['GET'])
@unified_error_handler('json')
def api_model_status():
    """è·å–æ¨¡å‹çŠ¶æ€API"""
    status = get_model_status()
    return ErrorHandler.handle_success(
        'æ¨¡å‹çŠ¶æ€æŸ¥è¯¢æˆåŠŸ',
        status
    )

@app.route('/api/generate_index', methods=['POST'])
@login_required
@unified_error_handler('json')
def api_generate_index():
    """ç”Ÿæˆå‘é‡ç´¢å¼•API"""
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯å¤„ç†çš„æ–‡ä»¶
    files = []
    for filename in os.listdir(app.config['UPLOAD_FOLDER']):
        if os.path.isfile(os.path.join(app.config['UPLOAD_FOLDER'], filename)):
            files.append(os.path.join(app.config['UPLOAD_FOLDER'], filename))
    
    if not files:
        return ErrorHandler.handle_error(
            ErrorHandler.VALIDATION_ERROR,
            'æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶'
        )
        
    # ç›´æ¥è°ƒç”¨update_embeddingså‡½æ•°ç”Ÿæˆå‘é‡ç´¢å¼•
    if update_embeddings():
        cache_file = os.path.join(app.config['EMBEDDINGS_FOLDER'], 'embedding_cache.pkl')
        return ErrorHandler.handle_success(
            'å‘é‡ç´¢å¼•ç”ŸæˆæˆåŠŸ',
            {'cache_path': cache_file}
        )
    else:
        return ErrorHandler.handle_error(
            ErrorHandler.SERVER_ERROR,
            'å‘é‡ç´¢å¼•ç”Ÿæˆå¤±è´¥ï¼šæ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£'
        )

@app.route('/api/doc_description/<filename>', methods=['GET', 'PUT'])
@login_required
@unified_error_handler('json')
def manage_doc_description(filename):
    """ç®¡ç†æ–‡æ¡£æè¿°API"""
    try:
        # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_path = safe_file_path(filename, app.config['UPLOAD_FOLDER'])
        if not os.path.exists(file_path):
            return ErrorHandler.handle_error(
                ErrorHandler.NOT_FOUND_ERROR,
                'æ–‡ä»¶ä¸å­˜åœ¨'
            )
        
        descriptions = load_doc_descriptions()
        
        if request.method == 'GET':
            # è·å–æ–‡æ¡£æè¿°
            description = descriptions.get(filename, '')
            return ErrorHandler.handle_success(
                'è·å–æè¿°æˆåŠŸ',
                {'filename': filename, 'description': description}
            )
        
        elif request.method == 'PUT':
            # æ›´æ–°æ–‡æ¡£æè¿°
            data = request.get_json() or {}
            new_description = data.get('description', '').strip()
            
            # æ›´æ–°æè¿°
            descriptions[filename] = new_description
            
            if save_doc_descriptions(descriptions):
                return ErrorHandler.handle_success(
                    'æè¿°æ›´æ–°æˆåŠŸ',
                    {'filename': filename, 'description': new_description}
                )
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR,
                    'ä¿å­˜æè¿°å¤±è´¥'
                )
                
    except ValueError as e:
        return ErrorHandler.handle_error(
            ErrorHandler.SECURITY_ERROR,
            str(e)
        )

# ===============================
# é…ç½®ç®¡ç† API
# ===============================

def load_app_config():
    """åŠ è½½åº”ç”¨é…ç½®"""
    try:
        config_file = app.config['APP_CONFIG_FILE']
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                'city_config': {
                    'name': 'æµ·å£',
                    'updated_at': datetime.datetime.now().isoformat()
                }
            }
    except Exception as e:
        logging.error(f"åŠ è½½åº”ç”¨é…ç½®å¤±è´¥: {e}")
        return {
            'city_config': {
                'name': 'æµ·å£',
                'updated_at': datetime.datetime.now().isoformat()
            }
        }

def save_app_config(config_data):
    """ä¿å­˜åº”ç”¨é…ç½®"""
    try:
        config_file = app.config['APP_CONFIG_FILE']
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(config_file), exist_ok=True)
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜åº”ç”¨é…ç½®å¤±è´¥: {e}")
        return False

def load_travel_purposes():
    """åŠ è½½æ—…è¡Œç›®çš„é…ç½®"""
    try:
        purposes_file = app.config['TRAVEL_PURPOSES_FILE']
        if os.path.exists(purposes_file):
            with open(purposes_file, 'r', encoding='utf-8') as f:
                purposes = json.load(f)
                # è¿‡æ»¤æ‰æœªå®Œæˆç¼–è¾‘çš„æ–°å¢é¡¹ç›®
                filtered_purposes = [p for p in purposes if p.get('name') != 'æ–°å¢æ—…æ¸¸ç›®çš„']
                return filtered_purposes
        else:
            # è¿”å›é»˜è®¤æ—…è¡Œç›®çš„
            return [
                {'id': 1, 'name': 'ä¼‘é—²åº¦å‡'},
                {'id': 2, 'name': 'äº²å­æ¸¸ç©'},
                {'id': 3, 'name': 'ç¾é£Ÿæ¢ç´¢'},
                {'id': 4, 'name': 'æ–‡åŒ–ä½“éªŒ'},
                {'id': 5, 'name': 'çœ‹æ¼”å”±ä¼š'},
                {'id': 6, 'name': 'å…ç¨è´­ç‰©'},
                {'id': 7, 'name': 'æ‘è½æ¢ç´¢'},
                {'id': 8, 'name': 'éœ²è¥'},
                {'id': 9, 'name': 'èµ¶æµ·'}
            ]
    except Exception as e:
        logging.error(f"åŠ è½½æ—…è¡Œç›®çš„é…ç½®å¤±è´¥: {e}")
        return []

def save_travel_purposes(purposes_data):
    """ä¿å­˜æ—…è¡Œç›®çš„é…ç½®"""
    try:
        purposes_file = app.config['TRAVEL_PURPOSES_FILE']
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(purposes_file), exist_ok=True)
        
        with open(purposes_file, 'w', encoding='utf-8') as f:
            json.dump(purposes_data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜æ—…è¡Œç›®çš„é…ç½®å¤±è´¥: {e}")
        return False

def load_travel_preferences():
    """åŠ è½½æ—…æ¸¸åå¥½é…ç½® - ä»…æ”¯æŒæ–°çš„ç®€åŒ–æ ¼å¼"""
    try:
        preferences_file = app.config['TRAVEL_PREFERENCES_FILE']
        if os.path.exists(preferences_file):
            with open(preferences_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('travel_preferences', {})
        
        # è¿”å›é»˜è®¤é…ç½®ï¼ˆæ–°æ ¼å¼ï¼‰
        return get_default_preferences()
    except Exception as e:
        logging.error(f"åŠ è½½æ—…æ¸¸åå¥½é…ç½®å¤±è´¥: {e}")
        return get_default_preferences()

def get_default_preferences():
    """è·å–é»˜è®¤åå¥½é…ç½®ï¼ˆæ‰å¹³åŒ–æ··åˆæ ¼å¼ï¼‰"""
    return {
        "ä½å®¿ç±»å‹": [
            "ç»æµå‹é…’åº—",
            "å•†åŠ¡é…’åº—", 
            "åº¦å‡é…’åº—",
            "æ°‘å®¿å®¢æ ˆ",
            "é’å¹´æ—…èˆ",
            {
                "type": "input",
                "name": "æŒ‡å®šé…’åº—å“ç‰Œ",
                "placeholder": "ä¾‹å¦‚ï¼šå…¨å­£ã€å¦‚å®¶ã€æ±‰åº­"
            },
            {
                "type": "input", 
                "name": "ç‰¹æ®Šä½å®¿éœ€æ±‚",
                "placeholder": "ä¾‹å¦‚ï¼šæµ·æ™¯æˆ¿ã€æ— çƒŸæˆ¿"
            }
        ],
        "é¤é¥®é€‰æ‹©": [
            "æµ·å—ç‰¹è‰²èœ",
            "æµ·é²œå¤§æ’æ¡£",
            "ç«é”…çƒ§çƒ¤",
            "å¿«é¤ç®€é¤",
            "ç²¾å“é¤å…",
            "è¡—è¾¹å°é£Ÿ",
            {
                "type": "input",
                "name": "æŒ‡å®šé¤å…",
                "placeholder": "ä¾‹å¦‚ï¼šéº¦å½“åŠ³ã€è‚¯å¾·åŸº"
            },
            {
                "type": "input",
                "name": "é¥®é£Ÿç¦å¿Œ",
                "placeholder": "ä¾‹å¦‚ï¼šä¸åƒè¾£ã€ç´ é£Ÿ"
            }
        ],
        "å‡ºè¡Œæ–¹å¼": [
            "å…¬å…±äº¤é€š",
            "å‡ºç§Ÿè½¦ç½‘çº¦è½¦", 
            "è‡ªé©¾ç§Ÿè½¦",
            "å…±äº«å•è½¦",
            "æ­¥è¡Œ",
            {
                "type": "input",
                "name": "å…¶ä»–äº¤é€šå·¥å…·",
                "placeholder": "ä¾‹å¦‚ï¼šæ‘©æ‰˜è½¦ã€ç”µåŠ¨è½¦"
            }
        ],
        "ç‰¹æ®Šéœ€æ±‚": [
            "æ— éšœç¢è®¾æ–½",
            "å„¿ç«¥å‹å¥½",
            "è€äººå‹å¥½", 
            "å® ç‰©å‹å¥½",
            {
                "type": "input",
                "name": "å¥åº·éœ€æ±‚",
                "placeholder": "ä¾‹å¦‚ï¼šè½®æ¤…é€šé“ã€è¿‡æ•æé†’"
            },
            {
                "type": "input",
                "name": "å…¶ä»–è¦æ±‚",
                "placeholder": "è¯·æè¿°å…¶ä»–ç‰¹æ®Šéœ€æ±‚"
            }
        ]
    }

def save_travel_preferences(preferences_data):
    """ä¿å­˜æ—…æ¸¸åå¥½é…ç½®"""
    try:
        preferences_file = app.config['TRAVEL_PREFERENCES_FILE']
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(preferences_file), exist_ok=True)
        
        # æ„å»ºå®Œæ•´çš„æ•°æ®ç»“æ„
        data = {
            'travel_preferences': preferences_data,
            'updated_at': datetime.datetime.now(beijing_tz).isoformat()
        }
        
        with open(preferences_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜æ—…æ¸¸åå¥½é…ç½®å¤±è´¥: {e}")
        return False

@app.route('/api/config/city', methods=['GET', 'PUT'])
@unified_error_handler('json')
def manage_city_config():
    """åŸå¸‚é…ç½®ç®¡ç†API"""
    try:
        app_config = load_app_config()
        
        if request.method == 'GET':
            # è·å–åŸå¸‚é…ç½® - ä¸éœ€è¦è®¤è¯
            # ç¡®ä¿è¿”å›å®é™…çš„åŸå¸‚é…ç½®ï¼Œè€Œä¸æ˜¯é»˜è®¤å€¼
            city_config = app_config.get('city_config', {'name': 'æµ·å£'})
            logging.info(f"è·å–åŸå¸‚é…ç½®: {city_config}")
            return ErrorHandler.handle_success(
                'è·å–åŸå¸‚é…ç½®æˆåŠŸ',
                {
                    'city_config': city_config,
                    'timestamp': datetime.datetime.now().isoformat()
                }
            )
        
        elif request.method == 'PUT':
            # æ›´æ–°åŸå¸‚é…ç½® - éœ€è¦è®¤è¯
            if not session.get('logged_in'):
                return ErrorHandler.handle_error(
                    ErrorHandler.SECURITY_ERROR,
                    'éœ€è¦ç®¡ç†å‘˜æƒé™'
                )
            
            data = request.get_json() or {}
            city_name = data.get('name', '').strip()
            
            if not city_name:
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    'åŸå¸‚åç§°ä¸èƒ½ä¸ºç©º'
                )
            
            # æ›´æ–°é…ç½®
            app_config['city_config'] = {
                'name': city_name,
                'updated_at': datetime.datetime.now().isoformat()
            }
            
            if save_app_config(app_config):
                logging.info(f"åŸå¸‚é…ç½®å·²æ›´æ–°ä¸º: {city_name}")
                # æ›´æ–°å…¨å±€å˜é‡
                update_current_city()
                return ErrorHandler.handle_success(
                    f'åŸå¸‚é…ç½®å·²ä¿å­˜: {city_name}',
                    {
                        'city_config': app_config['city_config'],
                        'timestamp': datetime.datetime.now().isoformat()
                    }
                )
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR,
                    'ä¿å­˜åŸå¸‚é…ç½®å¤±è´¥'
                )
    
    except Exception as e:
        logging.error(f"åŸå¸‚é…ç½®ç®¡ç†å‡ºé”™: {e}")
        return ErrorHandler.handle_error(
            ErrorHandler.SERVER_ERROR,
            'åŸå¸‚é…ç½®æ“ä½œå¤±è´¥',
            str(e)
        )

@app.route('/api/config/travel_purposes', methods=['GET', 'PUT', 'POST', 'DELETE'])
@unified_error_handler('json')
def manage_travel_purposes():
    """æ—…è¡Œç›®çš„é…ç½®ç®¡ç†API"""
    try:
        purposes = load_travel_purposes()
        
        if request.method == 'GET':
            # è·å–æ—…è¡Œç›®çš„é…ç½® - ä¸éœ€è¦è®¤è¯
            return ErrorHandler.handle_success(
                'è·å–æ—…è¡Œç›®çš„é…ç½®æˆåŠŸ',
                {
                    'travel_purposes': purposes,
                    'timestamp': datetime.datetime.now().isoformat()
                }
            )
        
        elif request.method == 'PUT':
            # æ›´æ–°æ•´ä¸ªæ—…è¡Œç›®çš„åˆ—è¡¨ - éœ€è¦è®¤è¯
            if not session.get('logged_in'):
                return ErrorHandler.handle_error(
                    ErrorHandler.SECURITY_ERROR,
                    'éœ€è¦ç®¡ç†å‘˜æƒé™'
                )
            
            data = request.get_json() or {}
            new_purposes = data.get('purposes', [])
            
            if not isinstance(new_purposes, list):
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    'æ—…è¡Œç›®çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®'
                )
            
            if save_travel_purposes(new_purposes):
                logging.info(f"æ—…è¡Œç›®çš„é…ç½®å·²æ›´æ–°ï¼Œå…±{len(new_purposes)}é¡¹")
                return ErrorHandler.handle_success(
                    f'æ—…è¡Œç›®çš„é…ç½®å·²ä¿å­˜ï¼Œå…±{len(new_purposes)}é¡¹',
                    {
                        'travel_purposes': new_purposes,
                        'timestamp': datetime.datetime.now().isoformat()
                    }
                )
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR,
                    'ä¿å­˜æ—…è¡Œç›®çš„é…ç½®å¤±è´¥'
                )
        
        elif request.method == 'POST':
            # æ·»åŠ æ–°çš„æ—…è¡Œç›®çš„ - éœ€è¦è®¤è¯
            if not session.get('logged_in'):
                return ErrorHandler.handle_error(
                    ErrorHandler.SECURITY_ERROR,
                    'éœ€è¦ç®¡ç†å‘˜æƒé™'
                )
            data = request.get_json() or {}
            name = data.get('name', '').strip()
            icon = data.get('icon', 'fa-map-marker-alt').strip()
            
            if not name:
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    'æ—…è¡Œç›®çš„åç§°ä¸èƒ½ä¸ºç©º'
                )
            
            # ç”Ÿæˆæ–°ID
            max_id = max([p.get('id', 0) for p in purposes]) if purposes else 0
            new_purpose = {
                'id': max_id + 1,
                'name': name,
                'icon': icon
            }
            
            purposes.append(new_purpose)
            
            if save_travel_purposes(purposes):
                logging.info(f"æ–°å¢æ—…è¡Œç›®çš„: {name}")
                return ErrorHandler.handle_success(
                    f'æ·»åŠ æ—…è¡Œç›®çš„æˆåŠŸ: {name}',
                    {
                        'new_purpose': new_purpose,
                        'travel_purposes': purposes,
                        'timestamp': datetime.datetime.now().isoformat()
                    }
                )
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR,
                    'ä¿å­˜æ—…è¡Œç›®çš„å¤±è´¥'
                )
        
        elif request.method == 'DELETE':
            # åˆ é™¤æŒ‡å®šçš„æ—…è¡Œç›®çš„ - éœ€è¦è®¤è¯
            if not session.get('logged_in'):
                return ErrorHandler.handle_error(
                    ErrorHandler.SECURITY_ERROR,
                    'éœ€è¦ç®¡ç†å‘˜æƒé™'
                )
            
            data = request.get_json() or {}
            purpose_id = data.get('id')
            
            if purpose_id is None:
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    'è¯·æŒ‡å®šè¦åˆ é™¤çš„æ—…è¡Œç›®çš„ID'
                )
            
            # æŸ¥æ‰¾å¹¶åˆ é™¤æŒ‡å®šçš„æ—…è¡Œç›®çš„
            original_count = len(purposes)
            purposes = [p for p in purposes if p.get('id') != purpose_id]
            
            if len(purposes) == original_count:
                return ErrorHandler.handle_error(
                    ErrorHandler.NOT_FOUND_ERROR,
                    f'æœªæ‰¾åˆ°IDä¸º{purpose_id}çš„æ—…è¡Œç›®çš„'
                )
            
            if save_travel_purposes(purposes):
                logging.info(f"åˆ é™¤æ—…è¡Œç›®çš„ID: {purpose_id}")
                return ErrorHandler.handle_success(
                    f'åˆ é™¤æ—…è¡Œç›®çš„æˆåŠŸ',
                    {
                        'deleted_id': purpose_id,
                        'travel_purposes': purposes,
                        'timestamp': datetime.datetime.now().isoformat()
                    }
                )
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR,
                    'ä¿å­˜æ—…è¡Œç›®çš„é…ç½®å¤±è´¥'
                )
    
    except Exception as e:
        logging.error(f"æ—…è¡Œç›®çš„é…ç½®ç®¡ç†å‡ºé”™: {e}")
        return ErrorHandler.handle_error(
            ErrorHandler.SERVER_ERROR,
            'æ—…è¡Œç›®çš„é…ç½®æ“ä½œå¤±è´¥',
            str(e)
        )

@app.route('/api/config/travel_preferences', methods=['GET', 'PUT'])
@unified_error_handler('json')
def manage_travel_preferences():
    """æ—…æ¸¸åå¥½é…ç½®ç®¡ç†API"""
    try:
        if request.method == 'GET':
            # è·å–æ—…æ¸¸åå¥½é…ç½® - ä¸éœ€è¦è®¤è¯
            preferences = load_travel_preferences()
            return ErrorHandler.handle_success(
                'è·å–æ—…æ¸¸åå¥½é…ç½®æˆåŠŸ',
                {
                    'travel_preferences': preferences,
                    'timestamp': datetime.datetime.now(beijing_tz).isoformat()
                }
            )
        
        elif request.method == 'PUT':
            # æ›´æ–°æ—…æ¸¸åå¥½é…ç½® - éœ€è¦è®¤è¯
            if not session.get('logged_in'):
                return ErrorHandler.handle_error(
                    ErrorHandler.SECURITY_ERROR,
                    'éœ€è¦ç®¡ç†å‘˜æƒé™'
                )
            
            data = request.get_json() or {}
            preferences = data.get('preferences', {})
            
            if not preferences:
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    'æ—…æ¸¸åå¥½é…ç½®ä¸èƒ½ä¸ºç©º'
                )
            
            # éªŒè¯æ–°æ ¼å¼é…ç½®ï¼šç›´æ¥æ˜¯ç±»åˆ«å-åå¥½é¡¹çš„æ˜ å°„
            for category_name, items in preferences.items():
                if not isinstance(items, list):
                    return ErrorHandler.handle_error(
                        ErrorHandler.VALIDATION_ERROR,
                        f'åå¥½ç±»åˆ« {category_name} å¿…é¡»æ˜¯æ•°ç»„æ ¼å¼'
                    )
                
                # éªŒè¯æ¯ä¸ªåå¥½é¡¹çš„æ ¼å¼ï¼ˆæ”¯æŒæ–‡æ¡£ä¸­çš„æ··åˆæ ¼å¼ï¼‰
                for item in items:
                    if isinstance(item, str):
                        # å­—ç¬¦ä¸²æ ¼å¼çš„é¢„å®šé€‰é¡¹ï¼Œç¬¦åˆæ–‡æ¡£è§„èŒƒ
                        continue
                    elif isinstance(item, dict):
                        # å¯¹è±¡æ ¼å¼éœ€è¦éªŒè¯å¿…è¦å­—æ®µ
                        if item.get('type') == 'input':
                            # è‡ªå®šä¹‰è¾“å…¥é¡¹æ ¼å¼ï¼ˆæŒ‰æ–‡æ¡£è§„èŒƒï¼‰
                            if 'name' not in item:
                                return ErrorHandler.handle_error(
                                    ErrorHandler.VALIDATION_ERROR,
                                    f'è‡ªå®šä¹‰è¾“å…¥é¡¹å¿…é¡»åŒ…å« name å­—æ®µ'
                                )
                        else:
                            return ErrorHandler.handle_error(
                                ErrorHandler.VALIDATION_ERROR,
                                f'åå¥½é¡¹æ ¼å¼é”™è¯¯ï¼Œå¯¹è±¡ç±»å‹å¿…é¡»æ˜¯è‡ªå®šä¹‰è¾“å…¥é¡¹ï¼ˆtype: "input"ï¼‰'
                            )
                    else:
                        return ErrorHandler.handle_error(
                            ErrorHandler.VALIDATION_ERROR,
                            f'åå¥½é¡¹å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å¯¹è±¡æ ¼å¼'
                        )
            
            if save_travel_preferences(preferences):
                logging.info(f"æ›´æ–°æ—…æ¸¸åå¥½é…ç½®æˆåŠŸï¼ŒåŒ…å« {len(preferences)} ä¸ªç±»åˆ«")
                return ErrorHandler.handle_success(
                    'æ—…æ¸¸åå¥½é…ç½®ä¿å­˜æˆåŠŸ',
                    {
                        'travel_preferences': preferences,
                        'timestamp': datetime.datetime.now(beijing_tz).isoformat()
                    }
                )
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR,
                    'ä¿å­˜æ—…æ¸¸åå¥½é…ç½®å¤±è´¥'
                )
    
    except Exception as e:
        logging.error(f"æ—…æ¸¸åå¥½é…ç½®ç®¡ç†å‡ºé”™: {e}")
        return ErrorHandler.handle_error(
            ErrorHandler.SERVER_ERROR,
            'æ—…æ¸¸åå¥½é…ç½®æ“ä½œå¤±è´¥',
            str(e)
        )

@app.route('/api/config/travel_preferences/categories', methods=['POST', 'DELETE'])
@unified_error_handler('json')
def manage_preference_categories():
    """åå¥½ç±»åˆ«ç®¡ç†API"""
    try:
        # éœ€è¦ç®¡ç†å‘˜æƒé™
        if not session.get('logged_in'):
            return ErrorHandler.handle_error(
                ErrorHandler.SECURITY_ERROR,
                'éœ€è¦ç®¡ç†å‘˜æƒé™'
            )
        
        if request.method == 'POST':
            # æ·»åŠ æ–°çš„åå¥½ç±»åˆ«
            data = request.get_json() or {}
            category_id = data.get('id', '').strip()
            category_name = data.get('name', '').strip()
            category_icon = data.get('icon', 'fas fa-star').strip()
            
            if not category_name:
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    'ç±»åˆ«åç§°ä¸èƒ½ä¸ºç©º'
                )
            
            # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œè‡ªåŠ¨ç”Ÿæˆ
            if not category_id:
                import time
                category_id = f'category_{int(time.time())}'
            
            # åŠ è½½å½“å‰é…ç½®
            current_preferences = load_travel_preferences()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if 'categories' in current_preferences:
                if category_id in current_preferences['categories']:
                    return ErrorHandler.handle_error(
                        ErrorHandler.VALIDATION_ERROR,
                        f'ç±»åˆ«ID {category_id} å·²å­˜åœ¨'
                    )
                
                # è®¡ç®—æ–°çš„æ’åºå€¼
                max_order = max([cat.get('order', 0) for cat in current_preferences['categories'].values()], default=0)
                
                # æ·»åŠ æ–°ç±»åˆ«
                current_preferences['categories'][category_id] = {
                    'name': category_name,
                    'icon': category_icon,
                    'order': max_order + 1
                }
                
                # åˆå§‹åŒ–ç©ºçš„åå¥½åˆ—è¡¨
                if 'preferences' not in current_preferences:
                    current_preferences['preferences'] = {}
                current_preferences['preferences'][category_id] = []
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    'æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘ categories å­—æ®µ'
                )
            
            # ä¿å­˜é…ç½®
            if save_travel_preferences(current_preferences):
                return ErrorHandler.handle_success(
                    f'åå¥½ç±»åˆ« {category_name} å·²æ·»åŠ ',
                    {
                        'category_id': category_id,
                        'category': current_preferences['categories'][category_id]
                    }
                )
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR,
                    'ä¿å­˜åå¥½ç±»åˆ«å¤±è´¥'
                )
        
        elif request.method == 'DELETE':
            # åˆ é™¤åå¥½ç±»åˆ«
            data = request.get_json() or {}
            category_id = data.get('id', '').strip()
            
            if not category_id:
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    'ç±»åˆ«IDä¸èƒ½ä¸ºç©º'
                )
            
            # åŠ è½½å½“å‰é…ç½®
            current_preferences = load_travel_preferences()
            
            if 'categories' not in current_preferences or category_id not in current_preferences['categories']:
                return ErrorHandler.handle_error(
                    ErrorHandler.VALIDATION_ERROR,
                    f'ç±»åˆ«ID {category_id} ä¸å­˜åœ¨'
                )
            
            # åˆ é™¤ç±»åˆ«å’Œç›¸å…³åå¥½
            category_name = current_preferences['categories'][category_id]['name']
            del current_preferences['categories'][category_id]
            if 'preferences' in current_preferences and category_id in current_preferences['preferences']:
                del current_preferences['preferences'][category_id]
            
            # ä¿å­˜é…ç½®
            if save_travel_preferences(current_preferences):
                return ErrorHandler.handle_success(
                    f'åå¥½ç±»åˆ« {category_name} å·²åˆ é™¤'
                )
            else:
                return ErrorHandler.handle_error(
                    ErrorHandler.SERVER_ERROR,
                    'åˆ é™¤åå¥½ç±»åˆ«å¤±è´¥'
                )
    
    except Exception as e:
        logging.error(f"åå¥½ç±»åˆ«ç®¡ç†å‡ºé”™: {e}")
        return ErrorHandler.handle_error(
            ErrorHandler.SERVER_ERROR,
            'åå¥½ç±»åˆ«ç®¡ç†æ“ä½œå¤±è´¥',
            str(e)
        )

# æ–‡æ¡£ç®¡ç†è·¯ç”±å·²åœ¨ç¬¬774-798è¡Œå®šä¹‰ï¼Œè¿™é‡Œåˆ é™¤é‡å¤å®šä¹‰

# ç”Ÿæˆå‘é‡ç´¢å¼•è·¯ç”±
@app.route('/generate_index', methods=['POST'])
@login_required
@unified_error_handler('json')
def generate_index():
    """ç”Ÿæˆå‘é‡ç´¢å¼•è·¯ç”±ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
    try:
        update_embeddings()
        return ErrorHandler.handle_success('å‘é‡ç´¢å¼•å·²ç”Ÿæˆ')
    except Exception as e:
        return ErrorHandler.handle_error(
            ErrorHandler.SERVER_ERROR,
            'ç”Ÿæˆç´¢å¼•æ—¶å‡ºé”™',
            f'Error generating index: {str(e)}'
        )

# ä¸»é¡µè·¯ç”±
@app.route('/')
def index():
    return app.send_static_file('index.html')


# ç¤ºä¾‹è¡Œç¨‹è¡¨æ¥å£ï¼šä»æ•°æ®ç¼“å­˜ç›®å½•è¯»å–å¹¶è¿”å›JSON
@app.route('/api/sample-itinerary', methods=['GET'])
def get_sample_itinerary():
    try:
        sample_path = os.path.join(app.root_path, 'data', 'cache', 'travel_sample.json')
        with open(sample_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return jsonify(data)
    except FileNotFoundError:
        return jsonify({"status": "error", "message": "ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨"}), 404
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500


# ç®¡ç†ç«¯ï¼šè¯»å–ä¸ä¿å­˜ç¤ºä¾‹è¡Œç¨‹JSONï¼ˆç”¨äºè·¯çº¿æ¨èé¡µå³ä¾§æ–‡æœ¬æ¡†ï¼‰
@app.route('/admin/sample-itinerary', methods=['GET', 'POST'])
@login_required
def admin_sample_itinerary():
    sample_path = os.path.join(app.config['CACHE_FOLDER'], 'travel_sample.json')
    try:
        if request.method == 'GET':
            with open(sample_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return jsonify({
                'code': 0,
                'data': data,
                'msg': 'ok'
            })

        # ä¿å­˜è¦†ç›–
        payload = request.get_json(silent=True) or {}
        content = payload.get('content', '')
        if isinstance(content, str) and content.strip():
            try:
                parsed = json.loads(content)
            except Exception as e:
                return jsonify({'code': 1, 'msg': f'JSONè§£æå¤±è´¥: {str(e)}'}), 400
        else:
            parsed = payload.get('data')  # å…¼å®¹ç›´æ¥ä¼ å¯¹è±¡
        if parsed is None:
            return jsonify({'code': 1, 'msg': 'ç¼ºå°‘å¾…ä¿å­˜å†…å®¹'}), 400

        # å†™å…¥æ–‡ä»¶
        with open(sample_path, 'w', encoding='utf-8') as f:
            json.dump(parsed, f, ensure_ascii=False, indent=2)

        return jsonify({'code': 0, 'msg': 'ä¿å­˜æˆåŠŸ'})
    except FileNotFoundError:
        return jsonify({'code': 1, 'msg': 'ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    except Exception as e:
        return jsonify({'code': 1, 'msg': str(e)}), 500


@app.route('/api/chat', methods=['POST'])
def chat():
    logging.info("è¿›å…¥ /api/chat è·¯ç”±")
    
    # æ›´æ–°å½“å‰åŸå¸‚é…ç½®
    update_current_city()
    
    try:
        data = request.json
        # logging.info(f"æ”¶åˆ°è¯·æ±‚æ•°æ®: {data}")
        messages = data.get('messages', [])
        current_itinerary = data.get('current_itinerary')  # æ–°å¢ï¼šæ¥æ”¶å½“å‰è¡Œç¨‹è¡¨æ•°æ®
        # logging.info(f"messages: {messages[-1]}")
        
        # è°ƒè¯•ï¼šæ‰“å°current_itineraryçš„è¯¦ç»†ä¿¡æ¯
        logging.info(f"è°ƒè¯• - current_itinerary ç±»å‹: {type(current_itinerary)}")
        logging.info(f"è°ƒè¯• - current_itinerary å†…å®¹: {current_itinerary}")
        if current_itinerary:
            logging.info("æ”¶åˆ°å½“å‰è¡Œç¨‹è¡¨æ•°æ®ï¼Œå°†å¯ç”¨çŸ­æœŸè®°å¿†åŠŸèƒ½")
            if isinstance(current_itinerary, dict):
                logging.info(f"è°ƒè¯• - current_itinerary.keys(): {current_itinerary.keys()}")
                logging.info(f"è°ƒè¯• - current_itinerary.get('days'): {current_itinerary.get('days')}")
        else:
            logging.info("è°ƒè¯• - æœªæ”¶åˆ°current_itineraryæ•°æ®æˆ–æ•°æ®ä¸ºç©º")
        tool_use = []
        now_beijing = lambda: datetime.datetime.now(beijing_tz).isoformat()
        
        # ä»¥ä¸‹æ˜¯åŸæœ‰çš„æ­£å¸¸èŠå¤©å¤„ç†é€»è¾‘
        # 1. æ£€æŸ¥ç¼“å­˜å’Œæ–‡æ¡£çŠ¶æ€
        cache_status = check_cache_and_docs_status()
        
        # 2. æ ¹æ®ç¼“å­˜çŠ¶æ€åŠ¨æ€æ„å»ºç³»ç»Ÿæç¤ºè¯
        # æ„å»ºå·¥å…·åˆ—è¡¨
        if cache_status['doc_query_available']:
            # æ–‡æ¡£æŸ¥è¯¢å¯ç”¨çš„æƒ…å†µ
            tools_section = """
                ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘
                ç³»ç»Ÿæ”¯æŒä»¥ä¸‹5ä¸ªå·¥å…·ï¼Œå·¥å…·åç§°å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ï¼š
                1. "è·å–å¤©æ°”ä¿¡æ¯" - æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”æƒ…å†µ
                2. "æœç´¢å…´è¶£ç‚¹" - å…³é”®è¯æœç´¢POIä¿¡æ¯
                3. "é™„è¿‘æœç´¢" - ä»¥æŸä¸ªä½ç½®ä¸ºä¸­å¿ƒæœç´¢å‘¨è¾¹POIä¿¡æ¯
                4. "ç›®çš„åœ°è·ç¦»" - æµ‹é‡ä¸¤ä¸ªåœ°ç‚¹ä¹‹é—´çš„è·ç¦»
                5. "æ–‡æ¡£æŸ¥è¯¢" - ä»æœ¬åœ°çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶å›ç­”é—®é¢˜"""
        else:
            # æ–‡æ¡£æŸ¥è¯¢ä¸å¯ç”¨çš„æƒ…å†µ
            tools_section = """
                ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘
                ç³»ç»Ÿæ”¯æŒä»¥ä¸‹4ä¸ªå·¥å…·ï¼Œå·¥å…·åç§°å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ï¼š
                1. "è·å–å¤©æ°”ä¿¡æ¯" - æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”æƒ…å†µ
                2. "æœç´¢å…´è¶£ç‚¹" - å…³é”®è¯æœç´¢POIä¿¡æ¯
                3. "é™„è¿‘æœç´¢" - ä»¥æŸä¸ªä½ç½®ä¸ºä¸­å¿ƒæœç´¢å‘¨è¾¹POIä¿¡æ¯
                4. "ç›®çš„åœ°è·ç¦»" - æµ‹é‡ä¸¤ä¸ªåœ°ç‚¹ä¹‹é—´çš„è·ç¦»"""
        
        # æ„å»ºå·¥å…·åç§°åˆ—è¡¨
        tool_names_section = """                  * "è·å–å¤©æ°”ä¿¡æ¯"
                  * "æœç´¢å…´è¶£ç‚¹" 
                  * "é™„è¿‘æœç´¢"
                  * "ç›®çš„åœ°è·ç¦»" """
        if cache_status['doc_query_available']:
            tool_names_section += """
                  * "æ–‡æ¡£æŸ¥è¯¢" """
        
        # å·¥å…·æ•°é‡æ–‡å­—
        tool_count = "5ä¸ªåç§°ä¹‹ä¸€" if cache_status['doc_query_available'] else "4ä¸ªåç§°ä¹‹ä¸€"

        system_message = {
            "role": "system",
            "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{current_city}æ—…æ¸¸è§„åˆ’åŠ©æ‰‹ã€‚

ã€æ ¸å¿ƒä»»åŠ¡ã€‘åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ã€ç”Ÿæˆè¡Œç¨‹è¡¨ã€åˆ†æè¡Œç¨‹æˆ–ç›´æ¥å›ç­”ã€‚

ã€å¯ç”¨å·¥å…·ç±»å‹ã€‘
- å¤©æ°”ä¿¡æ¯æŸ¥è¯¢
- æ™¯ç‚¹/é…’åº—/é¤å…æœç´¢
- é™„è¿‘åœ°ç‚¹æœç´¢  
- è·ç¦»æµ‹é‡
- æœ¬åœ°çŸ¥è¯†åº“æŸ¥è¯¢{"ï¼ˆå½“å‰å¯ç”¨ï¼‰" if cache_status['doc_query_available'] else "ï¼ˆå½“å‰ä¸å¯ç”¨ï¼‰"}

ã€åˆ¤æ–­åŸåˆ™ã€‘
- éœ€è¦å®æ—¶æ•°æ®ï¼ˆå¤©æ°”ã€å…·ä½“ä½ç½®ã€è¥ä¸šä¿¡æ¯ã€è·¯çº¿è·ç¦»ç­‰ï¼‰â†’ å›å¤"NEED_TOOLS"
- éœ€è¦å…·ä½“çš„æ™¯ç‚¹/é…’åº—/é¤å…æ¨è â†’ å›å¤"NEED_TOOLS"  
- éœ€è¦åˆ¶å®šè¯¦ç»†è¡Œç¨‹è§„åˆ’ â†’ å›å¤"NEED_TOOLS"
- ç”¨æˆ·æ˜ç¡®è¦æ±‚ç”Ÿæˆã€åˆ¶å®šã€å®‰æ’è¡Œç¨‹è¡¨/æ—¥ç¨‹ â†’ å›å¤"ITINERARY_UPDATE"
- ç”¨æˆ·è¯¢é—®å¦‚ä½•æ•´ç†ä¹‹å‰çš„æ¨èæˆå…·ä½“è¡Œç¨‹ â†’ å›å¤"ITINERARY_UPDATE"
- å¯¹è¯ä¸­å·²æœ‰å……åˆ†ä¿¡æ¯ï¼Œç”¨æˆ·å¸Œæœ›æ•´åˆæˆå¯æ‰§è¡Œè®¡åˆ’ â†’ å›å¤"ITINERARY_UPDATE"
- ç”¨æˆ·è¯¢é—®ç°æœ‰è¡Œç¨‹æ˜¯å¦åˆç†ã€æ—¶é—´å®‰æ’ã€è·¯çº¿è¯„ä¼°ç­‰åˆ†ææ€§é—®é¢˜ â†’ å›å¤"ITINERARY_ANALYZE"
- ç”¨æˆ·è¦æ±‚åˆ†æã€è¯„ä¼°ã€ç‚¹è¯„å½“å‰è¡Œç¨‹è¡¨ â†’ å›å¤"ITINERARY_ANALYZE"
- å¯ä»¥åŸºäºå¸¸è¯†ç›´æ¥å›ç­”çš„ä¸€èˆ¬æ€§é—®é¢˜ â†’ ç›´æ¥å›ç­”

ã€å›å¤è¦æ±‚ã€‘
- éœ€è¦å·¥å…·æ—¶ï¼šåªå›å¤"NEED_TOOLS"
- éœ€è¦ç”Ÿæˆè¡Œç¨‹è¡¨æ—¶ï¼šåªå›å¤"ITINERARY_UPDATE"
- éœ€è¦åˆ†æè¡Œç¨‹è¡¨æ—¶ï¼šåªå›å¤"ITINERARY_ANALYZE"
- ç›´æ¥å›ç­”æ—¶ï¼šæä¾›å®Œæ•´ã€ä¸“ä¸šçš„å›ç­”ï¼Œä½¿ç”¨Markdownæ ¼å¼ï¼Œé€‚å½“ä½¿ç”¨emoji

âš ï¸ **é‡è¦**ï¼šä¸è¦ç”Ÿæˆä»»ä½•å·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼Œåªåšåˆ¤æ–­ã€‚"""
        }
        messages.insert(0, system_message)
        # è®°å½•æ–°å¢åŠ çš„æ¶ˆæ¯åˆ°æ—¥å¿—
        if messages:
            logging.info(f"[NEW_MESSAGE] role={messages[-1].get('role')} content={messages[-1].get('content')}\n")
        
        # 2. åˆå§‹åŒ–å·¥å…·è°ƒç”¨å†å²å’Œç”¨æˆ·é—®é¢˜
        user_question = messages[-1] if messages and messages[-1].get('role') == 'user' else {"role": "user", "content": ""}
        
        # 3. ä¸‰è·¯ç”±æ¶æ„å¤„ç†
        logging.info("è·¯ç”±æ¶æ„åˆ¤æ–­")
        
        # è°ƒç”¨æ¨¡å‹è¿›è¡Œåˆå§‹åˆ¤æ–­
        try:
            response = client.chat.completions.create(
                model=BASE_MODEL,
                messages=messages
            )
            initial_response = response.choices[0].message.content.strip()
            logging.info(f"åˆå§‹åˆ¤æ–­ç»“æœ: {initial_response}")
            
            # æ ¹æ®å“åº”ç±»å‹è¿›è¡Œè·¯ç”±
            if initial_response == "NEED_TOOLS":
                # è·¯ç”±1: å·¥å…·è°ƒç”¨
                logging.info("è·¯ç”±åˆ°å·¥å…·è°ƒç”¨å¤„ç†")
                final_reply, tool_call_history, call_failed = reasoning_based_tool_calling(
                    user_question, messages, tool_use, now_beijing, cache_status['doc_query_available']
                )
                
                if call_failed:
                    return jsonify({
                        "status": "error", 
                        "message": final_reply
                    }), 500
                else:
                    return jsonify({
                        "status": "success",
                        "response": final_reply,
                        "tool_use": tool_use
                    })
                    
            elif initial_response == "ITINERARY_UPDATE":
                # è·¯ç”±2: è¡Œç¨‹è¡¨ç”Ÿæˆ
                logging.info("è·¯ç”±åˆ°è¡Œç¨‹è¡¨ç”Ÿæˆ")
                
                # è·å–å®Œæ•´å¯¹è¯å†å²ï¼ˆä¸åŒ…æ‹¬system_messageï¼‰
                conversation_for_itinerary = messages[1:]  # å»æ‰system_message
                
                # ç”Ÿæˆè¡Œç¨‹è¡¨ï¼Œä¼ é€’å½“å‰è¡Œç¨‹è¡¨æ•°æ®æ”¯æŒçŸ­æœŸè®°å¿†
                itinerary_result = generate_itinerary_from_conversation(
                    conversation_for_itinerary, 
                    current_itinerary  # ä¼ é€’å½“å‰è¡Œç¨‹è¡¨æ•°æ®
                )
                
                if itinerary_result["success"]:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¸©é¦¨æç¤º
                    if itinerary_result.get("action") == "friendly_prompt":
                        logging.info("è¿”å›æ¸©é¦¨æç¤ºç»™ç”¨æˆ·")
                        return jsonify({
                            "status": "success",
                            "response": itinerary_result["response"]
                        })
                    else:
                        # æ­£å¸¸çš„è¡Œç¨‹è¡¨ç”Ÿæˆ
                        logging.info(f"è¡Œç¨‹è¡¨ç”ŸæˆæˆåŠŸï¼Œå‡†å¤‡è¿”å›ç»™å‰ç«¯")
                        return jsonify({
                            "status": "success",
                            "response": "å·²ä¸ºæ‚¨ç”Ÿæˆè¡Œç¨‹è¡¨ï¼Œè¯·æŸ¥çœ‹å·¦è¾¹æ ï¼",
                            "action": "generate_itinerary",
                            "itinerary": itinerary_result["itinerary"]
                        })
                else:
                    return jsonify({
                        "status": "error",
                        "message": f"ç”Ÿæˆè¡Œç¨‹è¡¨å¤±è´¥: {itinerary_result['error']}"
                    }), 500
                    
            elif initial_response == "ITINERARY_ANALYZE":
                # è·¯ç”±3: è¡Œç¨‹åˆ†æï¼ˆæ–°å¢ï¼‰
                logging.info("è·¯ç”±åˆ°è¡Œç¨‹åˆ†æ")
                
                # åˆ†æè¡Œç¨‹è¡¨
                analysis_result = analyze_current_itinerary(
                    current_itinerary  # åªä¼ é€’å½“å‰è¡Œç¨‹è¡¨æ•°æ®
                )
                
                if analysis_result["success"]:
                    return jsonify({
                        "status": "success",
                        "response": analysis_result["response"],
                        "action": "analyze_itinerary"
                    })
                else:
                    return jsonify({
                        "status": "error",
                        "message": f"åˆ†æè¡Œç¨‹è¡¨å¤±è´¥: {analysis_result['error']}"
                    }), 500
                    
            else:
                # è·¯ç”±4: ç›´æ¥å›ç­”
                logging.info("è·¯ç”±åˆ°ç›´æ¥å›ç­”")
                return jsonify({
                    "status": "success",
                    "response": initial_response,
                    "tool_use": tool_use
                })
                
        except Exception as e:
            logging.error("å››è·¯ç”±æ¶æ„å¤„ç†å¼‚å¸¸", exc_info=True)
            return jsonify({
                "status": "error",
                "message": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
            }), 500
    except Exception as e:
        logging.error("/api/chat è·¯ç”±å‘ç”Ÿå¼‚å¸¸", exc_info=True)
        import traceback
        print(traceback.format_exc())
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500


def format_mcp_data(mcp_data):
    """æ ¼å¼åŒ–MCPæ•°æ®ï¼Œä¾¿äºLLMç†è§£å’Œå›ç­”"""
    if mcp_data["type"] == "weather":
        return format_weather_data(mcp_data)
    elif mcp_data["type"] == "location":
        return format_location_data(mcp_data)
    elif mcp_data["type"] == "poi":
        return format_poi_data(mcp_data)
    elif mcp_data["type"] == "direction":
        return format_direction_data(mcp_data)
    return json.dumps(mcp_data, ensure_ascii=False, indent=2)

def format_weather_data(mcp_data):
    """æ ¼å¼åŒ–å¤©æ°”æ•°æ®"""
    data = mcp_data["data"]
    result = f"## {mcp_data['city']}å¤©æ°”ä¿¡æ¯\n\n"
    
    if data.get("forecasts") and len(data["forecasts"]) > 0:
        forecast = data["forecasts"][0]
        result += f"**é¢„æŠ¥æ—¶é—´**: {forecast.get('reporttime', 'æœªçŸ¥')}\n\n"
        
        result += "**æœªæ¥å‡ å¤©å¤©æ°”é¢„æŠ¥**:\n\n"
        for cast in forecast.get("casts", []):
            result += f"- **{cast.get('date', 'æœªçŸ¥')}**: ç™½å¤© {cast.get('dayweather', 'æœªçŸ¥')} {cast.get('daytemp', 'æœªçŸ¥')}Â°C, å¤œé—´ {cast.get('nightweather', 'æœªçŸ¥')} {cast.get('nighttemp', 'æœªçŸ¥')}Â°C\n"
    
    return result

def format_location_data(mcp_data):
    """æ ¼å¼åŒ–åœ°ç†ä½ç½®æ•°æ®"""
    data = mcp_data["data"]
    result = f"## {mcp_data['address']}ä½ç½®ä¿¡æ¯\n\n"
    
    if data.get("geocodes") and len(data["geocodes"]) > 0:
        geo = data["geocodes"][0]
        result += f"**å®Œæ•´åœ°å€**: {geo.get('formatted_address', 'æœªçŸ¥')}\n"
        result += f"**åæ ‡**: {geo.get('location', 'æœªçŸ¥')}\n"
        result += f"**çœä»½**: {geo.get('province', 'æœªçŸ¥')}\n"
        result += f"**åŸå¸‚**: {geo.get('city', 'æœªçŸ¥')}\n"
        result += f"**åŒºå¿**: {geo.get('district', 'æœªçŸ¥')}\n"
    
    return result

def format_poi_data(mcp_data):
    """æ ¼å¼åŒ–POIæ•°æ®ï¼Œå±•ç¤ºå®Œæ•´çš„æœç´¢ç»“æœä¿¡æ¯"""
    data = mcp_data["data"]
    result = f"## {mcp_data['city']}çš„{mcp_data['keywords']}ä¿¡æ¯\n\n"
    
    if data.get("pois") and len(data["pois"]) > 0:
        total_count = int(data.get("count", 0))
        pois = data["pois"]
        
        # ğŸ”§ ä¼˜åŒ–ï¼šå½“æ€»ç»“æœæ•°>20æ—¶ï¼Œåªå¤„ç†å‰10æ¡ä»¥èŠ‚çœtoken
        if total_count > 20:
            pois = pois[:10]  # åªå–å‰10æ¡
            result += f"> âš ï¸ å…±æ‰¾åˆ° {total_count} æ¡ç»“æœï¼Œä¸ºèŠ‚çœèµ„æºï¼Œä»…å±•ç¤ºå‰ 10 æ¡\n\n"
        else:
            result += f"å…±æ‰¾åˆ° {len(pois)} æ¡ç»“æœï¼š\n\n"
        
    # æ³¨æ„ï¼šä¹‹å‰è¿™é‡Œé”™è¯¯åœ°ä½¿ç”¨ data["pois"]ï¼Œå¯¼è‡´å³ä½¿ä¸Šé¢æˆªæ–­äº†ä¹Ÿä»ç„¶éå†æ‰€æœ‰ç»“æœï¼Œå¼•èµ·å›å¤è¿‡é•¿è¢«å¤§æ¨¡å‹æˆªæ–­
    for i, poi in enumerate(pois, 1):
            result += f"### {i}. {poi.get('name', 'æœªçŸ¥åç§°')}\n"
            
            # ç±»å‹ä¿¡æ¯
            if poi.get('type'):
                result += f"- **ç±»å‹**: {poi.get('type')}\n"
            
            # åœ°ç†ä½ç½®ä¿¡æ¯
            location_parts = []
            if poi.get('cityname'): location_parts.append(poi['cityname'])
            if poi.get('adname'): location_parts.append(poi['adname'])
            if poi.get('address'): location_parts.append(poi['address'])
            if location_parts:
                result += f"- **åœ°å€**: {' '.join(location_parts)}\n"
            
            # åæ ‡
            if poi.get('location'):
                result += f"- **åæ ‡**: {poi.get('location')}\n"
            
            # IDä¿¡æ¯
            if poi.get('id'):
                result += f"- **POI ID**: {poi.get('id')}\n"
            
            # è”ç³»æ–¹å¼
            if poi.get('tel') and poi['tel']:
                result += f"- **ç”µè¯**: {poi.get('tel')}\n"
            
            # è¥ä¸šæ—¶é—´å’Œè¯„åˆ†
            if poi.get('biz_ext'):
                biz_ext = poi['biz_ext']
                if biz_ext.get('opentime2'):
                    result += f"- **å¼€æ”¾æ—¶é—´**: {biz_ext.get('opentime2')}\n"
                if biz_ext.get('rating'):
                    result += f"- **è¯„åˆ†**: â­{biz_ext.get('rating')}\n"
                if biz_ext.get('cost'):
                    result += f"- **å‚è€ƒä»·æ ¼**: Â¥{biz_ext.get('cost')}\n"
                if biz_ext.get('level'):
                    result += f"- **ç­‰çº§**: {biz_ext.get('level')}\n"
            
            # ç…§ç‰‡ä¿¡æ¯ - åªæœ‰åœ¨ç±»å‹åŒ…å«'é£æ™¯åèƒœ'æˆ–'æ™¯ç‚¹'æ—¶æ‰æ˜¾ç¤ºç…§ç‰‡æ¡†
            if poi.get('photos') and poi['photos'] and ('é£æ™¯åèƒœ' in poi.get('type', '') or 'æ™¯ç‚¹' in poi.get('type', '')):
                photos = poi['photos'][:3]  # æœ€å¤šæ˜¾ç¤º3å¼ å›¾ç‰‡
                result += "- **ç…§ç‰‡**:\n"
                # ä»…å½“ poi['id'] å­˜åœ¨æ—¶æ‰ç”Ÿæˆç›¸ç‰‡æ¡†
                if poi.get('id'):
                    poi_id = poi['id']
                    result += f'<div class="poi-photo-container" data-poi-index="{poi_id}">\n'
                    
                    # æ·»åŠ ç¿»é¡µæŒ‰é’® - `<` æ”¾åœ¨å·¦ä¾§ï¼Œ`>` æ”¾åœ¨å³ä¾§ï¼Œåªæœ‰ç…§ç‰‡æ•°é‡å¤§äº1æ—¶æ˜¾ç¤º
                    if len(photos) > 1:
                        # ä½¿ç”¨å•å¼•å·åŒ…è£¹å‚æ•°ï¼Œé¿å…åµŒå¥—åŒå¼•å·ç ´å HTML å±æ€§ï¼Œé˜²æ­¢å‰ç«¯æˆ–æ¨¡å‹æˆªæ–­
                        result += f"  <button class=\"poi-photo-nav poi-photo-nav-prev\" onclick=\"changePhoto(-1, '{poi_id}')\" style=\"left: 10px;\">&#10094;</button>\n"
                    
                    # ç…§ç‰‡æ¡†æ¶
                    result += '  <div class="poi-photo-frame">\n'
                    for j, photo in enumerate(photos):
                        display_style = "block" if j == 0 else "none"  # é»˜è®¤æ˜¾ç¤ºç¬¬ä¸€å¼ 
                        result += f'    <img src="{photo.get("url")}" alt="{photo.get("title", "æ™¯ç‚¹ç…§ç‰‡")}" class="poi-photo" style="display: {display_style};">\n'
                    result += '  </div>\n'
                    
                    if len(photos) > 1:
                        result += f"  <button class=\"poi-photo-nav poi-photo-nav-next\" onclick=\"changePhoto(1, '{poi_id}')\" style=\"right: 10px;\">&#10095;</button>\n"
                    
                    result += '</div>\n'
                else:
                    result += "(æš‚æ— å”¯ä¸€æ ‡è¯†ï¼Œæ— æ³•æ˜¾ç¤ºç…§ç‰‡æ¡†)\n"
            
            result += "\n"
    
    return result

def format_direction_data(mcp_data):
    """æ ¼å¼åŒ–è·¯çº¿æ•°æ®"""
    result = f"## ä»{mcp_data['origin']}åˆ°{mcp_data['destination']}çš„è·¯çº¿ä¿¡æ¯\n\n"
    
    driving_data = mcp_data.get("driving")
    walking_data = mcp_data.get("walking")
    transit_data = mcp_data.get("transit")
    
    if driving_data and driving_data.get("route") and driving_data["route"].get("paths") and len(driving_data["route"]["paths"]) > 0:
        path = driving_data["route"]["paths"][0]
        result += "### é©¾è½¦è·¯çº¿\n"
        result += f"- **è·ç¦»**: {path.get('distance', 'æœªçŸ¥')}ç±³ (çº¦{round(int(path.get('distance', 0))/1000, 2)}å…¬é‡Œ)\n"
        duration_min = int(int(path.get('duration', 0))/60)
        result += f"- **é¢„è®¡è€—æ—¶**: {duration_min}åˆ†é’Ÿ\n\n"
        
        if path.get("steps") and len(path["steps"]) > 0:
            result += "**è¯¦ç»†è·¯çº¿æŒ‡å¼•**:\n"
            for i, step in enumerate(path["steps"][:5], 1):  # åªæ˜¾ç¤ºå‰5æ­¥æŒ‡å¼•
                result += f"{i}. {step.get('instruction', 'æœªçŸ¥')}\n"
            if len(path["steps"]) > 5:
                result += "...(æ›´å¤šæŒ‡å¼•çœç•¥)\n"
        
        result += "\n"
    
    if walking_data and walking_data.get("route") and walking_data["route"].get("paths") and len(walking_data["route"]["paths"]) > 0:
        path = walking_data["route"]["paths"][0]
        result += "### æ­¥è¡Œè·¯çº¿\n"
        result += f"- **è·ç¦»**: {path.get('distance', 'æœªçŸ¥')}ç±³ (çº¦{round(int(path.get('distance', 0))/1000, 2)}å…¬é‡Œ)\n"
        duration_min = int(int(path.get('duration', 0))/60)
        result += f"- **é¢„è®¡è€—æ—¶**: {duration_min}åˆ†é’Ÿ\n"
        
        if int(path.get('distance', 0)) > 3000:
            result += "\n> âš ï¸ æ³¨æ„ï¼šæ­¥è¡Œè·ç¦»è¾ƒè¿œï¼Œå»ºè®®è€ƒè™‘å…¶ä»–äº¤é€šæ–¹å¼\n"
    
    if transit_data and transit_data.get('route') and transit_data['route'].get('transits'):
        transits = transit_data['route']['transits']
        if len(transits) > 0:
            first = transits[0]
            duration_min = int(int(first.get('duration', 0))/60)
            result += "### å…¬äº¤è·¯çº¿\n"
            result += f"- **é¢„è®¡è€—æ—¶**: {duration_min}åˆ†é’Ÿ\n"
            result += f"- **æ­¥è¡Œè·ç¦»**: {first.get('walking_distance', 'æœªçŸ¥')}ç±³\n"
            result += f"- **æ¢ä¹˜æ¬¡æ•°**: {first.get('segments') and len(first['segments']) - 1 or 0}\n"
            if first.get('segments'):
                result += "- **æ¢ä¹˜æ–¹æ¡ˆ**:\n"
                for i, seg in enumerate(first['segments'], 1):
                    if seg.get('bus') and seg['bus'].get('buslines'):
                        line = seg['bus']['buslines'][0]
                        result += f"    {i}. ä¹˜å {line.get('name', 'æœªçŸ¥çº¿è·¯')}ï¼Œä¸Šè½¦ç«™ï¼š{line.get('departure_stop', {}).get('name', 'æœªçŸ¥')}ï¼Œä¸‹è½¦ç«™ï¼š{line.get('arrival_stop', {}).get('name', 'æœªçŸ¥')}\n"
            result += "\n"
        else:
            result += "æœªæŸ¥è¯¢åˆ°å¯ç”¨çš„å…¬äº¤æ–¹æ¡ˆã€‚\n"
    
    return result

def build_dynamic_reasoning_prompt(doc_query_available=True):
    """æ ¹æ®æ–‡æ¡£æŸ¥è¯¢å¯ç”¨æ€§åŠ¨æ€æ„å»ºæ¨ç†æç¤ºè¯"""
    base_prompt = f"""
èƒŒæ™¯ï¼šæ‰€åœ¨åŸå¸‚æ˜¯{current_city}ï¼Œç”¨æˆ·ä¼šè¯¢é—®ä½ å…³äº{current_city}æ—…æ¸¸çš„ä»»ä½•é—®é¢˜ã€‚
åŸºäºå½“å‰è·å¾—çš„å·¥å…·è°ƒç”¨ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯å®Œæ•´å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

åˆ¤æ–­æ ‡å‡†ï¼š
- ç”¨æˆ·é—®é¢˜çš„æ‰€æœ‰å…³é”®ä¿¡æ¯ç‚¹æ˜¯å¦éƒ½å·²è·å–ï¼Ÿ
- æ˜¯å¦è¿˜æœ‰æ˜æ˜¾ç¼ºå¤±çš„æ•°æ®ï¼Ÿ
- å½“å‰ä¿¡æ¯æ˜¯å¦è¶³ä»¥ç»™å‡ºæ»¡æ„çš„å›ç­”ï¼Ÿ
- å¯¹äºä¾èµ–è°ƒç”¨åœºæ™¯ï¼šæ˜¯å¦è·å¾—äº†æ‰§è¡Œä¸‹ä¸€æ­¥æ‰€éœ€çš„å‚æ•°ï¼Ÿ
- å¯¹äºç‹¬ç«‹è°ƒç”¨åœºæ™¯ï¼šæ˜¯å¦è¦†ç›–äº†ç”¨æˆ·è¯¢é—®çš„æ‰€æœ‰åœ°ç‚¹/äº‹é¡¹ï¼Ÿ
- å¯¹äºé™„è¿‘æœç´¢ç»“æœï¼šå¦‚æœæœç´¢åŠå¾„å·²æ»¡è¶³ç”¨æˆ·è¦æ±‚ï¼Œæ— éœ€å†æ¬¡éªŒè¯è·ç¦»

è¿”å›æ ¼å¼ï¼š
SUFFICIENT: true/false
REASON: [è¯¦ç»†çš„åˆ¤æ–­ç†ç”±ï¼Œè¯´æ˜å·²è·å¾—å“ªäº›ä¿¡æ¯ï¼Œè¿˜ç¼ºå°‘ä»€ä¹ˆä¿¡æ¯]
NEXT_INSTRUCTION: [å¦‚æœä¸å……åˆ†ï¼Œç”Ÿæˆä¸‹ä¸€æ¡å·¥å…·è°ƒç”¨æŒ‡ä»¤ï¼Œæ ¼å¼ä¸º [{{"name": "å·¥å…·å", "parameters": {{...}}}}]]
"""
    
    if doc_query_available:
        tools_section = """
ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘
ç³»ç»Ÿæ”¯æŒä»¥ä¸‹5ä¸ªå·¥å…·ï¼Œå·¥å…·åç§°å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ï¼š
1. "è·å–å¤©æ°”ä¿¡æ¯" - æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”æƒ…å†µ
2. "æœç´¢å…´è¶£ç‚¹" - å…³é”®è¯æœç´¢POIä¿¡æ¯
3. "é™„è¿‘æœç´¢" - ä»¥æŸä¸ªä½ç½®ä¸ºä¸­å¿ƒæœç´¢å‘¨è¾¹POIä¿¡æ¯
4. "ç›®çš„åœ°è·ç¦»" - æµ‹é‡ä¸¤ä¸ªåœ°ç‚¹ä¹‹é—´çš„è·ç¦»
5. "æ–‡æ¡£æŸ¥è¯¢" - ä»æœ¬åœ°çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶å›ç­”é—®é¢˜
"""
        tool_names = '"è·å–å¤©æ°”ä¿¡æ¯"ã€"æœç´¢å…´è¶£ç‚¹"ã€"é™„è¿‘æœç´¢"ã€"ç›®çš„åœ°è·ç¦»"ã€"æ–‡æ¡£æŸ¥è¯¢"'
    else:
        tools_section = """
ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘
ç³»ç»Ÿæ”¯æŒä»¥ä¸‹4ä¸ªå·¥å…·ï¼Œå·¥å…·åç§°å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ï¼š
1. "è·å–å¤©æ°”ä¿¡æ¯" - æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”æƒ…å†µ
2. "æœç´¢å…´è¶£ç‚¹" - å…³é”®è¯æœç´¢POIä¿¡æ¯
3. "é™„è¿‘æœç´¢" - ä»¥æŸä¸ªä½ç½®ä¸ºä¸­å¿ƒæœç´¢å‘¨è¾¹POIä¿¡æ¯
4. "ç›®çš„åœ°è·ç¦»" - æµ‹é‡ä¸¤ä¸ªåœ°ç‚¹ä¹‹é—´çš„è·ç¦»

âš ï¸ **é‡è¦æé†’**ï¼šæ–‡æ¡£æŸ¥è¯¢å·¥å…·å½“å‰ä¸å¯ç”¨ï¼ˆç¼ºå°‘æ–‡æ¡£æˆ–å‘é‡ç´¢å¼•ï¼‰ï¼Œä¸è¦å°è¯•ç”Ÿæˆ"æ–‡æ¡£æŸ¥è¯¢"å·¥å…·è°ƒç”¨æŒ‡ä»¤ã€‚
"""
        tool_names = '"è·å–å¤©æ°”ä¿¡æ¯"ã€"æœç´¢å…´è¶£ç‚¹"ã€"é™„è¿‘æœç´¢"ã€"ç›®çš„åœ°è·ç¦»"'
    
    strategy_section = """
ã€å¤šå·¥å…·è°ƒç”¨ç­–ç•¥ã€‘
æŸäº›å¤æ‚æŸ¥è¯¢å¯èƒ½éœ€è¦å¤šè½®å·¥å…·è°ƒç”¨ï¼š
1. **ä¾èµ–è°ƒç”¨åœºæ™¯**ï¼šåç»­å·¥å…·éœ€è¦å‰é¢å·¥å…·çš„ç»“æœ
    - ç¤ºä¾‹ï¼š"ä¸‡ç»¿å›­é™„è¿‘çš„é…’åº—" â†’ å…ˆæœç´¢ä¸‡ç»¿å›­è·å–åæ ‡ â†’ å†æœç´¢é™„è¿‘é…’åº—
2. **ç‹¬ç«‹è°ƒç”¨åœºæ™¯**ï¼šç”¨æˆ·è¯¢é—®å¤šä¸ªç‹¬ç«‹çš„åœ°ç‚¹æˆ–ä¿¡æ¯
    - ç¤ºä¾‹ï¼š"äº”å…¬ç¥ å’Œæµ·å—çœåšç‰©é¦†çš„ä½ç½®" â†’ åˆ†åˆ«æœç´¢ä¸¤ä¸ªåœ°ç‚¹
3. **è·ç¦»æµ‹é‡åœºæ™¯**ï¼šéœ€è¦å…ˆè·å–ä¸¤ä¸ªåœ°ç‚¹çš„åæ ‡
    - ç¤ºä¾‹ï¼š"ä»Aåœ°åˆ°Båœ°å¤šè¿œ" â†’ æœç´¢Aåœ°åæ ‡ â†’ æœç´¢Båœ°åæ ‡ â†’ è®¡ç®—è·ç¦»
"""
    
    tools_details = """
å·¥å…·ä½¿ç”¨è¯´æ˜ï¼š
è·å–å¤©æ°”ä¿¡æ¯ï¼š
## å·¥å…·è¯´æ˜ï¼šæŸ¥è¯¢æŒ‡å®šåŸå¸‚æˆ–åœ°ç‚¹çš„å¤©æ°”æƒ…å†µï¼Œé€‚ç”¨äºç”¨æˆ·è¯¢é—®å¤©æ°”æ—¶ã€‚
## å‚æ•°ï¼šcityï¼ˆåŸå¸‚åæˆ–adcodeï¼Œå­—ç¬¦ä¸²ï¼‰
## è°ƒç”¨å·¥å…·çš„Promptç¤ºä¾‹ï¼š
- "æµ·å£çš„å¤©æ°”å¦‚ä½•ï¼Ÿ"
## è°ƒç”¨æŒ‡ä»¤ï¼š
{"name": "è·å–å¤©æ°”ä¿¡æ¯", "parameters": {"city": "æµ·å£"}}

æœç´¢å…´è¶£ç‚¹ï¼š
## å·¥å…·è¯´æ˜ï¼šå…³é”®è¯æœç´¢æŸåœ°çš„ç›¸å…³POIä¿¡æ¯ï¼Œé€‚ç”¨äºç”¨æˆ·è¯¢é—®æŸåœ°æœ‰ä»€ä¹ˆå¥½ç©çš„ã€å¥½åƒçš„ç­‰ã€‚
## å‚æ•°ï¼škeywordsï¼ˆå…³é”®è¯ï¼Œå­—ç¬¦ä¸²ï¼‰ï¼Œcityï¼ˆåŸå¸‚åï¼Œå¿…å¡«ï¼Œå­—ç¬¦ä¸²ï¼‰
## è°ƒç”¨å·¥å…·çš„Promptç¤ºä¾‹ï¼š
- "æµ·å£çš„æµ·ç”¸å²›ä¸Šæœ‰ä»€ä¹ˆè±ªåé…’åº—ï¼Ÿ"
## è°ƒç”¨æŒ‡ä»¤ï¼š
{"name": "æœç´¢å…´è¶£ç‚¹", "parameters": {"keywords": "æµ·ç”¸å²›è±ªåå‹é…’åº—", "city": "æµ·å£"}}

é™„è¿‘æœç´¢ï¼š
## å·¥å…·è¯´æ˜ï¼šä»¥æŸä¸ªä½ç½®ä¸ºä¸­å¿ƒç‚¹æœç´¢å‘¨è¾¹POIä¿¡æ¯ï¼Œé€‚ç”¨äºç”¨æˆ·è¯¢é—®"é™„è¿‘"ã€"å‘¨è¾¹"ç­‰åœºæ™¯ã€‚
## å‚æ•°è¯´æ˜ï¼š
1. locationï¼ˆå¿…å¡«ï¼‰ï¼šä¸­å¿ƒç‚¹ç»çº¬åº¦
   - æ ¼å¼ï¼šç»åº¦,çº¬åº¦ï¼ˆä¸å¸¦å¼•å·ï¼‰
   - ç¤ºä¾‹ï¼š110.312589,20.055793
   - æ³¨æ„ï¼šç»çº¬åº¦ä¹‹é—´ç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œä¸è¦å¸¦ç©ºæ ¼
   - âš ï¸ é‡è¦ï¼šå¦‚æœä¸çŸ¥é“åœ°ç‚¹çš„ç»çº¬åº¦ï¼Œéœ€è¦å…ˆè°ƒç”¨"æœç´¢å…´è¶£ç‚¹"å·¥å…·è·å–
2. keywordsï¼ˆå¿…å¡«ï¼‰ï¼šæœç´¢å…³é”®è¯
   - ç¤ºä¾‹ï¼š"é…’åº—"ã€"é¤å…"ã€"æ™¯ç‚¹"ç­‰
3. typesï¼ˆå¿…å¡«ï¼‰ï¼šPOIç±»å‹
   - å¯ä»¥ä¸ºç©ºå­—ç¬¦ä¸²ï¼š""
4. radiusï¼ˆå¿…å¡«ï¼‰ï¼šæœç´¢åŠå¾„
   - å•ä½ï¼šç±³
   - ç¤ºä¾‹ï¼š1000ï¼ˆè¡¨ç¤º1å…¬é‡Œï¼‰
   - é»˜è®¤å€¼ï¼š1000
## è°ƒç”¨ç¤ºä¾‹ï¼š
- "ä¸‡ç»¿å›­é™„è¿‘æœ‰ä»€ä¹ˆé…’åº—ï¼Ÿ"
## è°ƒç”¨é€»è¾‘ï¼š
1. å¦‚æœä¸çŸ¥é“ä¸‡ç»¿å›­çš„ç»çº¬åº¦ï¼Œå…ˆè°ƒç”¨ï¼š{"name": "æœç´¢å…´è¶£ç‚¹", "parameters": {"keywords": "ä¸‡ç»¿å›­", "city": "æµ·å£"}}
2. è·å¾—ç»çº¬åº¦åï¼Œå†è°ƒç”¨ï¼š{"name": "é™„è¿‘æœç´¢", "parameters": {"location": "110.312589,20.055793", "keywords": "é…’åº—", "types": "", "radius": 1000}}

ç›®çš„åœ°è·ç¦»ï¼š
## å·¥å…·è¯´æ˜ï¼šæµ‹é‡ä¸¤ä¸ªåœ°ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œé€‚ç”¨äºç”¨æˆ·è¯¢é—®è·ç¦»ã€è·¯ç¨‹ç­‰åœºæ™¯ã€‚
## å‚æ•°ï¼š
- originï¼ˆèµ·ç‚¹ç»çº¬åº¦ï¼Œå¿…å¡«ï¼Œå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"ï¼‰
- destinationï¼ˆç»ˆç‚¹ç»çº¬åº¦ï¼Œå¿…å¡«ï¼Œå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"ï¼‰
- typeï¼ˆè·ç¦»ç±»å‹ï¼Œå¯é€‰ï¼Œå­—ç¬¦ä¸²ï¼Œé»˜è®¤"1"ï¼‰
  * "0"ï¼šç›´çº¿è·ç¦»
  * "1"ï¼šé©¾è½¦å¯¼èˆªè·ç¦»
  * "3"ï¼šæ­¥è¡Œå¯¼èˆªè·ç¦»
## è°ƒç”¨å·¥å…·çš„Promptç¤ºä¾‹ï¼š
- "ä»æµ·å£æ¹¾å¹¿åœºåˆ°å‡æ—¥æµ·æ»©æœ‰å¤šè¿œï¼Ÿ"
## è°ƒç”¨é€»è¾‘ï¼š
âš ï¸ å¦‚æœä¸çŸ¥é“åœ°ç‚¹çš„ç»çº¬åº¦ï¼Œéœ€è¦å…ˆåˆ†åˆ«è°ƒç”¨"æœç´¢å…´è¶£ç‚¹"å·¥å…·è·å–èµ·ç‚¹å’Œç»ˆç‚¹çš„åæ ‡
1. æœç´¢èµ·ç‚¹ï¼š{{"name": "æœç´¢å…´è¶£ç‚¹", "parameters": {{"keywords": "æµ·å£æ¹¾å¹¿åœº", "city": "æµ·å£"}}}}
2. æœç´¢ç»ˆç‚¹ï¼š{{"name": "æœç´¢å…´è¶£ç‚¹", "parameters": {{"keywords": "å‡æ—¥æµ·æ»©", "city": "æµ·å£"}}}}
3. è®¡ç®—è·ç¦»ï¼š{"name": "ç›®çš„åœ°è·ç¦»", "parameters": {"origin": "110.312589,20.055793", "destination": "110.237390,20.036904"}}
"""
    
    if doc_query_available:
        # è·å–æ–‡æ¡£åˆ—è¡¨
        cache_status = check_cache_and_docs_status()
        doc_list_section = ""
        if cache_status.get('doc_list'):
            doc_list_section = "\n## å¯æŸ¥è¯¢çš„æ–‡æ¡£èµ„æ–™ï¼š\n"
            for doc in cache_status['doc_list']:
                doc_list_section += f"- {doc['name']}: {doc['description']}\n"
        
        doc_query_details = """
æ–‡æ¡£æŸ¥è¯¢ï¼š
## å·¥å…·è¯´æ˜ï¼šä»æœ¬åœ°çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œé€‚ç”¨äºè¯¢é—®æµ·å—æ—…æ¸¸æ”»ç•¥ã€å…ç¨æ”¿ç­–ã€é…’åº—é¤å…æ¨èç­‰æœ¬åœ°çŸ¥è¯†ã€‚""" + doc_list_section + """
## âš ï¸ **é‡è¦**ï¼šæ–‡æ¡£æŸ¥è¯¢çš„èŒƒå›´ä»…é™äºä¸Šè¿°æ–‡æ¡£åˆ—è¡¨ä¸­çš„å†…å®¹ï¼Œåªæœ‰å½“ç”¨æˆ·é—®é¢˜ä¸åˆ—è¡¨ä¸­çš„æ–‡æ¡£å†…å®¹ç›¸å…³æ—¶æ‰ä½¿ç”¨æ­¤å·¥å…·ï¼
## å‚æ•°ï¼šqueryï¼ˆæ£€ç´¢å…³é”®è¯ï¼Œå­—ç¬¦ä¸²ï¼‰- **é‡è¦**ï¼šéœ€è¦æ ¹æ®ç”¨æˆ·çš„å®Œæ•´é—®é¢˜æå–æ ¸å¿ƒæ£€ç´¢å…³é”®è¯ï¼Œå»é™¤è¯­æ°”è¯ã€æ— å…³ä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´ã€å‡†ç¡®çš„æŸ¥è¯¢è¯
## å‚æ•°ç”ŸæˆåŸåˆ™ï¼š
- ä»ç”¨æˆ·é—®é¢˜ä¸­æå–æœ€æ ¸å¿ƒçš„æŸ¥è¯¢æ„å›¾
- å»é™¤è¯­æ°”è¯ã€æ„Ÿå¹è¯ã€æ— å…³çš„ä¿®é¥°è¯­
- ä¿ç•™å…³é”®çš„åœ°ç‚¹ã€ç±»å‹ã€ç‰¹å¾ç­‰å®ä½“ä¿¡æ¯
- ä½¿ç”¨ç®€æ´æ˜äº†çš„å…³é”®è¯ç»„åˆ
## è°ƒç”¨å·¥å…·çš„Promptç¤ºä¾‹ï¼š
- ç”¨æˆ·é—®ï¼š"å“å‘€ï¼Œæˆ‘æƒ³çŸ¥é“æµ·å—å…ç¨è´­ç‰©åˆ°åº•æœ‰ä»€ä¹ˆé™åˆ¶å•Šï¼Ÿå¬è¯´æŒºå¤æ‚çš„" â†’ query: "æµ·å—å…ç¨è´­ç‰©é™åˆ¶"
- ç”¨æˆ·é—®ï¼š"èƒ½ä¸èƒ½æ¨èä¸€äº›æµ·å£çš„ç‰¹è‰²é¤å…ï¼Œæˆ‘æ¯”è¾ƒå–œæ¬¢æµ·é²œ" â†’ query: "æµ·å£ç‰¹è‰²æµ·é²œé¤å…"
- ç”¨æˆ·é—®ï¼š"æˆ‘ä»¬ä¸€å®¶ä¸‰å£è¦å»æµ·å£æ—…æ¸¸ï¼Œå­©å­8å²ï¼Œæœ‰ä»€ä¹ˆé€‚åˆäº²å­æ¸¸çš„é…’åº—å—ï¼Ÿ" â†’ query: "æµ·å£äº²å­é…’åº—"
## è°ƒç”¨æŒ‡ä»¤ï¼š
{"name": "æ–‡æ¡£æŸ¥è¯¢", "parameters": {"query": "æµ·å—å…ç¨è´­ç‰©é™åˆ¶"}}
"""
    else:
        doc_query_details = ""
    
    final_instructions = f"""
ã€é‡è¦æŒ‡ä»¤ã€‘
- å·¥å…·åç§°å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä»¥ä¸‹å·¥å…·åç§°ï¼š{tool_names}
- æœ‰ç»çº¬åº¦å‚æ•°éƒ½å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·åŒ…è£¹ï¼Œä½œä¸ºå­—ç¬¦ä¸²ä¼ é€’ï¼Œä¾‹å¦‚"110.237390,20.036904"
- å‚æ•°å¿…é¡»æŒ‰ç…§é¡ºåºæä¾›ï¼šlocation -> keywords -> types -> radius
- location å‚æ•°ä¸è¦å¸¦åŒå¼•å·
- types å‚æ•°å³ä½¿ä¸ºç©ºä¹Ÿå¿…é¡»ä¼ å…¥ç©ºå­—ç¬¦ä¸²
- radius å‚æ•°å¯ä»¥æ˜¯æ•°å­—æˆ–å­—ç¬¦ä¸²ç±»å‹
"""
    
    return base_prompt + tools_section + strategy_section + tools_details + doc_query_details + final_instructions

def build_context_for_llm_call(user_question, tool_call_history, call_type, doc_query_available=True):
    """ä¸ºä¸åŒç±»å‹çš„LLMè°ƒç”¨æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
    context = [user_question]  # å§‹ç»ˆä»¥ç”¨æˆ·é—®é¢˜å¼€å¤´
    
    # æ·»åŠ æ‰€æœ‰å·¥å…·è°ƒç”¨å†å²
    for record in tool_call_history:
        context.append({
            "role": "assistant",
            "content": record["instruction"]
        })
        context.append({
            "role": "system", 
            "content": f"MCPå·¥å…·è¿”å›ä¿¡æ¯ï¼š\n{record['result']}"
        })
    
    # æ ¹æ®è°ƒç”¨ç±»å‹æ·»åŠ ç›¸åº”çš„ç³»ç»Ÿæç¤ºè¯
    if call_type == "reasoning":
        # æ ¹æ®æ–‡æ¡£æŸ¥è¯¢å¯ç”¨æ€§åŠ¨æ€æ„å»ºæ¨ç†æç¤ºè¯
        reasoning_prompt = build_dynamic_reasoning_prompt(doc_query_available)
        context.append({
            "role": "system",
            "content": reasoning_prompt
        })
    elif call_type == "final_response":
        context.append({
            "role": "system",
            "content": FINAL_RESPONSE_SYSTEM_PROMPT
        })
    
    return context

def detect_tool_call_loop(tool_call_history):
    """æ£€æµ‹å·¥å…·è°ƒç”¨å¾ªç¯ï¼Œé¿å…æ— é™é‡å¤"""
    if len(tool_call_history) < LOOP_DETECTION_WINDOW:
        return False
    
    # æå–æœ€è¿‘çš„å·¥å…·è°ƒç”¨ç­¾å
    recent_instructions = [
        record["instruction"] 
        for record in tool_call_history[-LOOP_DETECTION_WINDOW:]
    ]
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„å·¥å…·è°ƒç”¨
    return len(recent_instructions) != len(set(recent_instructions))

def optimize_context_length(context, max_tokens=MAX_CONTEXT_LENGTH):
    """æ™ºèƒ½å‹ç¼©ä¸Šä¸‹æ–‡ï¼Œä¿ç•™æœ€é‡è¦ä¿¡æ¯"""
    # ç®€å•å®ç°ï¼šä¼°ç®—tokenæ•°é‡
    estimated_tokens = sum(len(str(msg.get("content", ""))) // 4 for msg in context)
    
    if estimated_tokens <= max_tokens:
        return context
    
    # ä¿ç•™ç­–ç•¥ï¼šç”¨æˆ·é—®é¢˜ + æœ€è¿‘Nè½®å·¥å…·è°ƒç”¨ + ç³»ç»Ÿæç¤ºè¯
    if len(context) <= 3:
        return context
        
    user_question = context[0]
    system_prompt = context[-1]
    
    # ä¿ç•™æœ€è¿‘çš„å·¥å…·è°ƒç”¨å†å²
    middle_content = context[1:-1]
    # ä¿ç•™æœ€è¿‘6æ¡æ¶ˆæ¯ï¼ˆçº¦3è½®å·¥å…·è°ƒç”¨ï¼‰
    optimized_middle = middle_content[-6:] if len(middle_content) > 6 else middle_content
    
    return [user_question] + optimized_middle + [system_prompt]

def parse_reasoning_result(llm_reply):
    """è§£æLLMçš„æ¨ç†åˆ¤æ–­ç»“æœ"""
    try:
        # è§£æSUFFICIENTå­—æ®µ
        sufficient_match = re.search(r'SUFFICIENT:\s*(true|false)', llm_reply, re.IGNORECASE)
        sufficient = sufficient_match.group(1).lower() == 'true' if sufficient_match else False
        
        # è§£æREASONå­—æ®µ
        reason_match = re.search(r'REASON:\s*(.+?)(?=\nNEXT_INSTRUCTION|$)', llm_reply, re.DOTALL)
        reason = reason_match.group(1).strip() if reason_match else "æ— æ³•è§£ææ¨ç†åŸå› "
        
        # è§£æNEXT_INSTRUCTIONå­—æ®µ
        next_instruction = None
        if not sufficient:
            instruction_match = re.search(r'NEXT_INSTRUCTION:\s*(.+?)$', llm_reply, re.DOTALL | re.MULTILINE)
            if instruction_match:
                next_instruction = instruction_match.group(1).strip()
                # å°è¯•æå–JSONæ ¼å¼çš„å·¥å…·è°ƒç”¨æŒ‡ä»¤
                json_match = re.search(r'\[(.*?)\]', next_instruction, re.DOTALL)
                if json_match:
                    next_instruction = json_match.group(0)
        
        return {
            "sufficient": sufficient,
            "reason": reason,
            "next_instruction": next_instruction
        }
    except Exception as e:
        logging.error(f"è§£ææ¨ç†ç»“æœå¤±è´¥: {str(e)}")
        return {
            "sufficient": True,  # é»˜è®¤ç»ˆæ­¢ï¼Œé¿å…æ— é™å¾ªç¯
            "reason": "è§£ææ¨ç†ç»“æœå¤±è´¥ï¼Œå¼ºåˆ¶ç»ˆæ­¢",
            "next_instruction": None
        }

def analyze_information_sufficiency(user_question, tool_call_history, doc_query_available=True):
    """LLMåˆ†æä¿¡æ¯å……åˆ†æ€§å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
    try:
        # æ„å»ºæ¨ç†ä¸Šä¸‹æ–‡
        context = build_context_for_llm_call(user_question, tool_call_history, "reasoning", doc_query_available)
        
        # ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦
        context = optimize_context_length(context)
        
        # è¾“å‡ºå‘é€ç»™æ¨ç†æ¨¡å‹çš„ä¸Šä¸‹æ–‡
        logging.info(f"[CONTEXT_TO_REASONING] ä½¿ç”¨æ¨¡å‹: {REASONING_MODEL}, è¶…æ—¶: {REASONING_TIMEOUT}ç§’")
        logging.info(f"[CONTEXT_TO_REASONING] å‘é€ç»™æ¨ç†æ¨¡å‹çš„ä¸Šä¸‹æ–‡:\n{format_context_for_debug(context, full_output_for_reasoning=True)}")
        
        # è°ƒç”¨LLMè¿›è¡Œæ¨ç†åˆ¤æ–­
        completion = client.chat.completions.create(
            model=REASONING_MODEL,
            messages=context,
            timeout=REASONING_TIMEOUT
        )
        
        llm_reply = completion.choices[0].message.content
        logging.info(f"[REASONING_REPLY] {llm_reply}")
        
        # è§£ææ¨ç†ç»“æœ
        return parse_reasoning_result(llm_reply)
        
    except Exception as e:
        logging.error(f"æ¨ç†åˆ¤æ–­å¤±è´¥: {str(e)}")
        return {
            "sufficient": True,  # å‡ºé”™æ—¶é»˜è®¤ç»ˆæ­¢
            "reason": f"æ¨ç†åˆ¤æ–­å¤±è´¥: {str(e)}",
            "next_instruction": None
        }

def reasoning_based_tool_calling(user_question, initial_messages, tool_use, now_beijing, doc_query_available=True):
    """
    åŸºäºæ¨ç†åˆ¤æ–­çš„å¤šå·¥å…·è°ƒç”¨æ ¸å¿ƒç®—æ³•
    
    æµç¨‹ï¼š
    1. å¯¹è¯é˜¶æ®µï¼šç†è§£ç”¨æˆ·æ„å›¾ï¼Œå†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    2. å¾ªç¯æ‰§è¡Œï¼šå·¥å…·è°ƒç”¨ â†’ æ¨ç†åˆ¤æ–­ â†’ ç»§ç»­æˆ–ç»“æŸ
    3. æœ€ç»ˆå›å¤ï¼šåŸºäºå·¥å…·ç»“æœç”Ÿæˆç”¨æˆ·å‹å¥½çš„å›å¤
    """
    tool_call_history = []
    iteration = 0
    
    # å¯¹è¯é˜¶æ®µï¼šå¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œå†³å®šæ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
    try:
        # è¾“å‡ºå‘é€ç»™å¯¹è¯æ¨¡å‹çš„ä¸Šä¸‹æ–‡
        logging.info(f"[CONTEXT_TO_CHAT] ä½¿ç”¨æ¨¡å‹: {TOOL_GENERATION_MODEL}")
        logging.info(f"[CONTEXT_TO_CHAT] å‘é€ç»™å¯¹è¯æ¨¡å‹çš„ä¸Šä¸‹æ–‡:\n{format_context_for_debug(initial_messages)}")
        
        completion = client.chat.completions.create(
            model=TOOL_GENERATION_MODEL,
            messages=initial_messages,
        )
        llm_reply = completion.choices[0].message.content
        logging.info(f"[CHAT_REPLY] {llm_reply}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
        if "NEED_TOOLS" in llm_reply:
            # éœ€è¦å·¥å…·è°ƒç”¨ï¼Œè®©æ¨ç†æ¨¡å‹æ¥å†³å®šç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨
            logging.info("[CHAT_DECISION] å¯¹è¯æ¨¡å‹åˆ¤æ–­éœ€è¦å·¥å…·è°ƒç”¨ï¼Œè½¬äº¤æ¨ç†æ¨¡å‹å¤„ç†")
            # ç›´æ¥è¿›å…¥æ¨ç†é˜¶æ®µï¼Œè®©æ¨ç†æ¨¡å‹ç”Ÿæˆç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨æŒ‡ä»¤
            reasoning_result = analyze_information_sufficiency(user_question, tool_call_history, doc_query_available)
            if reasoning_result["next_instruction"]:
                current_instruction = reasoning_result["next_instruction"]
            else:
                return "æŠ±æ­‰ï¼Œæ— æ³•ç¡®å®šéœ€è¦è°ƒç”¨ä»€ä¹ˆå·¥å…·ã€‚", tool_call_history, True
        else:
            # ä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›LLMå›å¤
            logging.info("[CHAT_DECISION] å¯¹è¯æ¨¡å‹åˆ¤æ–­ä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›å¤")
            return llm_reply, tool_call_history, False
        
    except Exception as e:
        logging.error(f"å¯¹è¯é˜¶æ®µå¤„ç†å¤±è´¥: {str(e)}")
        return "æŠ±æ­‰ï¼Œå¯¹è¯å¤„ç†å¤±è´¥ã€‚", tool_call_history, True
    
    # å¾ªç¯æ‰§è¡Œå·¥å…·è°ƒç”¨å’Œæ¨ç†åˆ¤æ–­
    while iteration < MAX_TOOL_ITERATIONS:
        iteration += 1
        logging.info(f"[ITERATION] ç¬¬{iteration}è½®å·¥å…·è°ƒç”¨")
        
        # æ£€æµ‹å¾ªç¯
        if detect_tool_call_loop(tool_call_history):
            logging.warning("æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨å¾ªç¯ï¼Œç»ˆæ­¢æ‰§è¡Œ")
            break
        
        # è§£æå’Œæ‰§è¡Œå·¥å…·è°ƒç”¨
        try:
            tool_calls = json.loads(current_instruction)
            if not tool_calls or not isinstance(tool_calls, list):
                break
                
            tool = tool_calls[0]
            tool_name = tool.get("name")
            params = tool.get("parameters", {})
            
            # è®°å½•å·¥å…·è°ƒç”¨
            tool_use.append({
                "type": "tool_call",
                "icon": "âš™ï¸",
                "title": f"è°ƒç”¨[{tool_name}]å·¥å…·......",
                "tool_name": tool_name,
                "content": json.dumps(params, ensure_ascii=False),
                "timestamp": now_beijing(),
                "collapsible": True
            })
            
            # æ‰§è¡ŒMCPå·¥å…·è°ƒç”¨
            tool_result, tool_failed = call_mcp_tool_and_format_result(
                tool_name, params, tool_use, now_beijing, mcp_client
            )
            
            if tool_failed:
                logging.error(f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_name}")
                break
            
            # æ›´æ–°å·¥å…·è°ƒç”¨å†å²
            tool_call_history.append({
                "instruction": current_instruction,
                "result": tool_result,
                "timestamp": now_beijing(),
                "iteration": iteration
            })
            
            logging.info(f"[TOOL_RESULT] ç¬¬{iteration}è½®å·¥å…·è°ƒç”¨å®Œæˆ")
            
        except Exception as e:
            logging.error(f"æ‰§è¡Œå·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}")
            break
        
        # æ¨ç†åˆ¤æ–­ä¿¡æ¯å……åˆ†æ€§
        reasoning_result = analyze_information_sufficiency(user_question, tool_call_history, doc_query_available)
        
        if reasoning_result["sufficient"]:
            logging.info(f"[REASONING] ä¿¡æ¯å……åˆ†ï¼Œç»ˆæ­¢å¾ªç¯ã€‚åŸå› : {reasoning_result['reason']}")
            break
        else:
            logging.info(f"[REASONING] ä¿¡æ¯ä¸è¶³ï¼Œç»§ç»­è°ƒç”¨ã€‚åŸå› : {reasoning_result['reason']}")
            if reasoning_result["next_instruction"]:
                current_instruction = reasoning_result["next_instruction"]
            else:
                logging.warning("æ— æ³•ç”Ÿæˆä¸‹ä¸€æ¡æŒ‡ä»¤ï¼Œç»ˆæ­¢å¾ªç¯")
                break
    
    # ç”Ÿæˆæœ€ç»ˆå›å¤
    try:
        final_context = build_context_for_llm_call(user_question, tool_call_history, "final_response", doc_query_available)
        final_context = optimize_context_length(final_context)
        
        # è¾“å‡ºå‘é€ç»™æœ€ç»ˆå›å¤æ¨¡å‹çš„ä¸Šä¸‹æ–‡
        logging.info(f"[CONTEXT_TO_FINAL_RESPONSE] ä½¿ç”¨æ¨¡å‹: {FINAL_RESPONSE_MODEL}")
        logging.info(f"[CONTEXT_TO_FINAL_RESPONSE] å‘é€ç»™æœ€ç»ˆå›å¤æ¨¡å‹çš„ä¸Šä¸‹æ–‡:\n{format_context_for_debug(final_context)}")
        
        completion = client.chat.completions.create(
            model=FINAL_RESPONSE_MODEL,
            messages=final_context,
        )
        
        final_reply = completion.choices[0].message.content
        logging.info(f"[FINAL_REPLY] {final_reply}")
        
        return final_reply, tool_call_history, False
        
    except Exception as e:
        logging.error(f"ç”Ÿæˆæœ€ç»ˆå›å¤å¤±è´¥: {str(e)}")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆæœ€ç»ˆå›å¤å¤±è´¥ã€‚", tool_call_history, True

def call_mcp_tool_and_format_result(tool_name, params, tool_use, now, mcp_client, user_question=None):
    """
    æ ¹æ®å·¥å…·åå’Œå‚æ•°è°ƒç”¨MCPï¼Œå¹¶æ ¼å¼åŒ–ç»“æœï¼Œè¿”å› (tool_result, tool_failed)
    
    Args:
        tool_name: å·¥å…·åç§°
        params: å·¥å…·å‚æ•°
        tool_use: å·¥å…·ä½¿ç”¨è®°å½•åˆ—è¡¨
        now: æ—¶é—´æˆ³å‡½æ•°
        mcp_client: MCPå®¢æˆ·ç«¯
        user_question: ç”¨æˆ·åŸå§‹é—®é¢˜ï¼ˆç”¨äºæ–‡æ¡£æŸ¥è¯¢ç­‰éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡çš„å·¥å…·ï¼‰
    """
    tool_result = None
    tool_failed = False
    try:
        match tool_name:
            case "è·å–å¤©æ°”ä¿¡æ¯":
                city = params.get("location") or params.get("city") or "æµ·å£"
                weather_data = mcp_client.get_weather(city)
                tool_use.append({
                    "type": "tool_result",
                    "icon": "âš™ï¸",
                    "title": f"[{tool_name}]å·¥å…·è¿”å›ä¿¡æ¯......",
                    "content": json.dumps(weather_data, ensure_ascii=False, indent=2),
                    "formatted": tool_result,
                    "collapsible": True,
                    "timestamp": now()
                })
                if weather_data:
                    tool_result = format_weather_data({"city": city, "data": weather_data, "type": "weather"})
                else:
                    tool_failed = True

            case "æœç´¢å…´è¶£ç‚¹":
                keywords = params.get("keywords")
                city = params.get("city", "")
                search_data = mcp_client.search_pois(keywords, city)
                tool_use.append({
                    "type": "tool_result",
                    "icon": "âš™ï¸",
                    "title": f"[{tool_name}]å·¥å…·è¿”å›ä¿¡æ¯......",
                    "content": json.dumps(search_data, ensure_ascii=False, indent=2),
                    "formatted": tool_result,
                    "collapsible": True,
                    "timestamp": now()
                })
                if search_data:
                    tool_result = format_poi_data({"keywords": keywords, "city": city, "data": search_data, "type": "poi"})
                else:
                    tool_failed = True


            case "é™„è¿‘æœç´¢":
                # æŒ‰ç…§ mcp_client_wrapper.py ä¸­ search_around å‡½æ•°çš„å‚æ•°é¡ºåºè·å–å‚æ•°
                location = params.get("location")
                keywords = params.get("keywords")
                types = params.get("types", "")  # ç¡®ä¿typesä¸ºç©ºå­—ç¬¦ä¸²è€Œä¸æ˜¯None
                radius = params.get("radius", 1000)
                
                # æ£€æŸ¥å¿…å¡«å‚æ•°
                if not all([location, keywords is not None, types is not None]):
                    logging.error(f"é™„è¿‘æœç´¢ç¼ºå°‘å¿…å¡«å‚æ•°: location={location}, keywords={keywords}, types={types}")
                    tool_failed = True
                # æ£€æŸ¥locationæ ¼å¼
                elif "," in location:
                    try:
                        # è°ƒç”¨æ—¶ä¸¥æ ¼æŒ‰ç…§å‡½æ•°å®šä¹‰çš„å‚æ•°é¡ºåº
                        search_data = mcp_client.search_around(
                            location=location,
                            keywords=keywords,
                            types=types,
                            radius=radius
                        )
                        tool_use.append({
                            "type": "tool_result",
                            "icon": "âš™ï¸",
                            "title": f"[{tool_name}]å·¥å…·è¿”å›ä¿¡æ¯......",
                            "content": json.dumps(search_data, ensure_ascii=False, indent=2),
                            "formatted": tool_result,
                            "collapsible": True,
                            "timestamp": now()
                        })
                        if search_data:
                            # å¯é‡ç”¨format_poi_dataæ ¼å¼åŒ–
                            tool_result = format_poi_data({"keywords": keywords, "city": "", "data": search_data, "type": "poi"})
                        else:
                            logging.error("search_around è¿”å›äº†ç©ºæ•°æ®")
                            tool_failed = True
                    except Exception as e:
                        logging.error(f"è°ƒç”¨ search_around æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                        tool_failed = True
                else:
                    logging.error(f"location æ ¼å¼é”™è¯¯: {location}")
                    tool_failed = True
                    
            case "ç›®çš„åœ°è·ç¦»":
                origins = params.get("origin")
                destination = params.get("destination")
                type_ = params.get("type", "1")
                if origins and destination:
                    distance_data = mcp_client.get_distance(origins, destination, type_)
                    tool_use.append({
                        "type": "tool_result",
                        "icon": "âš™ï¸",
                        "title": f"[{tool_name}]å·¥å…·è¿”å›ä¿¡æ¯......",
                        "content": json.dumps(distance_data, ensure_ascii=False, indent=2),
                        "formatted": tool_result,
                        "collapsible": True,
                        "timestamp": now()
                    })
                    if distance_data and distance_data.get("results"):
                        result = distance_data["results"][0]
                        dist = result.get("distance", "æœªçŸ¥")
                        duration = result.get("duration", None)
                        tool_result = f"## è·ç¦»æµ‹é‡ç»“æœ\n\n- **èµ·ç‚¹**: {origins}\n- **ç»ˆç‚¹**: {destination}\n- **è·ç¦»**: {dist}ç±³ (çº¦{round(int(dist)/1000, 2) if dist != 'æœªçŸ¥' else 'æœªçŸ¥'}å…¬é‡Œ)\n"
                        if duration:
                            tool_result += f"- **é¢„è®¡è€—æ—¶**: {int(int(duration)/60)}åˆ†é’Ÿ\n"
                    else:
                        tool_failed = True
                else:
                    tool_failed = True
            case "æ–‡æ¡£æŸ¥è¯¢":
                query = params.get("query")
                if not query:
                    tool_failed = True
                    tool_result = "ç¼ºå°‘æŸ¥è¯¢å…³é”®è¯å‚æ•°"
                else:
                    # ä¼˜å…ˆä½¿ç”¨ä¼˜åŒ–åçš„queryè¿›è¡Œæ£€ç´¢ï¼Œç”¨æˆ·åŸå§‹é—®é¢˜ç”¨äºç”Ÿæˆå›ç­”
                    original_question = user_question.get('content', '') if user_question else None
                    success, answer, docs = perform_rag_query(query, original_question)
                    tool_use.append({
                        "type": "tool_result",
                        "icon": "ğŸ“š",
                        "title": f"[{tool_name}]å·¥å…·è¿”å›ä¿¡æ¯......",
                        "content": json.dumps({
                            "query": query,
                            "answer": answer,
                            "relevant_docs_count": len(docs),
                            "similarity_scores": [doc.get('similarity', 0) for doc in docs[:3]] if docs else []
                        }, ensure_ascii=False, indent=2),
                        "formatted": answer if success else None,
                        "collapsible": True,
                        "timestamp": now()
                    })
                    if success:
                        tool_result = answer
                    else:
                        tool_failed = True
                        tool_result = answer  # é”™è¯¯ä¿¡æ¯
            case _:
                tool_failed = True
    except Exception as e:
        tool_failed = True
    return tool_result, tool_failed

@app.route('/api/weather/<city>', methods=['GET'])
def get_weather(city):
    try:
        # ä½¿ç”¨MCPå®¢æˆ·ç«¯è·å–å¤©æ°”ä¿¡æ¯
        weather_data = mcp_client.get_weather(city)
        
        if not weather_data:
            return jsonify({
                "status": "error",
                "message": f"æ— æ³•è·å–{city}çš„å¤©æ°”ä¿¡æ¯"
            }), 404
            
        return jsonify({
            "status": "success",
            "data": weather_data
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/tools', methods=['GET'])
def list_tools():
    try:
        # ä½¿ç”¨MCPå®¢æˆ·ç«¯åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·
        tools = mcp_client.list_tools()
        
        return jsonify({
            "status": "success",
            "data": tools
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/geo/<address>', methods=['GET'])
def get_geo_location(address):
    try:
        # ä»æŸ¥è¯¢å‚æ•°è·å–å¯é€‰çš„åŸå¸‚åï¼ˆå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
        city = request.args.get('city', '')
        
        # ä½¿ç”¨MCPå®¢æˆ·ç«¯è·å–åœ°ç†ç¼–ç ä¿¡æ¯ï¼Œä¼ å…¥åœ°å€å’ŒåŸå¸‚
        geo_data = mcp_client.get_geo_location(address, city)
        
        # å¦‚æœæ²¡æœ‰è·å–åˆ°åœ°ç†ç¼–ç æ•°æ®ï¼Œè¿”å›404é”™è¯¯å’Œæç¤ºä¿¡æ¯
        if not geo_data:
            return jsonify({
                "status": "error",
                "message": f"æ— æ³•è·å–åœ°å€'{address}'çš„åœ°ç†ç¼–ç "
            }), 404
            
        # è·å–æˆåŠŸåˆ™è¿”å›åœ°ç†ç¼–ç æ•°æ®ï¼ŒçŠ¶æ€ä¸ºsuccess
        return jsonify({
            "status": "success",
            "data": geo_data
        })
        
    except Exception as e:
        # æ•è·å¼‚å¸¸ï¼Œè¿”å›500é”™è¯¯å’Œå¼‚å¸¸ä¿¡æ¯
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/directions/driving', methods=['GET'])
def get_driving_directions():
    try:
        origin = request.args.get('origin')
        destination = request.args.get('destination')
        
        if not origin or not destination:
            return jsonify({
                "status": "error",
                "message": "ç¼ºå°‘èµ·ç‚¹æˆ–ç»ˆç‚¹å‚æ•°"
            }), 400
            
        # ä½¿ç”¨MCPå®¢æˆ·ç«¯è·å–é©¾è½¦è·¯å¾„è§„åˆ’
        directions_data = mcp_client.get_driving_directions(origin, destination)
        
        if not directions_data:
            return jsonify({
                "status": "error",
                "message": "æ— æ³•è·å–é©¾è½¦è·¯å¾„è§„åˆ’"
            }), 404
            
        return jsonify({
            "status": "success",
            "data": directions_data
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/directions/walking', methods=['GET'])
def get_walking_directions():
    try:
        origin = request.args.get('origin')
        destination = request.args.get('destination')
        
        if not origin or not destination:
            return jsonify({
                "status": "error",
                "message": "ç¼ºå°‘èµ·ç‚¹æˆ–ç»ˆç‚¹å‚æ•°"
            }), 400
            
        # ä½¿ç”¨MCPå®¢æˆ·ç«¯è·å–æ­¥è¡Œè·¯å¾„è§„åˆ’
        directions_data = mcp_client.get_walking_directions(origin, destination)
        
        if not directions_data:
            return jsonify({
                "status": "error",
                "message": "æ— æ³•è·å–æ­¥è¡Œè·¯å¾„è§„åˆ’"
            }), 404
            
        return jsonify({
            "status": "success",
            "data": directions_data
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/search', methods=['GET'])
def search_pois():
    try:
        keywords = request.args.get('keywords')
        city = request.args.get('city', '')
        page = int(request.args.get('page', 1))
        offset = int(request.args.get('offset', 20))
        
        if not keywords:
            return jsonify({
                "status": "error",
                "message": "ç¼ºå°‘å…³é”®å­—å‚æ•°"
            }), 400
            
        # ä½¿ç”¨MCPå®¢æˆ·ç«¯æœç´¢POI
        search_data = mcp_client.search_pois(keywords, city, page, offset)
        
        if not search_data:
            return jsonify({
                "status": "error",
                "message": f"æ— æ³•è·å–å…³é”®å­—'{keywords}'çš„æœç´¢ç»“æœ"
            }), 404
            
        return jsonify({
            "status": "success",
            "data": search_data
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/poi/detail', methods=['GET'])
def get_poi_detail():
    """
    è·å–POIè¯¦ç»†ä¿¡æ¯
    å‚æ•°ï¼š
        - id: POI IDï¼ˆé€šè¿‡text_search/around_searchè·å–ï¼‰
    """
    try:
        poi_id = request.args.get('id')
        if not poi_id:
            return jsonify({
                "status": "error",
                "message": "ç¼ºå°‘POI IDå‚æ•°"
            }), 400
        detail = mcp_client.get_poi_detail(poi_id)
        if not detail:
            return jsonify({
                "status": "error",
                "message": f"æ— æ³•è·å–POI IDä¸º'{poi_id}'çš„è¯¦ç»†ä¿¡æ¯"
            }), 404
        return jsonify({
            "status": "success",
            "data": detail
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/distance', methods=['GET'])
def get_distance():
    """
    æµ‹é‡ä¸¤ç‚¹é—´è·ç¦»
    å‚æ•°ï¼š
        - origins: èµ·ç‚¹ç»çº¬åº¦ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"ï¼Œæ”¯æŒå¤šä¸ªèµ·ç‚¹ç”¨|åˆ†éš”ï¼Œå¦‚"110.1,20.1|110.2,20.2"
        - destination: ç»ˆç‚¹ç»çº¬åº¦ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"
        - type: è·ç¦»ç±»å‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤1ï¼‰
            - 0ï¼šç›´çº¿è·ç¦»
            - 1ï¼šé©¾è½¦å¯¼èˆªè·ç¦»
            - 3ï¼šæ­¥è¡Œå¯¼èˆªè·ç¦»
    """
    try:
        origins = request.args.get('origins')
        destination = request.args.get('destination')
        type_ = request.args.get('type', '1')
        if not origins or not destination:
            return jsonify({
                "status": "error",
                "message": "ç¼ºå°‘originsæˆ–destinationå‚æ•°"
            }), 400
        result = mcp_client.get_distance(origins, destination, type_)
        if not result:
            return jsonify({
                "status": "error",
                "message": "æ— æ³•æµ‹é‡è·ç¦»"
            }), 404
        return jsonify({
            "status": "success",
            "data": result
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/directions/transit', methods=['GET'])
def get_transit_directions():
    """
    å…¬äº¤/åœ°é“/ç«è½¦ç­‰ç»¼åˆå…¬å…±äº¤é€šè·¯å¾„è§„åˆ’
    å‚æ•°ï¼š
        - origin: èµ·ç‚¹ç»çº¬åº¦ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"
        - destination: ç»ˆç‚¹ç»çº¬åº¦ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"
        - city: èµ·ç‚¹åŸå¸‚åç§°æˆ–adcode
        - cityd: ç»ˆç‚¹åŸå¸‚åç§°æˆ–adcodeï¼ˆè·¨åŸæ—¶å¿…å¡«ï¼‰
    """
    try:
        origin = request.args.get('origin')
        destination = request.args.get('destination')
        city = request.args.get('city')
        cityd = request.args.get('cityd', None)
        if not origin or not destination or not city:
            return jsonify({
                "status": "error",
                "message": "ç¼ºå°‘originã€destinationæˆ–cityå‚æ•°"
            }), 400
        result = mcp_client.get_transit_directions(origin, destination, city, cityd)
        if not result:
            return jsonify({
                "status": "error",
                "message": "æ— æ³•è·å–å…¬äº¤è·¯å¾„è§„åˆ’"
            }), 404
        return jsonify({
            "status": "success",
            "data": result
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/directions/bicycling', methods=['GET'])
def get_bicycling_directions():
    """
    éª‘è¡Œè·¯å¾„è§„åˆ’
    å‚æ•°ï¼š
        - origin: èµ·ç‚¹ç»çº¬åº¦ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"
        - destination: ç»ˆç‚¹ç»çº¬åº¦ï¼Œæ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"
    """
    try:
        origin = request.args.get('origin')
        destination = request.args.get('destination')
        if not origin or not destination:
            return jsonify({
                "status": "error",
                "message": "ç¼ºå°‘originæˆ–destinationå‚æ•°"
            }), 400
        result = mcp_client.get_bicycling_directions(origin, destination)
        if not result:
            return jsonify({
                "status": "error",
                "message": "æ— æ³•è·å–éª‘è¡Œè·¯å¾„è§„åˆ’"
            }), 404
        return jsonify({
            "status": "success",
            "data": result
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@app.route('/api/heartbeat', methods=['GET', 'POST'])
def heartbeat():
    logging.info(f"æ”¶åˆ°å¿ƒè·³è¯·æ±‚: {datetime.datetime.now(beijing_tz).isoformat()}")
    return '', 204

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œæ£€æŸ¥åº”ç”¨å’Œæ¨¡å‹çŠ¶æ€"""
    try:
        health_status = {
            "status": "healthy",
            "timestamp": datetime.datetime.now(beijing_tz).isoformat(),
            "checks": {}
        }
        
        # æ£€æŸ¥åŸºç¡€æœåŠ¡
        health_status["checks"]["app"] = "ok"
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        try:
            global _embedding_model
            if _embedding_model is not None:
                health_status["checks"]["embedding_model"] = "loaded"
            else:
                health_status["checks"]["embedding_model"] = "not_loaded"
        except Exception as e:
            health_status["checks"]["embedding_model"] = f"error: {str(e)}"
        
        # æ£€æŸ¥å‘é‡ç¼“å­˜çŠ¶æ€
        try:
            cache_data = load_embedding_cache()
            if cache_data is not None:
                doc_count = len(cache_data.get('texts', []))
                health_status["checks"]["vector_cache"] = f"loaded ({doc_count} docs)"
            else:
                health_status["checks"]["vector_cache"] = "not_available"
        except Exception as e:
            health_status["checks"]["vector_cache"] = f"error: {str(e)}"
        
        # æ£€æŸ¥MCPå®¢æˆ·ç«¯çŠ¶æ€
        try:
            # ç®€å•æµ‹è¯•MCPå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
            if mcp_client:
                health_status["checks"]["mcp_client"] = "ok"
            else:
                health_status["checks"]["mcp_client"] = "not_available"
        except Exception as e:
            health_status["checks"]["mcp_client"] = f"error: {str(e)}"
        
        # åˆ¤æ–­æ•´ä½“å¥åº·çŠ¶æ€
        failed_checks = [k for k, v in health_status["checks"].items() if "error" in str(v)]
        if failed_checks:
            health_status["status"] = "degraded"
            health_status["failed_checks"] = failed_checks
        
        return jsonify(health_status)
        
    except Exception as e:
        return jsonify({
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.datetime.now(beijing_tz).isoformat()
        }), 500

@app.route('/api/check_rag_status', methods=['GET'])
def check_rag_status():
    """æ£€æŸ¥RAGç³»ç»ŸçŠ¶æ€çš„APIç«¯ç‚¹"""
    try:
        cache_data = load_embedding_cache()
        
        if cache_data is None:
            return jsonify({
                "status": "unavailable",
                "message": "çŸ¥è¯†åº“ç´¢å¼•æœªç”Ÿæˆ",
                "has_cache": False,
                "doc_count": 0
            })
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if not all(key in cache_data for key in ['texts', 'embeddings', 'meta']):
            return jsonify({
                "status": "error",
                "message": "çŸ¥è¯†åº“ç´¢å¼•æ•°æ®ä¸å®Œæ•´",
                "has_cache": True,
                "doc_count": 0
            })
        
        doc_count = len(cache_data.get('texts', []))
        if doc_count == 0:
            return jsonify({
                "status": "empty",
                "message": "çŸ¥è¯†åº“ä¸ºç©º",
                "has_cache": True,
                "doc_count": 0
            })
        
        # è·å–ç¼“å­˜æ–‡ä»¶ä¿¡æ¯
        cache_file = os.path.join(app.config['EMBEDDINGS_FOLDER'], 'embedding_cache.pkl')
        cache_time = None
        cache_size = None
        
        if os.path.exists(cache_file):
            mtime = os.path.getmtime(cache_file)
            cache_time = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
            cache_size = os.path.getsize(cache_file) / (1024 * 1024)  # MB
        
        return jsonify({
            "status": "ready",
            "message": "çŸ¥è¯†åº“å°±ç»ª",
            "has_cache": True,
            "doc_count": doc_count,
            "cache_time": cache_time,
            "cache_size_mb": round(cache_size, 2) if cache_size else None,
            "sources": list(set([meta.get('source', 'unknown') for meta in cache_data.get('meta', [])]))
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": f"æ£€æŸ¥çŠ¶æ€å¤±è´¥: {str(e)}",
            "has_cache": False,
            "doc_count": 0
        }), 500

@app.route('/api/rag_query', methods=['POST'])
def test_rag_query():
    """æµ‹è¯•RAGæŸ¥è¯¢åŠŸèƒ½çš„APIç«¯ç‚¹"""
    try:
        data = request.get_json()
        query = data.get('query')
        original_question = data.get('original_question')
        
        if not query:
            return jsonify({
                "status": "error",
                "message": "ç¼ºå°‘æŸ¥è¯¢å‚æ•°"
            }), 400
        
        success, answer, docs = perform_rag_query(query, original_question)
        
        return jsonify({
            "status": "success" if success else "error",
            "query": query,
            "original_question": original_question,
            "answer": answer,
            "relevant_docs_count": len(docs),
            "docs": [{"text": doc["text"][:200] + "..." if len(doc["text"]) > 200 else doc["text"], 
                     "source": doc["meta"]["source"], 
                     "similarity": doc["similarity"]} for doc in docs[:3]] if docs else []
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

def format_context_for_debug(messages, max_content_length=2000, full_output_for_reasoning=False):
    """
    æ ¼å¼åŒ–æ¶ˆæ¯ä¸Šä¸‹æ–‡ï¼Œç”¨äºè°ƒè¯•æ—¥å¿—è¾“å‡º
    :param messages: è¦æ ¼å¼åŒ–çš„æ¶ˆæ¯åˆ—è¡¨
    :param max_content_length: æœ€å¤§å†…å®¹é•¿åº¦ï¼ˆç”¨äºæˆªæ–­ï¼‰
    :param full_output_for_reasoning: æ˜¯å¦ä¸ºæ¨ç†æ¨¡å‹å®Œæ•´è¾“å‡ºï¼ˆä¸æˆªæ–­MCPå·¥å…·è¿”å›ä¿¡æ¯ï¼‰
    """
    formatted_messages = []
    total_chars = 0
    total_estimated_tokens = 0
    
    for i, msg in enumerate(messages):
        content = msg.get('content', '')
        content_length = len(content)
        estimated_tokens = content_length // 4  # ç²—ç•¥ä¼°ç®—tokenæ•°
        total_chars += content_length
        total_estimated_tokens += estimated_tokens
        
        # å¯¹äºæ¨ç†æ¨¡å‹ï¼Œå¦‚æœæ¶ˆæ¯å†…å®¹æ˜¯MCPå·¥å…·è¿”å›ä¿¡æ¯ï¼Œåˆ™å®Œæ•´è¾“å‡º
        if full_output_for_reasoning and msg.get('role') == 'system' and content.startswith('MCPå·¥å…·è¿”å›ä¿¡æ¯ï¼š'):
            display_content = content  # å®Œæ•´è¾“å‡ºï¼Œä¸æˆªæ–­
        elif content_length > max_content_length:
            display_content = content[:max_content_length] + f"...(å‰©ä½™{content_length - max_content_length}å­—ç¬¦)"
        else:
            display_content = content
        
        formatted_messages.append({
            'index': i + 1,
            'role': msg.get('role'),
            'content_length': content_length,
            'estimated_tokens': estimated_tokens,
            'content': display_content
        })
    
    summary = {
        'total_messages': len(messages),
        'total_characters': total_chars,
        'estimated_total_tokens': total_estimated_tokens,
        'messages': formatted_messages
    }
    
    return json.dumps(summary, ensure_ascii=False, indent=2)

# åœ¨gunicornç¯å¢ƒä¸­å¯ç”¨æ™ºèƒ½åŠ è½½ç­–ç•¥
if __name__ != '__main__':
    # è¿™è¡¨ç¤ºæˆ‘ä»¬åœ¨gunicorn workerä¸­è¿è¡Œ
    logging.info("åœ¨gunicorn workerä¸­è¿è¡Œï¼Œå¯ç”¨æ™ºèƒ½åŠ è½½ç­–ç•¥...")
    smart_startup_initialization()  # æ ¹æ®ç¼“å­˜çŠ¶æ€æ™ºèƒ½å†³å®šæ˜¯å¦é¢„åŠ è½½æ¨¡å‹

if __name__ == '__main__':
    print("Flask is starting...")
    
    # åˆå§‹åŒ–å½“å‰åŸå¸‚é…ç½®
    update_current_city()
    
    # åœ¨å¼€å‘æ¨¡å¼ä¸‹ä¹Ÿå¯ç”¨æ™ºèƒ½åŠ è½½ç­–ç•¥
    smart_startup_initialization()  # æ ¹æ®ç¼“å­˜çŠ¶æ€æ™ºèƒ½å†³å®šæ˜¯å¦é¢„åŠ è½½æ¨¡å‹
    
    # æ–¹æ¡ˆ1: ç¦ç”¨é‡è½½å™¨é¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼ˆæ¨èï¼‰
    app.run(
        debug=True, 
        use_reloader=False,  # ç¦ç”¨é‡è½½å™¨ï¼Œé¿å…é‡å¤åˆå§‹åŒ–å’Œæ¨¡å‹åŠ è½½
        host='127.0.0.1',
        port=8000
    )
    
    print("Flask has started.")